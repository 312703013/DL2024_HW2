05/20/2024 23:50:29 - INFO - transformers.tokenization_utils_base - loading file tokenizer.model from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\tokenizer.model

05/20/2024 23:50:29 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\tokenizer.json

05/20/2024 23:50:29 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json from cache at None

05/20/2024 23:50:29 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\special_tokens_map.json

05/20/2024 23:50:29 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\tokenizer_config.json

05/20/2024 23:50:31 - INFO - llamafactory.data.loader - Loading dataset QuestionTest.json...

05/20/2024 23:50:33 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/20/2024 23:50:33 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "_name_or_path": "google/gemma-2b",
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/20/2024 23:50:33 - WARNING - llamafactory.model.utils.attention - FlashAttention-2 is not installed.

05/20/2024 23:50:33 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.

05/20/2024 23:50:33 - INFO - transformers.modeling_utils - loading weights file model.safetensors from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\model.safetensors.index.json

05/20/2024 23:50:33 - INFO - transformers.modeling_utils - Instantiating GemmaForCausalLM model under default dtype torch.bfloat16.

05/20/2024 23:50:33 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 1,
  "pad_token_id": 0
}


05/20/2024 23:50:33 - WARNING - transformers.models.gemma.modeling_gemma - `config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.

05/20/2024 23:50:39 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing GemmaForCausalLM.


05/20/2024 23:50:39 - INFO - transformers.modeling_utils - All the weights of GemmaForCausalLM were initialized from the model checkpoint at google/gemma-2b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GemmaForCausalLM for predictions without further training.

05/20/2024 23:50:39 - INFO - transformers.generation.configuration_utils - loading configuration file generation_config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\generation_config.json

05/20/2024 23:50:39 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 2,
  "eos_token_id": 1,
  "pad_token_id": 0
}


05/20/2024 23:50:39 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.

05/20/2024 23:50:39 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.

05/20/2024 23:50:39 - INFO - llamafactory.model.adapter - DeepSpeed/FSDP/PureBF16/BAdam detected, remaining trainable params as their original precision.

05/20/2024 23:50:39 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

05/20/2024 23:50:39 - INFO - llamafactory.model.loader - trainable params: 921600 || all params: 2507094016 || trainable%: 0.0368

05/20/2024 23:50:39 - INFO - transformers.trainer - Using auto half precision backend

05/20/2024 23:50:40 - INFO - llamafactory.train.utils - Using BAdam optimizer with layer-wise update, switch mode is ascending, switch block every 50 steps, default start block is None

05/20/2024 23:50:40 - INFO - transformers.trainer - ***** Running training *****

05/20/2024 23:50:40 - INFO - transformers.trainer -   Num examples = 13,550

05/20/2024 23:50:40 - INFO - transformers.trainer -   Num Epochs = 3

05/20/2024 23:50:40 - INFO - transformers.trainer -   Instantaneous batch size per device = 2

05/20/2024 23:50:40 - INFO - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 16

05/20/2024 23:50:40 - INFO - transformers.trainer -   Gradient Accumulation steps = 8

05/20/2024 23:50:40 - INFO - transformers.trainer -   Total optimization steps = 2,538

05/20/2024 23:50:40 - INFO - transformers.trainer -   Number of trainable parameters = 51,200

05/20/2024 23:51:15 - INFO - llamafactory.extras.callbacks - {'loss': 4.4783, 'learning_rate': 5.0000e-05, 'epoch': 0.01}

05/20/2024 23:51:57 - INFO - llamafactory.extras.callbacks - {'loss': 4.5090, 'learning_rate': 4.9998e-05, 'epoch': 0.01}

05/20/2024 23:52:32 - INFO - llamafactory.extras.callbacks - {'loss': 4.5891, 'learning_rate': 4.9996e-05, 'epoch': 0.02}

05/20/2024 23:53:06 - INFO - llamafactory.extras.callbacks - {'loss': 4.5224, 'learning_rate': 4.9992e-05, 'epoch': 0.02}

05/20/2024 23:53:41 - INFO - llamafactory.extras.callbacks - {'loss': 4.6422, 'learning_rate': 4.9988e-05, 'epoch': 0.03}

05/20/2024 23:54:22 - INFO - llamafactory.extras.callbacks - {'loss': 4.3539, 'learning_rate': 4.9983e-05, 'epoch': 0.04}

05/20/2024 23:54:58 - INFO - llamafactory.extras.callbacks - {'loss': 4.4322, 'learning_rate': 4.9977e-05, 'epoch': 0.04}

05/20/2024 23:55:44 - INFO - llamafactory.extras.callbacks - {'loss': 4.3115, 'learning_rate': 4.9969e-05, 'epoch': 0.05}

05/20/2024 23:56:19 - INFO - llamafactory.extras.callbacks - {'loss': 4.3000, 'learning_rate': 4.9961e-05, 'epoch': 0.05}

05/20/2024 23:57:01 - INFO - llamafactory.extras.callbacks - {'loss': 4.2738, 'learning_rate': 4.9952e-05, 'epoch': 0.06}

05/20/2024 23:57:37 - INFO - llamafactory.extras.callbacks - {'loss': 4.1889, 'learning_rate': 4.9942e-05, 'epoch': 0.06}

05/20/2024 23:58:16 - INFO - llamafactory.extras.callbacks - {'loss': 4.2243, 'learning_rate': 4.9931e-05, 'epoch': 0.07}

05/20/2024 23:58:55 - INFO - llamafactory.extras.callbacks - {'loss': 4.1797, 'learning_rate': 4.9919e-05, 'epoch': 0.08}

05/20/2024 23:59:30 - INFO - llamafactory.extras.callbacks - {'loss': 4.1613, 'learning_rate': 4.9906e-05, 'epoch': 0.08}

05/21/2024 00:00:10 - INFO - llamafactory.extras.callbacks - {'loss': 4.1570, 'learning_rate': 4.9892e-05, 'epoch': 0.09}

05/21/2024 00:00:48 - INFO - llamafactory.extras.callbacks - {'loss': 4.1740, 'learning_rate': 4.9878e-05, 'epoch': 0.09}

05/21/2024 00:01:23 - INFO - llamafactory.extras.callbacks - {'loss': 4.2235, 'learning_rate': 4.9862e-05, 'epoch': 0.10}

05/21/2024 00:02:00 - INFO - llamafactory.extras.callbacks - {'loss': 4.0566, 'learning_rate': 4.9845e-05, 'epoch': 0.11}

05/21/2024 00:02:31 - INFO - llamafactory.extras.callbacks - {'loss': 4.0474, 'learning_rate': 4.9827e-05, 'epoch': 0.11}

05/21/2024 00:03:14 - INFO - llamafactory.extras.callbacks - {'loss': 4.0193, 'learning_rate': 4.9809e-05, 'epoch': 0.12}

05/21/2024 00:03:14 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-100

05/21/2024 00:03:15 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 00:03:15 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 00:03:15 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-100\tokenizer_config.json

05/21/2024 00:03:15 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-100\special_tokens_map.json

05/21/2024 00:03:55 - INFO - llamafactory.extras.callbacks - {'loss': 3.9902, 'learning_rate': 4.9789e-05, 'epoch': 0.12}

05/21/2024 00:04:29 - INFO - llamafactory.extras.callbacks - {'loss': 4.0341, 'learning_rate': 4.9769e-05, 'epoch': 0.13}

05/21/2024 00:05:07 - INFO - llamafactory.extras.callbacks - {'loss': 3.9622, 'learning_rate': 4.9747e-05, 'epoch': 0.14}

05/21/2024 00:05:43 - INFO - llamafactory.extras.callbacks - {'loss': 3.9459, 'learning_rate': 4.9725e-05, 'epoch': 0.14}

05/21/2024 00:06:19 - INFO - llamafactory.extras.callbacks - {'loss': 4.0215, 'learning_rate': 4.9701e-05, 'epoch': 0.15}

05/21/2024 00:06:50 - INFO - llamafactory.extras.callbacks - {'loss': 3.9589, 'learning_rate': 4.9677e-05, 'epoch': 0.15}

05/21/2024 00:07:26 - INFO - llamafactory.extras.callbacks - {'loss': 3.8879, 'learning_rate': 4.9652e-05, 'epoch': 0.16}

05/21/2024 00:08:04 - INFO - llamafactory.extras.callbacks - {'loss': 3.8181, 'learning_rate': 4.9626e-05, 'epoch': 0.17}

05/21/2024 00:08:39 - INFO - llamafactory.extras.callbacks - {'loss': 3.8729, 'learning_rate': 4.9598e-05, 'epoch': 0.17}

05/21/2024 00:09:13 - INFO - llamafactory.extras.callbacks - {'loss': 3.8666, 'learning_rate': 4.9570e-05, 'epoch': 0.18}

05/21/2024 00:09:48 - INFO - llamafactory.extras.callbacks - {'loss': 3.8782, 'learning_rate': 4.9541e-05, 'epoch': 0.18}

05/21/2024 00:10:21 - INFO - llamafactory.extras.callbacks - {'loss': 3.9086, 'learning_rate': 4.9511e-05, 'epoch': 0.19}

05/21/2024 00:10:54 - INFO - llamafactory.extras.callbacks - {'loss': 3.8456, 'learning_rate': 4.9480e-05, 'epoch': 0.19}

05/21/2024 00:11:26 - INFO - llamafactory.extras.callbacks - {'loss': 3.8829, 'learning_rate': 4.9449e-05, 'epoch': 0.20}

05/21/2024 00:11:59 - INFO - llamafactory.extras.callbacks - {'loss': 3.8318, 'learning_rate': 4.9416e-05, 'epoch': 0.21}

05/21/2024 00:12:32 - INFO - llamafactory.extras.callbacks - {'loss': 3.7505, 'learning_rate': 4.9382e-05, 'epoch': 0.21}

05/21/2024 00:13:06 - INFO - llamafactory.extras.callbacks - {'loss': 3.8218, 'learning_rate': 4.9347e-05, 'epoch': 0.22}

05/21/2024 00:13:43 - INFO - llamafactory.extras.callbacks - {'loss': 3.6429, 'learning_rate': 4.9312e-05, 'epoch': 0.22}

05/21/2024 00:14:17 - INFO - llamafactory.extras.callbacks - {'loss': 3.6477, 'learning_rate': 4.9275e-05, 'epoch': 0.23}

05/21/2024 00:14:55 - INFO - llamafactory.extras.callbacks - {'loss': 3.5598, 'learning_rate': 4.9238e-05, 'epoch': 0.24}

05/21/2024 00:14:55 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-200

05/21/2024 00:14:55 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 00:14:55 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 00:14:55 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-200\tokenizer_config.json

05/21/2024 00:14:55 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-200\special_tokens_map.json

05/21/2024 00:15:30 - INFO - llamafactory.extras.callbacks - {'loss': 3.5825, 'learning_rate': 4.9199e-05, 'epoch': 0.24}

05/21/2024 00:16:06 - INFO - llamafactory.extras.callbacks - {'loss': 3.5425, 'learning_rate': 4.9160e-05, 'epoch': 0.25}

05/21/2024 00:16:38 - INFO - llamafactory.extras.callbacks - {'loss': 3.5632, 'learning_rate': 4.9120e-05, 'epoch': 0.25}

05/21/2024 00:17:24 - INFO - llamafactory.extras.callbacks - {'loss': 3.5343, 'learning_rate': 4.9079e-05, 'epoch': 0.26}

05/21/2024 00:17:58 - INFO - llamafactory.extras.callbacks - {'loss': 3.4819, 'learning_rate': 4.9037e-05, 'epoch': 0.27}

05/21/2024 00:18:29 - INFO - llamafactory.extras.callbacks - {'loss': 3.4327, 'learning_rate': 4.8994e-05, 'epoch': 0.27}

05/21/2024 00:19:01 - INFO - llamafactory.extras.callbacks - {'loss': 3.5495, 'learning_rate': 4.8950e-05, 'epoch': 0.28}

05/21/2024 00:19:35 - INFO - llamafactory.extras.callbacks - {'loss': 3.4327, 'learning_rate': 4.8905e-05, 'epoch': 0.28}

05/21/2024 00:20:04 - INFO - llamafactory.extras.callbacks - {'loss': 3.4724, 'learning_rate': 4.8859e-05, 'epoch': 0.29}

05/21/2024 00:20:37 - INFO - llamafactory.extras.callbacks - {'loss': 3.3306, 'learning_rate': 4.8812e-05, 'epoch': 0.30}

05/21/2024 00:21:14 - INFO - llamafactory.extras.callbacks - {'loss': 3.3346, 'learning_rate': 4.8765e-05, 'epoch': 0.30}

05/21/2024 00:22:09 - INFO - llamafactory.extras.callbacks - {'loss': 3.3474, 'learning_rate': 4.8716e-05, 'epoch': 0.31}

05/21/2024 00:22:42 - INFO - llamafactory.extras.callbacks - {'loss': 3.3161, 'learning_rate': 4.8667e-05, 'epoch': 0.31}

05/21/2024 00:23:09 - INFO - llamafactory.extras.callbacks - {'loss': 3.2948, 'learning_rate': 4.8617e-05, 'epoch': 0.32}

05/21/2024 00:23:38 - INFO - llamafactory.extras.callbacks - {'loss': 3.3253, 'learning_rate': 4.8566e-05, 'epoch': 0.32}

05/21/2024 00:24:07 - INFO - llamafactory.extras.callbacks - {'loss': 3.2852, 'learning_rate': 4.8513e-05, 'epoch': 0.33}

05/21/2024 00:24:39 - INFO - llamafactory.extras.callbacks - {'loss': 3.2114, 'learning_rate': 4.8460e-05, 'epoch': 0.34}

05/21/2024 00:25:13 - INFO - llamafactory.extras.callbacks - {'loss': 3.1373, 'learning_rate': 4.8406e-05, 'epoch': 0.34}

05/21/2024 00:25:58 - INFO - llamafactory.extras.callbacks - {'loss': 3.1371, 'learning_rate': 4.8352e-05, 'epoch': 0.35}

05/21/2024 00:26:28 - INFO - llamafactory.extras.callbacks - {'loss': 3.0779, 'learning_rate': 4.8296e-05, 'epoch': 0.35}

05/21/2024 00:26:28 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-300

05/21/2024 00:26:29 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 00:26:29 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 00:26:29 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-300\tokenizer_config.json

05/21/2024 00:26:29 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-300\special_tokens_map.json

05/21/2024 00:27:01 - INFO - llamafactory.extras.callbacks - {'loss': 3.1154, 'learning_rate': 4.8239e-05, 'epoch': 0.36}

05/21/2024 00:27:31 - INFO - llamafactory.extras.callbacks - {'loss': 3.1631, 'learning_rate': 4.8182e-05, 'epoch': 0.37}

05/21/2024 00:28:05 - INFO - llamafactory.extras.callbacks - {'loss': 3.0252, 'learning_rate': 4.8124e-05, 'epoch': 0.37}

05/21/2024 00:28:36 - INFO - llamafactory.extras.callbacks - {'loss': 3.0211, 'learning_rate': 4.8064e-05, 'epoch': 0.38}

05/21/2024 00:29:06 - INFO - llamafactory.extras.callbacks - {'loss': 3.0710, 'learning_rate': 4.8004e-05, 'epoch': 0.38}

05/21/2024 00:29:41 - INFO - llamafactory.extras.callbacks - {'loss': 3.0510, 'learning_rate': 4.7943e-05, 'epoch': 0.39}

05/21/2024 00:30:11 - INFO - llamafactory.extras.callbacks - {'loss': 2.9828, 'learning_rate': 4.7881e-05, 'epoch': 0.40}

05/21/2024 00:30:43 - INFO - llamafactory.extras.callbacks - {'loss': 3.0087, 'learning_rate': 4.7818e-05, 'epoch': 0.40}

05/21/2024 00:31:19 - INFO - llamafactory.extras.callbacks - {'loss': 2.8856, 'learning_rate': 4.7755e-05, 'epoch': 0.41}

05/21/2024 00:31:50 - INFO - llamafactory.extras.callbacks - {'loss': 3.0300, 'learning_rate': 4.7690e-05, 'epoch': 0.41}

05/21/2024 00:32:22 - INFO - llamafactory.extras.callbacks - {'loss': 2.8937, 'learning_rate': 4.7625e-05, 'epoch': 0.42}

05/21/2024 00:32:53 - INFO - llamafactory.extras.callbacks - {'loss': 2.8974, 'learning_rate': 4.7559e-05, 'epoch': 0.43}

05/21/2024 00:33:27 - INFO - llamafactory.extras.callbacks - {'loss': 2.8785, 'learning_rate': 4.7492e-05, 'epoch': 0.43}

05/21/2024 00:33:57 - INFO - llamafactory.extras.callbacks - {'loss': 2.8568, 'learning_rate': 4.7424e-05, 'epoch': 0.44}

05/21/2024 00:34:28 - INFO - llamafactory.extras.callbacks - {'loss': 2.8550, 'learning_rate': 4.7355e-05, 'epoch': 0.44}

05/21/2024 00:35:01 - INFO - llamafactory.extras.callbacks - {'loss': 2.7836, 'learning_rate': 4.7285e-05, 'epoch': 0.45}

05/21/2024 00:35:31 - INFO - llamafactory.extras.callbacks - {'loss': 2.7962, 'learning_rate': 4.7214e-05, 'epoch': 0.45}

05/21/2024 00:36:01 - INFO - llamafactory.extras.callbacks - {'loss': 2.8336, 'learning_rate': 4.7143e-05, 'epoch': 0.46}

05/21/2024 00:36:29 - INFO - llamafactory.extras.callbacks - {'loss': 2.9422, 'learning_rate': 4.7071e-05, 'epoch': 0.47}

05/21/2024 00:37:00 - INFO - llamafactory.extras.callbacks - {'loss': 2.7376, 'learning_rate': 4.6998e-05, 'epoch': 0.47}

05/21/2024 00:37:00 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-400

05/21/2024 00:37:01 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 00:37:01 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 00:37:01 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-400\tokenizer_config.json

05/21/2024 00:37:01 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-400\special_tokens_map.json

05/21/2024 00:37:33 - INFO - llamafactory.extras.callbacks - {'loss': 2.7606, 'learning_rate': 4.6924e-05, 'epoch': 0.48}

05/21/2024 00:38:04 - INFO - llamafactory.extras.callbacks - {'loss': 2.6720, 'learning_rate': 4.6849e-05, 'epoch': 0.48}

05/21/2024 00:38:32 - INFO - llamafactory.extras.callbacks - {'loss': 2.7051, 'learning_rate': 4.6773e-05, 'epoch': 0.49}

05/21/2024 00:39:03 - INFO - llamafactory.extras.callbacks - {'loss': 2.7139, 'learning_rate': 4.6697e-05, 'epoch': 0.50}

05/21/2024 00:39:32 - INFO - llamafactory.extras.callbacks - {'loss': 2.6204, 'learning_rate': 4.6620e-05, 'epoch': 0.50}

05/21/2024 00:40:04 - INFO - llamafactory.extras.callbacks - {'loss': 2.6245, 'learning_rate': 4.6542e-05, 'epoch': 0.51}

05/21/2024 00:40:31 - INFO - llamafactory.extras.callbacks - {'loss': 2.5920, 'learning_rate': 4.6463e-05, 'epoch': 0.51}

05/21/2024 00:41:00 - INFO - llamafactory.extras.callbacks - {'loss': 2.5224, 'learning_rate': 4.6383e-05, 'epoch': 0.52}

05/21/2024 00:41:30 - INFO - llamafactory.extras.callbacks - {'loss': 2.5803, 'learning_rate': 4.6302e-05, 'epoch': 0.53}

05/21/2024 00:42:02 - INFO - llamafactory.extras.callbacks - {'loss': 2.4042, 'learning_rate': 4.6221e-05, 'epoch': 0.53}

05/21/2024 00:42:29 - INFO - llamafactory.extras.callbacks - {'loss': 2.4386, 'learning_rate': 4.6139e-05, 'epoch': 0.54}

05/21/2024 00:42:59 - INFO - llamafactory.extras.callbacks - {'loss': 2.4683, 'learning_rate': 4.6056e-05, 'epoch': 0.54}

05/21/2024 00:43:28 - INFO - llamafactory.extras.callbacks - {'loss': 2.5159, 'learning_rate': 4.5972e-05, 'epoch': 0.55}

05/21/2024 00:43:57 - INFO - llamafactory.extras.callbacks - {'loss': 2.4562, 'learning_rate': 4.5887e-05, 'epoch': 0.55}

05/21/2024 00:44:26 - INFO - llamafactory.extras.callbacks - {'loss': 2.3960, 'learning_rate': 4.5802e-05, 'epoch': 0.56}

05/21/2024 00:44:53 - INFO - llamafactory.extras.callbacks - {'loss': 2.4618, 'learning_rate': 4.5716e-05, 'epoch': 0.57}

05/21/2024 00:45:20 - INFO - llamafactory.extras.callbacks - {'loss': 2.4078, 'learning_rate': 4.5629e-05, 'epoch': 0.57}

05/21/2024 00:45:47 - INFO - llamafactory.extras.callbacks - {'loss': 2.3605, 'learning_rate': 4.5541e-05, 'epoch': 0.58}

05/21/2024 00:46:16 - INFO - llamafactory.extras.callbacks - {'loss': 2.3620, 'learning_rate': 4.5452e-05, 'epoch': 0.58}

05/21/2024 00:46:43 - INFO - llamafactory.extras.callbacks - {'loss': 2.2891, 'learning_rate': 4.5363e-05, 'epoch': 0.59}

05/21/2024 00:46:43 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-500

05/21/2024 00:46:43 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 00:46:43 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 00:46:43 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-500\tokenizer_config.json

05/21/2024 00:46:43 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-500\special_tokens_map.json

05/21/2024 00:47:16 - INFO - llamafactory.extras.callbacks - {'loss': 2.2122, 'learning_rate': 4.5273e-05, 'epoch': 0.60}

05/21/2024 00:47:49 - INFO - llamafactory.extras.callbacks - {'loss': 2.2546, 'learning_rate': 4.5182e-05, 'epoch': 0.60}

05/21/2024 00:48:20 - INFO - llamafactory.extras.callbacks - {'loss': 2.2687, 'learning_rate': 4.5090e-05, 'epoch': 0.61}

05/21/2024 00:48:48 - INFO - llamafactory.extras.callbacks - {'loss': 2.2578, 'learning_rate': 4.4998e-05, 'epoch': 0.61}

05/21/2024 00:49:14 - INFO - llamafactory.extras.callbacks - {'loss': 2.1926, 'learning_rate': 4.4904e-05, 'epoch': 0.62}

05/21/2024 00:49:44 - INFO - llamafactory.extras.callbacks - {'loss': 2.1776, 'learning_rate': 4.4810e-05, 'epoch': 0.63}

05/21/2024 00:50:11 - INFO - llamafactory.extras.callbacks - {'loss': 2.1381, 'learning_rate': 4.4716e-05, 'epoch': 0.63}

05/21/2024 00:50:37 - INFO - llamafactory.extras.callbacks - {'loss': 2.1828, 'learning_rate': 4.4620e-05, 'epoch': 0.64}

05/21/2024 00:51:05 - INFO - llamafactory.extras.callbacks - {'loss': 2.0717, 'learning_rate': 4.4524e-05, 'epoch': 0.64}

05/21/2024 00:51:33 - INFO - llamafactory.extras.callbacks - {'loss': 2.0391, 'learning_rate': 4.4427e-05, 'epoch': 0.65}

05/21/2024 00:52:00 - INFO - llamafactory.extras.callbacks - {'loss': 2.0611, 'learning_rate': 4.4329e-05, 'epoch': 0.66}

05/21/2024 00:52:27 - INFO - llamafactory.extras.callbacks - {'loss': 2.0061, 'learning_rate': 4.4230e-05, 'epoch': 0.66}

05/21/2024 00:52:53 - INFO - llamafactory.extras.callbacks - {'loss': 2.0885, 'learning_rate': 4.4131e-05, 'epoch': 0.67}

05/21/2024 00:53:22 - INFO - llamafactory.extras.callbacks - {'loss': 2.0410, 'learning_rate': 4.4031e-05, 'epoch': 0.67}

05/21/2024 00:53:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.9806, 'learning_rate': 4.3931e-05, 'epoch': 0.68}

05/21/2024 00:54:16 - INFO - llamafactory.extras.callbacks - {'loss': 2.0112, 'learning_rate': 4.3829e-05, 'epoch': 0.68}

05/21/2024 00:54:40 - INFO - llamafactory.extras.callbacks - {'loss': 1.9483, 'learning_rate': 4.3727e-05, 'epoch': 0.69}

05/21/2024 00:55:03 - INFO - llamafactory.extras.callbacks - {'loss': 1.8748, 'learning_rate': 4.3624e-05, 'epoch': 0.70}

05/21/2024 00:55:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.9604, 'learning_rate': 4.3521e-05, 'epoch': 0.70}

05/21/2024 00:55:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.8259, 'learning_rate': 4.3416e-05, 'epoch': 0.71}

05/21/2024 00:55:58 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-600

05/21/2024 00:55:59 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 00:55:59 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 00:55:59 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-600\tokenizer_config.json

05/21/2024 00:55:59 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-600\special_tokens_map.json

05/21/2024 00:56:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.8127, 'learning_rate': 4.3311e-05, 'epoch': 0.71}

05/21/2024 00:56:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.8668, 'learning_rate': 4.3206e-05, 'epoch': 0.72}

05/21/2024 00:57:18 - INFO - llamafactory.extras.callbacks - {'loss': 1.7489, 'learning_rate': 4.3099e-05, 'epoch': 0.73}

05/21/2024 00:57:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.8181, 'learning_rate': 4.2992e-05, 'epoch': 0.73}

05/21/2024 00:58:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.8693, 'learning_rate': 4.2884e-05, 'epoch': 0.74}

05/21/2024 00:58:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.8299, 'learning_rate': 4.2776e-05, 'epoch': 0.74}

05/21/2024 00:58:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.7578, 'learning_rate': 4.2667e-05, 'epoch': 0.75}

05/21/2024 00:59:21 - INFO - llamafactory.extras.callbacks - {'loss': 1.8116, 'learning_rate': 4.2557e-05, 'epoch': 0.76}

05/21/2024 00:59:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.7903, 'learning_rate': 4.2446e-05, 'epoch': 0.76}

05/21/2024 01:00:13 - INFO - llamafactory.extras.callbacks - {'loss': 1.6685, 'learning_rate': 4.2335e-05, 'epoch': 0.77}

05/21/2024 01:00:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.5980, 'learning_rate': 4.2223e-05, 'epoch': 0.77}

05/21/2024 01:01:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.6866, 'learning_rate': 4.2111e-05, 'epoch': 0.78}

05/21/2024 01:01:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.7349, 'learning_rate': 4.1998e-05, 'epoch': 0.79}

05/21/2024 01:01:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.6606, 'learning_rate': 4.1884e-05, 'epoch': 0.79}

05/21/2024 01:02:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.5997, 'learning_rate': 4.1770e-05, 'epoch': 0.80}

05/21/2024 01:02:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.6252, 'learning_rate': 4.1655e-05, 'epoch': 0.80}

05/21/2024 01:03:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.5683, 'learning_rate': 4.1539e-05, 'epoch': 0.81}

05/21/2024 01:03:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.5413, 'learning_rate': 4.1422e-05, 'epoch': 0.81}

05/21/2024 01:03:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.5218, 'learning_rate': 4.1306e-05, 'epoch': 0.82}

05/21/2024 01:04:18 - INFO - llamafactory.extras.callbacks - {'loss': 1.6332, 'learning_rate': 4.1188e-05, 'epoch': 0.83}

05/21/2024 01:04:18 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-700

05/21/2024 01:04:19 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 01:04:19 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 01:04:19 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-700\tokenizer_config.json

05/21/2024 01:04:19 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-700\special_tokens_map.json

05/21/2024 01:04:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.5016, 'learning_rate': 4.1070e-05, 'epoch': 0.83}

05/21/2024 01:05:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.5312, 'learning_rate': 4.0951e-05, 'epoch': 0.84}

05/21/2024 01:05:32 - INFO - llamafactory.extras.callbacks - {'loss': 1.5519, 'learning_rate': 4.0831e-05, 'epoch': 0.84}

05/21/2024 01:05:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.5032, 'learning_rate': 4.0711e-05, 'epoch': 0.85}

05/21/2024 01:06:23 - INFO - llamafactory.extras.callbacks - {'loss': 1.5520, 'learning_rate': 4.0591e-05, 'epoch': 0.86}

05/21/2024 01:06:46 - INFO - llamafactory.extras.callbacks - {'loss': 1.4270, 'learning_rate': 4.0469e-05, 'epoch': 0.86}

05/21/2024 01:07:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.4626, 'learning_rate': 4.0348e-05, 'epoch': 0.87}

05/21/2024 01:07:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.4738, 'learning_rate': 4.0225e-05, 'epoch': 0.87}

05/21/2024 01:07:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.3958, 'learning_rate': 4.0102e-05, 'epoch': 0.88}

05/21/2024 01:08:17 - INFO - llamafactory.extras.callbacks - {'loss': 1.4793, 'learning_rate': 3.9979e-05, 'epoch': 0.89}

05/21/2024 01:08:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.3973, 'learning_rate': 3.9854e-05, 'epoch': 0.89}

05/21/2024 01:09:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.3843, 'learning_rate': 3.9730e-05, 'epoch': 0.90}

05/21/2024 01:09:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.4383, 'learning_rate': 3.9604e-05, 'epoch': 0.90}

05/21/2024 01:09:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.4492, 'learning_rate': 3.9478e-05, 'epoch': 0.91}

05/21/2024 01:10:15 - INFO - llamafactory.extras.callbacks - {'loss': 1.3070, 'learning_rate': 3.9352e-05, 'epoch': 0.92}

05/21/2024 01:10:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.3175, 'learning_rate': 3.9225e-05, 'epoch': 0.92}

05/21/2024 01:10:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.2698, 'learning_rate': 3.9098e-05, 'epoch': 0.93}

05/21/2024 01:11:20 - INFO - llamafactory.extras.callbacks - {'loss': 1.3378, 'learning_rate': 3.8970e-05, 'epoch': 0.93}

05/21/2024 01:11:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.3617, 'learning_rate': 3.8841e-05, 'epoch': 0.94}

05/21/2024 01:12:12 - INFO - llamafactory.extras.callbacks - {'loss': 1.2756, 'learning_rate': 3.8712e-05, 'epoch': 0.94}

05/21/2024 01:12:12 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-800

05/21/2024 01:12:12 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 01:12:12 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 01:12:12 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-800\tokenizer_config.json

05/21/2024 01:12:12 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-800\special_tokens_map.json

05/21/2024 01:12:36 - INFO - llamafactory.extras.callbacks - {'loss': 1.2478, 'learning_rate': 3.8582e-05, 'epoch': 0.95}

05/21/2024 01:12:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.3266, 'learning_rate': 3.8452e-05, 'epoch': 0.96}

05/21/2024 01:13:20 - INFO - llamafactory.extras.callbacks - {'loss': 1.3213, 'learning_rate': 3.8321e-05, 'epoch': 0.96}

05/21/2024 01:13:41 - INFO - llamafactory.extras.callbacks - {'loss': 1.3517, 'learning_rate': 3.8190e-05, 'epoch': 0.97}

05/21/2024 01:14:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.2671, 'learning_rate': 3.8059e-05, 'epoch': 0.97}

05/21/2024 01:14:23 - INFO - llamafactory.extras.callbacks - {'loss': 1.3377, 'learning_rate': 3.7926e-05, 'epoch': 0.98}

05/21/2024 01:14:45 - INFO - llamafactory.extras.callbacks - {'loss': 1.2146, 'learning_rate': 3.7794e-05, 'epoch': 0.99}

05/21/2024 01:15:04 - INFO - llamafactory.extras.callbacks - {'loss': 1.2757, 'learning_rate': 3.7660e-05, 'epoch': 0.99}

05/21/2024 01:15:27 - INFO - llamafactory.extras.callbacks - {'loss': 1.1159, 'learning_rate': 3.7527e-05, 'epoch': 1.00}

05/21/2024 01:15:46 - INFO - llamafactory.extras.callbacks - {'loss': 1.2136, 'learning_rate': 3.7393e-05, 'epoch': 1.00}

05/21/2024 01:16:03 - INFO - llamafactory.extras.callbacks - {'loss': 1.2258, 'learning_rate': 3.7258e-05, 'epoch': 1.01}

05/21/2024 01:16:29 - INFO - llamafactory.extras.callbacks - {'loss': 1.2309, 'learning_rate': 3.7123e-05, 'epoch': 1.02}

05/21/2024 01:16:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.2304, 'learning_rate': 3.6987e-05, 'epoch': 1.02}

05/21/2024 01:17:16 - INFO - llamafactory.extras.callbacks - {'loss': 1.3014, 'learning_rate': 3.6851e-05, 'epoch': 1.03}

05/21/2024 01:17:38 - INFO - llamafactory.extras.callbacks - {'loss': 1.2201, 'learning_rate': 3.6715e-05, 'epoch': 1.03}

05/21/2024 01:17:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.1927, 'learning_rate': 3.6578e-05, 'epoch': 1.04}

05/21/2024 01:18:20 - INFO - llamafactory.extras.callbacks - {'loss': 1.1885, 'learning_rate': 3.6441e-05, 'epoch': 1.05}

05/21/2024 01:18:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.1561, 'learning_rate': 3.6303e-05, 'epoch': 1.05}

05/21/2024 01:19:04 - INFO - llamafactory.extras.callbacks - {'loss': 1.1492, 'learning_rate': 3.6165e-05, 'epoch': 1.06}

05/21/2024 01:19:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.1923, 'learning_rate': 3.6026e-05, 'epoch': 1.06}

05/21/2024 01:19:31 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-900

05/21/2024 01:19:32 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 01:19:32 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 01:19:32 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-900\tokenizer_config.json

05/21/2024 01:19:32 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-900\special_tokens_map.json

05/21/2024 01:20:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.1390, 'learning_rate': 3.5887e-05, 'epoch': 1.07}

05/21/2024 01:20:49 - INFO - llamafactory.extras.callbacks - {'loss': 1.0258, 'learning_rate': 3.5747e-05, 'epoch': 1.07}

05/21/2024 01:21:27 - INFO - llamafactory.extras.callbacks - {'loss': 1.0862, 'learning_rate': 3.5608e-05, 'epoch': 1.08}

05/21/2024 01:22:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.0554, 'learning_rate': 3.5467e-05, 'epoch': 1.09}

05/21/2024 01:22:49 - INFO - llamafactory.extras.callbacks - {'loss': 1.0458, 'learning_rate': 3.5326e-05, 'epoch': 1.09}

05/21/2024 01:23:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.9578, 'learning_rate': 3.5185e-05, 'epoch': 1.10}

05/21/2024 01:23:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.9460, 'learning_rate': 3.5044e-05, 'epoch': 1.10}

05/21/2024 01:24:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.9250, 'learning_rate': 3.4902e-05, 'epoch': 1.11}

05/21/2024 01:25:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.8110, 'learning_rate': 3.4760e-05, 'epoch': 1.12}

05/21/2024 01:25:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.8876, 'learning_rate': 3.4617e-05, 'epoch': 1.12}

05/21/2024 01:26:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.8122, 'learning_rate': 3.4474e-05, 'epoch': 1.13}

05/21/2024 01:27:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.7491, 'learning_rate': 3.4331e-05, 'epoch': 1.13}

05/21/2024 01:27:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.8189, 'learning_rate': 3.4187e-05, 'epoch': 1.14}

05/21/2024 01:28:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.8778, 'learning_rate': 3.4043e-05, 'epoch': 1.15}

05/21/2024 01:28:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.8075, 'learning_rate': 3.3898e-05, 'epoch': 1.15}

05/21/2024 01:29:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.8829, 'learning_rate': 3.3754e-05, 'epoch': 1.16}

05/21/2024 01:30:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.8449, 'learning_rate': 3.3609e-05, 'epoch': 1.16}

05/21/2024 01:30:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.7671, 'learning_rate': 3.3463e-05, 'epoch': 1.17}

05/21/2024 01:31:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.7357, 'learning_rate': 3.3317e-05, 'epoch': 1.17}

05/21/2024 01:32:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.7423, 'learning_rate': 3.3171e-05, 'epoch': 1.18}

05/21/2024 01:32:09 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1000

05/21/2024 01:32:10 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 01:32:10 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 01:32:10 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1000\tokenizer_config.json

05/21/2024 01:32:10 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1000\special_tokens_map.json

05/21/2024 01:32:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.6980, 'learning_rate': 3.3025e-05, 'epoch': 1.19}

05/21/2024 01:33:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.7190, 'learning_rate': 3.2878e-05, 'epoch': 1.19}

05/21/2024 01:33:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.6854, 'learning_rate': 3.2731e-05, 'epoch': 1.20}

05/21/2024 01:34:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.7277, 'learning_rate': 3.2584e-05, 'epoch': 1.20}

05/21/2024 01:35:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.7972, 'learning_rate': 3.2436e-05, 'epoch': 1.21}

05/21/2024 01:35:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.6498, 'learning_rate': 3.2289e-05, 'epoch': 1.22}

05/21/2024 01:36:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.7389, 'learning_rate': 3.2140e-05, 'epoch': 1.22}

05/21/2024 01:37:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.7443, 'learning_rate': 3.1992e-05, 'epoch': 1.23}

05/21/2024 01:37:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.6887, 'learning_rate': 3.1843e-05, 'epoch': 1.23}

05/21/2024 01:38:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.6274, 'learning_rate': 3.1694e-05, 'epoch': 1.24}

05/21/2024 01:38:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.6523, 'learning_rate': 3.1545e-05, 'epoch': 1.25}

05/21/2024 01:39:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.6191, 'learning_rate': 3.1396e-05, 'epoch': 1.25}

05/21/2024 01:40:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.6833, 'learning_rate': 3.1246e-05, 'epoch': 1.26}

05/21/2024 01:40:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.6627, 'learning_rate': 3.1096e-05, 'epoch': 1.26}

05/21/2024 01:41:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.6635, 'learning_rate': 3.0946e-05, 'epoch': 1.27}

05/21/2024 01:41:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.5780, 'learning_rate': 3.0796e-05, 'epoch': 1.28}

05/21/2024 01:42:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.6920, 'learning_rate': 3.0645e-05, 'epoch': 1.28}

05/21/2024 01:42:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.5859, 'learning_rate': 3.0494e-05, 'epoch': 1.29}

05/21/2024 01:43:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.6119, 'learning_rate': 3.0343e-05, 'epoch': 1.29}

05/21/2024 01:44:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.5850, 'learning_rate': 3.0192e-05, 'epoch': 1.30}

05/21/2024 01:44:06 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1100

05/21/2024 01:44:07 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 01:44:07 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 01:44:07 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1100\tokenizer_config.json

05/21/2024 01:44:07 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1100\special_tokens_map.json

05/21/2024 01:44:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.6131, 'learning_rate': 3.0040e-05, 'epoch': 1.30}

05/21/2024 01:45:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.5522, 'learning_rate': 2.9889e-05, 'epoch': 1.31}

05/21/2024 01:45:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.6316, 'learning_rate': 2.9737e-05, 'epoch': 1.32}

05/21/2024 01:46:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.5612, 'learning_rate': 2.9585e-05, 'epoch': 1.32}

05/21/2024 01:47:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.5884, 'learning_rate': 2.9433e-05, 'epoch': 1.33}

05/21/2024 01:47:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.5872, 'learning_rate': 2.9280e-05, 'epoch': 1.33}

05/21/2024 01:48:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.5768, 'learning_rate': 2.9128e-05, 'epoch': 1.34}

05/21/2024 01:48:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.5535, 'learning_rate': 2.8975e-05, 'epoch': 1.35}

05/21/2024 01:49:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.5767, 'learning_rate': 2.8822e-05, 'epoch': 1.35}

05/21/2024 01:49:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.5509, 'learning_rate': 2.8669e-05, 'epoch': 1.36}

05/21/2024 01:50:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.5460, 'learning_rate': 2.8516e-05, 'epoch': 1.36}

05/21/2024 01:50:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.5423, 'learning_rate': 2.8363e-05, 'epoch': 1.37}

05/21/2024 01:51:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.5225, 'learning_rate': 2.8209e-05, 'epoch': 1.38}

05/21/2024 01:51:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.5451, 'learning_rate': 2.8056e-05, 'epoch': 1.38}

05/21/2024 01:52:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.6272, 'learning_rate': 2.7902e-05, 'epoch': 1.39}

05/21/2024 01:53:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.5487, 'learning_rate': 2.7749e-05, 'epoch': 1.39}

05/21/2024 01:53:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.4681, 'learning_rate': 2.7595e-05, 'epoch': 1.40}

05/21/2024 01:54:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.4982, 'learning_rate': 2.7441e-05, 'epoch': 1.41}

05/21/2024 01:54:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.4852, 'learning_rate': 2.7287e-05, 'epoch': 1.41}

05/21/2024 01:55:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.4832, 'learning_rate': 2.7133e-05, 'epoch': 1.42}

05/21/2024 01:55:17 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1200

05/21/2024 01:55:18 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 01:55:18 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 01:55:18 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1200\tokenizer_config.json

05/21/2024 01:55:18 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1200\special_tokens_map.json

05/21/2024 01:55:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.5208, 'learning_rate': 2.6978e-05, 'epoch': 1.42}

05/21/2024 01:56:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.5115, 'learning_rate': 2.6824e-05, 'epoch': 1.43}

05/21/2024 01:57:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.5140, 'learning_rate': 2.6670e-05, 'epoch': 1.43}

05/21/2024 01:57:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.4921, 'learning_rate': 2.6515e-05, 'epoch': 1.44}

05/21/2024 01:58:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.5119, 'learning_rate': 2.6361e-05, 'epoch': 1.45}

05/21/2024 01:58:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.4915, 'learning_rate': 2.6206e-05, 'epoch': 1.45}

05/21/2024 01:59:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.4670, 'learning_rate': 2.6052e-05, 'epoch': 1.46}

05/21/2024 01:59:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.4473, 'learning_rate': 2.5897e-05, 'epoch': 1.46}

05/21/2024 02:00:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.4866, 'learning_rate': 2.5743e-05, 'epoch': 1.47}

05/21/2024 02:00:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.4489, 'learning_rate': 2.5588e-05, 'epoch': 1.48}

05/21/2024 02:01:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.4430, 'learning_rate': 2.5433e-05, 'epoch': 1.48}

05/21/2024 02:01:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.4987, 'learning_rate': 2.5279e-05, 'epoch': 1.49}

05/21/2024 02:02:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.4465, 'learning_rate': 2.5124e-05, 'epoch': 1.49}

05/21/2024 02:02:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.4187, 'learning_rate': 2.4969e-05, 'epoch': 1.50}

05/21/2024 02:03:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.4341, 'learning_rate': 2.4814e-05, 'epoch': 1.51}

05/21/2024 02:03:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.4455, 'learning_rate': 2.4660e-05, 'epoch': 1.51}

05/21/2024 02:04:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.4533, 'learning_rate': 2.4505e-05, 'epoch': 1.52}

05/21/2024 02:05:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.4326, 'learning_rate': 2.4350e-05, 'epoch': 1.52}

05/21/2024 02:05:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.4497, 'learning_rate': 2.4196e-05, 'epoch': 1.53}

05/21/2024 02:06:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.3938, 'learning_rate': 2.4041e-05, 'epoch': 1.54}

05/21/2024 02:06:24 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1300

05/21/2024 02:06:24 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 02:06:24 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 02:06:24 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1300\tokenizer_config.json

05/21/2024 02:06:24 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1300\special_tokens_map.json

05/21/2024 02:06:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.4116, 'learning_rate': 2.3886e-05, 'epoch': 1.54}

05/21/2024 02:07:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.4426, 'learning_rate': 2.3732e-05, 'epoch': 1.55}

05/21/2024 02:07:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.3517, 'learning_rate': 2.3577e-05, 'epoch': 1.55}

05/21/2024 02:08:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.3740, 'learning_rate': 2.3423e-05, 'epoch': 1.56}

05/21/2024 02:08:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.3903, 'learning_rate': 2.3268e-05, 'epoch': 1.56}

05/21/2024 02:09:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.4030, 'learning_rate': 2.3114e-05, 'epoch': 1.57}

05/21/2024 02:09:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.4403, 'learning_rate': 2.2960e-05, 'epoch': 1.58}

05/21/2024 02:10:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.3385, 'learning_rate': 2.2806e-05, 'epoch': 1.58}

05/21/2024 02:10:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.3575, 'learning_rate': 2.2652e-05, 'epoch': 1.59}

05/21/2024 02:11:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.3826, 'learning_rate': 2.2498e-05, 'epoch': 1.59}

05/21/2024 02:11:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.3681, 'learning_rate': 2.2344e-05, 'epoch': 1.60}

05/21/2024 02:12:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.3277, 'learning_rate': 2.2190e-05, 'epoch': 1.61}

05/21/2024 02:12:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.3872, 'learning_rate': 2.2036e-05, 'epoch': 1.61}

05/21/2024 02:13:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.3673, 'learning_rate': 2.1883e-05, 'epoch': 1.62}

05/21/2024 02:13:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.3413, 'learning_rate': 2.1729e-05, 'epoch': 1.62}

05/21/2024 02:14:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.3125, 'learning_rate': 2.1576e-05, 'epoch': 1.63}

05/21/2024 02:14:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.3530, 'learning_rate': 2.1423e-05, 'epoch': 1.64}

05/21/2024 02:15:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.3542, 'learning_rate': 2.1270e-05, 'epoch': 1.64}

05/21/2024 02:15:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.2987, 'learning_rate': 2.1117e-05, 'epoch': 1.65}

05/21/2024 02:16:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.3599, 'learning_rate': 2.0964e-05, 'epoch': 1.65}

05/21/2024 02:16:14 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1400

05/21/2024 02:16:15 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 02:16:15 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 02:16:15 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1400\tokenizer_config.json

05/21/2024 02:16:15 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1400\special_tokens_map.json

05/21/2024 02:16:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.3854, 'learning_rate': 2.0811e-05, 'epoch': 1.66}

05/21/2024 02:17:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.3753, 'learning_rate': 2.0659e-05, 'epoch': 1.66}

05/21/2024 02:17:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.2997, 'learning_rate': 2.0507e-05, 'epoch': 1.67}

05/21/2024 02:18:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.2897, 'learning_rate': 2.0354e-05, 'epoch': 1.68}

05/21/2024 02:18:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.3111, 'learning_rate': 2.0202e-05, 'epoch': 1.68}

05/21/2024 02:19:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.3064, 'learning_rate': 2.0051e-05, 'epoch': 1.69}

05/21/2024 02:19:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.3182, 'learning_rate': 1.9899e-05, 'epoch': 1.69}

05/21/2024 02:20:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.2891, 'learning_rate': 1.9748e-05, 'epoch': 1.70}

05/21/2024 02:20:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.3059, 'learning_rate': 1.9597e-05, 'epoch': 1.71}

05/21/2024 02:20:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.2711, 'learning_rate': 1.9446e-05, 'epoch': 1.71}

05/21/2024 02:21:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.3032, 'learning_rate': 1.9295e-05, 'epoch': 1.72}

05/21/2024 02:21:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.2927, 'learning_rate': 1.9144e-05, 'epoch': 1.72}

05/21/2024 02:22:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.3722, 'learning_rate': 1.8994e-05, 'epoch': 1.73}

05/21/2024 02:22:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.2750, 'learning_rate': 1.8844e-05, 'epoch': 1.74}

05/21/2024 02:23:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.3091, 'learning_rate': 1.8694e-05, 'epoch': 1.74}

05/21/2024 02:23:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.3434, 'learning_rate': 1.8544e-05, 'epoch': 1.75}

05/21/2024 02:24:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.2775, 'learning_rate': 1.8395e-05, 'epoch': 1.75}

05/21/2024 02:24:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.2590, 'learning_rate': 1.8246e-05, 'epoch': 1.76}

05/21/2024 02:25:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.3270, 'learning_rate': 1.8097e-05, 'epoch': 1.77}

05/21/2024 02:25:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.2586, 'learning_rate': 1.7949e-05, 'epoch': 1.77}

05/21/2024 02:25:32 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1500

05/21/2024 02:25:33 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 02:25:33 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 02:25:33 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1500\tokenizer_config.json

05/21/2024 02:25:33 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1500\special_tokens_map.json

05/21/2024 02:26:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.2829, 'learning_rate': 1.7800e-05, 'epoch': 1.78}

05/21/2024 02:26:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.2675, 'learning_rate': 1.7652e-05, 'epoch': 1.78}

05/21/2024 02:26:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.2673, 'learning_rate': 1.7504e-05, 'epoch': 1.79}

05/21/2024 02:27:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.2844, 'learning_rate': 1.7357e-05, 'epoch': 1.79}

05/21/2024 02:27:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.3112, 'learning_rate': 1.7210e-05, 'epoch': 1.80}

05/21/2024 02:28:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.2655, 'learning_rate': 1.7063e-05, 'epoch': 1.81}

05/21/2024 02:28:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.2907, 'learning_rate': 1.6916e-05, 'epoch': 1.81}

05/21/2024 02:29:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.3175, 'learning_rate': 1.6770e-05, 'epoch': 1.82}

05/21/2024 02:29:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.2854, 'learning_rate': 1.6624e-05, 'epoch': 1.82}

05/21/2024 02:29:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.2312, 'learning_rate': 1.6479e-05, 'epoch': 1.83}

05/21/2024 02:30:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.2756, 'learning_rate': 1.6333e-05, 'epoch': 1.84}

05/21/2024 02:30:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.2913, 'learning_rate': 1.6188e-05, 'epoch': 1.84}

05/21/2024 02:31:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.2403, 'learning_rate': 1.6044e-05, 'epoch': 1.85}

05/21/2024 02:31:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.2414, 'learning_rate': 1.5899e-05, 'epoch': 1.85}

05/21/2024 02:32:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.2301, 'learning_rate': 1.5755e-05, 'epoch': 1.86}

05/21/2024 02:32:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.2777, 'learning_rate': 1.5612e-05, 'epoch': 1.87}

05/21/2024 02:33:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.2396, 'learning_rate': 1.5469e-05, 'epoch': 1.87}

05/21/2024 02:33:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.2304, 'learning_rate': 1.5326e-05, 'epoch': 1.88}

05/21/2024 02:34:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2916, 'learning_rate': 1.5183e-05, 'epoch': 1.88}

05/21/2024 02:34:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.2791, 'learning_rate': 1.5041e-05, 'epoch': 1.89}

05/21/2024 02:34:28 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1600

05/21/2024 02:34:28 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 02:34:28 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 02:34:29 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1600\tokenizer_config.json

05/21/2024 02:34:29 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1600\special_tokens_map.json

05/21/2024 02:34:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.2788, 'learning_rate': 1.4899e-05, 'epoch': 1.90}

05/21/2024 02:35:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2690, 'learning_rate': 1.4758e-05, 'epoch': 1.90}

05/21/2024 02:35:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.2540, 'learning_rate': 1.4617e-05, 'epoch': 1.91}

05/21/2024 02:36:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.2809, 'learning_rate': 1.4477e-05, 'epoch': 1.91}

05/21/2024 02:36:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2748, 'learning_rate': 1.4336e-05, 'epoch': 1.92}

05/21/2024 02:37:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2974, 'learning_rate': 1.4197e-05, 'epoch': 1.92}

05/21/2024 02:37:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.2686, 'learning_rate': 1.4057e-05, 'epoch': 1.93}

05/21/2024 02:37:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.2523, 'learning_rate': 1.3919e-05, 'epoch': 1.94}

05/21/2024 02:38:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.2429, 'learning_rate': 1.3780e-05, 'epoch': 1.94}

05/21/2024 02:38:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.2513, 'learning_rate': 1.3642e-05, 'epoch': 1.95}

05/21/2024 02:39:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.2392, 'learning_rate': 1.3504e-05, 'epoch': 1.95}

05/21/2024 02:39:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.2400, 'learning_rate': 1.3367e-05, 'epoch': 1.96}

05/21/2024 02:39:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.2554, 'learning_rate': 1.3230e-05, 'epoch': 1.97}

05/21/2024 02:40:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.2279, 'learning_rate': 1.3094e-05, 'epoch': 1.97}

05/21/2024 02:40:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.2178, 'learning_rate': 1.2958e-05, 'epoch': 1.98}

05/21/2024 02:41:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.2491, 'learning_rate': 1.2823e-05, 'epoch': 1.98}

05/21/2024 02:41:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.2771, 'learning_rate': 1.2688e-05, 'epoch': 1.99}

05/21/2024 02:42:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.2851, 'learning_rate': 1.2554e-05, 'epoch': 2.00}

05/21/2024 02:42:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.2348, 'learning_rate': 1.2420e-05, 'epoch': 2.00}

05/21/2024 02:43:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.2181, 'learning_rate': 1.2286e-05, 'epoch': 2.01}

05/21/2024 02:43:01 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1700

05/21/2024 02:43:02 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 02:43:02 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 02:43:02 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1700\tokenizer_config.json

05/21/2024 02:43:02 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1700\special_tokens_map.json

05/21/2024 02:43:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.2348, 'learning_rate': 1.2153e-05, 'epoch': 2.01}

05/21/2024 02:43:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.2818, 'learning_rate': 1.2021e-05, 'epoch': 2.02}

05/21/2024 02:44:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.2155, 'learning_rate': 1.1889e-05, 'epoch': 2.03}

05/21/2024 02:44:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.2433, 'learning_rate': 1.1757e-05, 'epoch': 2.03}

05/21/2024 02:45:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.2951, 'learning_rate': 1.1626e-05, 'epoch': 2.04}

05/21/2024 02:45:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.2205, 'learning_rate': 1.1496e-05, 'epoch': 2.04}

05/21/2024 02:45:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.2394, 'learning_rate': 1.1366e-05, 'epoch': 2.05}

05/21/2024 02:46:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.2484, 'learning_rate': 1.1236e-05, 'epoch': 2.05}

05/21/2024 02:46:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.2138, 'learning_rate': 1.1108e-05, 'epoch': 2.06}

05/21/2024 02:46:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.2807, 'learning_rate': 1.0979e-05, 'epoch': 2.07}

05/21/2024 02:47:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.2467, 'learning_rate': 1.0851e-05, 'epoch': 2.07}

05/21/2024 02:47:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2820, 'learning_rate': 1.0724e-05, 'epoch': 2.08}

05/21/2024 02:48:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2387, 'learning_rate': 1.0597e-05, 'epoch': 2.08}

05/21/2024 02:48:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.2288, 'learning_rate': 1.0471e-05, 'epoch': 2.09}

05/21/2024 02:48:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.2704, 'learning_rate': 1.0345e-05, 'epoch': 2.10}

05/21/2024 02:49:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.2319, 'learning_rate': 1.0220e-05, 'epoch': 2.10}

05/21/2024 02:49:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.2360, 'learning_rate': 1.0096e-05, 'epoch': 2.11}

05/21/2024 02:49:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.2542, 'learning_rate': 9.9719e-06, 'epoch': 2.11}

05/21/2024 02:50:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2383, 'learning_rate': 9.8486e-06, 'epoch': 2.12}

05/21/2024 02:50:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.2504, 'learning_rate': 9.7258e-06, 'epoch': 2.13}

05/21/2024 02:50:46 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1800

05/21/2024 02:50:46 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 02:50:46 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 02:50:46 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1800\tokenizer_config.json

05/21/2024 02:50:46 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1800\special_tokens_map.json

05/21/2024 02:51:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.2428, 'learning_rate': 9.6036e-06, 'epoch': 2.13}

05/21/2024 02:52:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.2278, 'learning_rate': 9.4820e-06, 'epoch': 2.14}

05/21/2024 02:53:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.2167, 'learning_rate': 9.3610e-06, 'epoch': 2.14}

05/21/2024 02:53:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.2482, 'learning_rate': 9.2406e-06, 'epoch': 2.15}

05/21/2024 02:54:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.2676, 'learning_rate': 9.1207e-06, 'epoch': 2.15}

05/21/2024 02:55:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.2114, 'learning_rate': 9.0015e-06, 'epoch': 2.16}

05/21/2024 02:56:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.1965, 'learning_rate': 8.8829e-06, 'epoch': 2.17}

05/21/2024 02:56:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.2982, 'learning_rate': 8.7650e-06, 'epoch': 2.17}

05/21/2024 02:57:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.2305, 'learning_rate': 8.6476e-06, 'epoch': 2.18}

05/21/2024 02:58:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.2880, 'learning_rate': 8.5309e-06, 'epoch': 2.18}

05/21/2024 02:58:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.2381, 'learning_rate': 8.4148e-06, 'epoch': 2.19}

05/21/2024 02:59:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.2007, 'learning_rate': 8.2993e-06, 'epoch': 2.20}

05/21/2024 03:00:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.2454, 'learning_rate': 8.1845e-06, 'epoch': 2.20}

05/21/2024 03:00:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.2429, 'learning_rate': 8.0704e-06, 'epoch': 2.21}

05/21/2024 03:01:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.2133, 'learning_rate': 7.9568e-06, 'epoch': 2.21}

05/21/2024 03:01:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.2294, 'learning_rate': 7.8440e-06, 'epoch': 2.22}

05/21/2024 03:02:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.2418, 'learning_rate': 7.7317e-06, 'epoch': 2.23}

05/21/2024 03:03:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.2723, 'learning_rate': 7.6202e-06, 'epoch': 2.23}

05/21/2024 03:03:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.2697, 'learning_rate': 7.5093e-06, 'epoch': 2.24}

05/21/2024 03:04:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.2310, 'learning_rate': 7.3991e-06, 'epoch': 2.24}

05/21/2024 03:04:25 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1900

05/21/2024 03:04:26 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 03:04:26 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 03:04:26 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1900\tokenizer_config.json

05/21/2024 03:04:26 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-1900\special_tokens_map.json

05/21/2024 03:05:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.2524, 'learning_rate': 7.2895e-06, 'epoch': 2.25}

05/21/2024 03:05:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2182, 'learning_rate': 7.1807e-06, 'epoch': 2.26}

05/21/2024 03:06:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.2649, 'learning_rate': 7.0725e-06, 'epoch': 2.26}

05/21/2024 03:06:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.2340, 'learning_rate': 6.9650e-06, 'epoch': 2.27}

05/21/2024 03:07:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.2202, 'learning_rate': 6.8582e-06, 'epoch': 2.27}

05/21/2024 03:08:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.2238, 'learning_rate': 6.7521e-06, 'epoch': 2.28}

05/21/2024 03:08:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.2555, 'learning_rate': 6.6467e-06, 'epoch': 2.28}

05/21/2024 03:09:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.2131, 'learning_rate': 6.5420e-06, 'epoch': 2.29}

05/21/2024 03:09:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.2238, 'learning_rate': 6.4380e-06, 'epoch': 2.30}

05/21/2024 03:10:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.2056, 'learning_rate': 6.3347e-06, 'epoch': 2.30}

05/21/2024 03:11:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.2110, 'learning_rate': 6.2321e-06, 'epoch': 2.31}

05/21/2024 03:11:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.2348, 'learning_rate': 6.1302e-06, 'epoch': 2.31}

05/21/2024 03:12:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.2658, 'learning_rate': 6.0291e-06, 'epoch': 2.32}

05/21/2024 03:13:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.2372, 'learning_rate': 5.9287e-06, 'epoch': 2.33}

05/21/2024 03:13:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.2355, 'learning_rate': 5.8290e-06, 'epoch': 2.33}

05/21/2024 03:14:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.2167, 'learning_rate': 5.7301e-06, 'epoch': 2.34}

05/21/2024 03:15:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.2251, 'learning_rate': 5.6319e-06, 'epoch': 2.34}

05/21/2024 03:15:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.2121, 'learning_rate': 5.5344e-06, 'epoch': 2.35}

05/21/2024 03:16:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.2234, 'learning_rate': 5.4377e-06, 'epoch': 2.36}

05/21/2024 03:16:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.2282, 'learning_rate': 5.3417e-06, 'epoch': 2.36}

05/21/2024 03:16:47 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2000

05/21/2024 03:16:48 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 03:16:48 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 03:16:48 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2000\tokenizer_config.json

05/21/2024 03:16:48 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2000\special_tokens_map.json

05/21/2024 03:17:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.2291, 'learning_rate': 5.2465e-06, 'epoch': 2.37}

05/21/2024 03:18:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.2348, 'learning_rate': 5.1520e-06, 'epoch': 2.37}

05/21/2024 03:18:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.2376, 'learning_rate': 5.0583e-06, 'epoch': 2.38}

05/21/2024 03:19:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.2280, 'learning_rate': 4.9654e-06, 'epoch': 2.39}

05/21/2024 03:19:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2475, 'learning_rate': 4.8732e-06, 'epoch': 2.39}

05/21/2024 03:20:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.2430, 'learning_rate': 4.7819e-06, 'epoch': 2.40}

05/21/2024 03:20:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.2767, 'learning_rate': 4.6912e-06, 'epoch': 2.40}

05/21/2024 03:21:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.2573, 'learning_rate': 4.6014e-06, 'epoch': 2.41}

05/21/2024 03:21:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.2412, 'learning_rate': 4.5123e-06, 'epoch': 2.41}

05/21/2024 03:22:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.3103, 'learning_rate': 4.4241e-06, 'epoch': 2.42}

05/21/2024 03:23:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.2229, 'learning_rate': 4.3366e-06, 'epoch': 2.43}

05/21/2024 03:23:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.2342, 'learning_rate': 4.2499e-06, 'epoch': 2.43}

05/21/2024 03:24:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.2415, 'learning_rate': 4.1640e-06, 'epoch': 2.44}

05/21/2024 03:24:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.2096, 'learning_rate': 4.0789e-06, 'epoch': 2.44}

05/21/2024 03:25:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.2780, 'learning_rate': 3.9946e-06, 'epoch': 2.45}

05/21/2024 03:26:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.2520, 'learning_rate': 3.9111e-06, 'epoch': 2.46}

05/21/2024 03:26:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.2105, 'learning_rate': 3.8284e-06, 'epoch': 2.46}

05/21/2024 03:27:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.2312, 'learning_rate': 3.7465e-06, 'epoch': 2.47}

05/21/2024 03:27:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.2130, 'learning_rate': 3.6654e-06, 'epoch': 2.47}

05/21/2024 03:28:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.2210, 'learning_rate': 3.5852e-06, 'epoch': 2.48}

05/21/2024 03:28:12 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2100

05/21/2024 03:28:12 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 03:28:12 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 03:28:12 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2100\tokenizer_config.json

05/21/2024 03:28:12 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2100\special_tokens_map.json

05/21/2024 03:28:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.2008, 'learning_rate': 3.5057e-06, 'epoch': 2.49}

05/21/2024 03:29:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.2262, 'learning_rate': 3.4271e-06, 'epoch': 2.49}

05/21/2024 03:29:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.2243, 'learning_rate': 3.3494e-06, 'epoch': 2.50}

05/21/2024 03:30:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.2279, 'learning_rate': 3.2724e-06, 'epoch': 2.50}

05/21/2024 03:30:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.2278, 'learning_rate': 3.1963e-06, 'epoch': 2.51}

05/21/2024 03:31:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.2235, 'learning_rate': 3.1210e-06, 'epoch': 2.52}

05/21/2024 03:31:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.2106, 'learning_rate': 3.0466e-06, 'epoch': 2.52}

05/21/2024 03:32:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.2118, 'learning_rate': 2.9730e-06, 'epoch': 2.53}

05/21/2024 03:32:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.2298, 'learning_rate': 2.9002e-06, 'epoch': 2.53}

05/21/2024 03:33:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.2595, 'learning_rate': 2.8283e-06, 'epoch': 2.54}

05/21/2024 03:33:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.2296, 'learning_rate': 2.7572e-06, 'epoch': 2.54}

05/21/2024 03:34:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.2525, 'learning_rate': 2.6870e-06, 'epoch': 2.55}

05/21/2024 03:34:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.2350, 'learning_rate': 2.6177e-06, 'epoch': 2.56}

05/21/2024 03:35:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.2271, 'learning_rate': 2.5492e-06, 'epoch': 2.56}

05/21/2024 03:35:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.2156, 'learning_rate': 2.4815e-06, 'epoch': 2.57}

05/21/2024 03:36:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.2092, 'learning_rate': 2.4148e-06, 'epoch': 2.57}

05/21/2024 03:36:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.2099, 'learning_rate': 2.3488e-06, 'epoch': 2.58}

05/21/2024 03:37:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.2637, 'learning_rate': 2.2838e-06, 'epoch': 2.59}

05/21/2024 03:37:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.2112, 'learning_rate': 2.2196e-06, 'epoch': 2.59}

05/21/2024 03:38:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.2035, 'learning_rate': 2.1563e-06, 'epoch': 2.60}

05/21/2024 03:38:29 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2200

05/21/2024 03:38:30 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 03:38:30 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 03:38:30 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2200\tokenizer_config.json

05/21/2024 03:38:30 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2200\special_tokens_map.json

05/21/2024 03:39:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2088, 'learning_rate': 2.0939e-06, 'epoch': 2.60}

05/21/2024 03:39:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.2045, 'learning_rate': 2.0324e-06, 'epoch': 2.61}

05/21/2024 03:40:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.2306, 'learning_rate': 1.9717e-06, 'epoch': 2.62}

05/21/2024 03:40:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.2348, 'learning_rate': 1.9119e-06, 'epoch': 2.62}

05/21/2024 03:41:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.2153, 'learning_rate': 1.8530e-06, 'epoch': 2.63}

05/21/2024 03:41:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.2488, 'learning_rate': 1.7950e-06, 'epoch': 2.63}

05/21/2024 03:42:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.2372, 'learning_rate': 1.7379e-06, 'epoch': 2.64}

05/21/2024 03:42:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2040, 'learning_rate': 1.6816e-06, 'epoch': 2.65}

05/21/2024 03:43:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.2301, 'learning_rate': 1.6263e-06, 'epoch': 2.65}

05/21/2024 03:43:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.2190, 'learning_rate': 1.5718e-06, 'epoch': 2.66}

05/21/2024 03:44:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.2265, 'learning_rate': 1.5183e-06, 'epoch': 2.66}

05/21/2024 03:44:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.2112, 'learning_rate': 1.4656e-06, 'epoch': 2.67}

05/21/2024 03:45:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.2851, 'learning_rate': 1.4139e-06, 'epoch': 2.67}

05/21/2024 03:45:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.2339, 'learning_rate': 1.3630e-06, 'epoch': 2.68}

05/21/2024 03:46:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2607, 'learning_rate': 1.3131e-06, 'epoch': 2.69}

05/21/2024 03:46:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.2345, 'learning_rate': 1.2641e-06, 'epoch': 2.69}

05/21/2024 03:47:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.2906, 'learning_rate': 1.2159e-06, 'epoch': 2.70}

05/21/2024 03:47:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.2109, 'learning_rate': 1.1687e-06, 'epoch': 2.70}

05/21/2024 03:48:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.2751, 'learning_rate': 1.1224e-06, 'epoch': 2.71}

05/21/2024 03:48:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.2143, 'learning_rate': 1.0771e-06, 'epoch': 2.72}

05/21/2024 03:48:31 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2300

05/21/2024 03:48:32 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 03:48:32 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 03:48:32 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2300\tokenizer_config.json

05/21/2024 03:48:32 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2300\special_tokens_map.json

05/21/2024 03:49:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.2088, 'learning_rate': 1.0326e-06, 'epoch': 2.72}

05/21/2024 03:49:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.2131, 'learning_rate': 9.8904e-07, 'epoch': 2.73}

05/21/2024 03:49:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.2132, 'learning_rate': 9.4640e-07, 'epoch': 2.73}

05/21/2024 03:50:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.2065, 'learning_rate': 9.0470e-07, 'epoch': 2.74}

05/21/2024 03:50:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.2060, 'learning_rate': 8.6391e-07, 'epoch': 2.75}

05/21/2024 03:51:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.2286, 'learning_rate': 8.2405e-07, 'epoch': 2.75}

05/21/2024 03:51:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.2358, 'learning_rate': 7.8511e-07, 'epoch': 2.76}

05/21/2024 03:52:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.2031, 'learning_rate': 7.4711e-07, 'epoch': 2.76}

05/21/2024 03:52:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.2197, 'learning_rate': 7.1003e-07, 'epoch': 2.77}

05/21/2024 03:53:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.2369, 'learning_rate': 6.7388e-07, 'epoch': 2.77}

05/21/2024 03:53:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.2196, 'learning_rate': 6.3866e-07, 'epoch': 2.78}

05/21/2024 03:54:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.1999, 'learning_rate': 6.0438e-07, 'epoch': 2.79}

05/21/2024 03:54:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.2024, 'learning_rate': 5.7103e-07, 'epoch': 2.79}

05/21/2024 03:55:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.3083, 'learning_rate': 5.3862e-07, 'epoch': 2.80}

05/21/2024 03:55:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.2144, 'learning_rate': 5.0714e-07, 'epoch': 2.80}

05/21/2024 03:56:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2252, 'learning_rate': 4.7660e-07, 'epoch': 2.81}

05/21/2024 03:56:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.2379, 'learning_rate': 4.4700e-07, 'epoch': 2.82}

05/21/2024 03:57:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.2078, 'learning_rate': 4.1835e-07, 'epoch': 2.82}

05/21/2024 03:57:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.2075, 'learning_rate': 3.9063e-07, 'epoch': 2.83}

05/21/2024 03:58:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2372, 'learning_rate': 3.6386e-07, 'epoch': 2.83}

05/21/2024 03:58:19 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2400

05/21/2024 03:58:20 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 03:58:20 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 03:58:20 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2400\tokenizer_config.json

05/21/2024 03:58:20 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2400\special_tokens_map.json

05/21/2024 03:58:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.2343, 'learning_rate': 3.3802e-07, 'epoch': 2.84}

05/21/2024 03:59:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.2081, 'learning_rate': 3.1314e-07, 'epoch': 2.85}

05/21/2024 03:59:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.2234, 'learning_rate': 2.8920e-07, 'epoch': 2.85}

05/21/2024 04:00:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.2488, 'learning_rate': 2.6621e-07, 'epoch': 2.86}

05/21/2024 04:00:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.2266, 'learning_rate': 2.4416e-07, 'epoch': 2.86}

05/21/2024 04:01:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.2077, 'learning_rate': 2.2306e-07, 'epoch': 2.87}

05/21/2024 04:01:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.2231, 'learning_rate': 2.0291e-07, 'epoch': 2.88}

05/21/2024 04:02:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2272, 'learning_rate': 1.8372e-07, 'epoch': 2.88}

05/21/2024 04:02:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.2286, 'learning_rate': 1.6547e-07, 'epoch': 2.89}

05/21/2024 04:03:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.2155, 'learning_rate': 1.4817e-07, 'epoch': 2.89}

05/21/2024 04:03:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.2135, 'learning_rate': 1.3183e-07, 'epoch': 2.90}

05/21/2024 04:03:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.2060, 'learning_rate': 1.1643e-07, 'epoch': 2.90}

05/21/2024 04:04:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.2493, 'learning_rate': 1.0199e-07, 'epoch': 2.91}

05/21/2024 04:04:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.2105, 'learning_rate': 8.8509e-08, 'epoch': 2.92}

05/21/2024 04:05:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2398, 'learning_rate': 7.5978e-08, 'epoch': 2.92}

05/21/2024 04:05:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.2510, 'learning_rate': 6.4401e-08, 'epoch': 2.93}

05/21/2024 04:06:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.2031, 'learning_rate': 5.3780e-08, 'epoch': 2.93}

05/21/2024 04:06:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.2175, 'learning_rate': 4.4114e-08, 'epoch': 2.94}

05/21/2024 04:07:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.2552, 'learning_rate': 3.5405e-08, 'epoch': 2.95}

05/21/2024 04:07:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.2052, 'learning_rate': 2.7651e-08, 'epoch': 2.95}

05/21/2024 04:07:31 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2500

05/21/2024 04:07:31 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 04:07:31 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 04:07:31 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2500\tokenizer_config.json

05/21/2024 04:07:31 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\checkpoint-2500\special_tokens_map.json

05/21/2024 04:07:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.3177, 'learning_rate': 2.0854e-08, 'epoch': 2.96}

05/21/2024 04:08:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.2709, 'learning_rate': 1.5014e-08, 'epoch': 2.96}

05/21/2024 04:08:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.2536, 'learning_rate': 1.0131e-08, 'epoch': 2.97}

05/21/2024 04:09:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2578, 'learning_rate': 6.2052e-09, 'epoch': 2.98}

05/21/2024 04:09:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.2276, 'learning_rate': 3.2367e-09, 'epoch': 2.98}

05/21/2024 04:10:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.2906, 'learning_rate': 1.2258e-09, 'epoch': 2.99}

05/21/2024 04:10:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.1918, 'learning_rate': 1.7237e-10, 'epoch': 2.99}

05/21/2024 04:10:53 - INFO - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)



05/21/2024 04:10:53 - INFO - transformers.trainer - Saving model checkpoint to saves\Gemma-2B\lora\train_2024-05-20-23-44-24

05/21/2024 04:10:53 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--google--gemma-2b\snapshots\2ac59a5d7bf4e1425010f0d457dde7d146658953\config.json

05/21/2024 04:10:53 - INFO - transformers.configuration_utils - Model config GemmaConfig {
  "architectures": [
    "GemmaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "eos_token_id": 1,
  "head_dim": 256,
  "hidden_act": "gelu",
  "hidden_activation": null,
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 16384,
  "max_position_embeddings": 8192,
  "model_type": "gemma",
  "num_attention_heads": 8,
  "num_hidden_layers": 18,
  "num_key_value_heads": 1,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 256000
}


05/21/2024 04:10:53 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\tokenizer_config.json

05/21/2024 04:10:53 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Gemma-2B\lora\train_2024-05-20-23-44-24\special_tokens_map.json

05/21/2024 04:10:54 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.

05/21/2024 04:10:54 - INFO - transformers.modelcard - Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

