05/18/2024 22:01:14 - INFO - transformers.tokenization_utils_base - loading file vocab.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\vocab.json

05/18/2024 22:01:14 - INFO - transformers.tokenization_utils_base - loading file merges.txt from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\merges.txt

05/18/2024 22:01:14 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\tokenizer.json

05/18/2024 22:01:14 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json from cache at None

05/18/2024 22:01:14 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json from cache at None

05/18/2024 22:01:14 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\tokenizer_config.json

05/18/2024 22:01:14 - WARNING - transformers.tokenization_utils_base - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

05/18/2024 22:01:14 - INFO - llamafactory.data.loader - Loading dataset QuestionTest.json...

05/18/2024 22:01:59 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:01:59 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "_name_or_path": "Qwen/Qwen1.5-0.5B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:01:59 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.

05/18/2024 22:05:29 - INFO - transformers.modeling_utils - loading weights file model.safetensors from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\model.safetensors

05/18/2024 22:05:29 - INFO - transformers.modeling_utils - Instantiating Qwen2ForCausalLM model under default dtype torch.float16.

05/18/2024 22:05:29 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643
}


05/18/2024 22:05:31 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing Qwen2ForCausalLM.


05/18/2024 22:05:31 - INFO - transformers.modeling_utils - All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen1.5-0.5B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.

05/18/2024 22:05:32 - INFO - transformers.generation.configuration_utils - loading configuration file generation_config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\generation_config.json

05/18/2024 22:05:32 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "max_new_tokens": 2048
}


05/18/2024 22:05:32 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.

05/18/2024 22:05:32 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.

05/18/2024 22:05:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.

05/18/2024 22:05:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

05/18/2024 22:05:32 - INFO - llamafactory.model.loader - trainable params: 786432 || all params: 464774144 || trainable%: 0.1692

05/18/2024 22:05:32 - INFO - transformers.trainer - Using auto half precision backend

05/18/2024 22:05:32 - INFO - transformers.trainer - ***** Running training *****

05/18/2024 22:05:32 - INFO - transformers.trainer -   Num examples = 10,840

05/18/2024 22:05:32 - INFO - transformers.trainer -   Num Epochs = 3

05/18/2024 22:05:32 - INFO - transformers.trainer -   Instantaneous batch size per device = 1

05/18/2024 22:05:32 - INFO - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 8

05/18/2024 22:05:32 - INFO - transformers.trainer -   Gradient Accumulation steps = 8

05/18/2024 22:05:32 - INFO - transformers.trainer -   Total optimization steps = 4,065

05/18/2024 22:05:32 - INFO - transformers.trainer -   Number of trainable parameters = 786,432

05/18/2024 22:05:42 - INFO - llamafactory.extras.callbacks - {'loss': 2.4632, 'learning_rate': 5.0000e-05, 'epoch': 0.00}

05/18/2024 22:05:51 - INFO - llamafactory.extras.callbacks - {'loss': 1.9044, 'learning_rate': 4.9999e-05, 'epoch': 0.01}

05/18/2024 22:06:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.4153, 'learning_rate': 4.9998e-05, 'epoch': 0.01}

05/18/2024 22:06:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.8808, 'learning_rate': 4.9997e-05, 'epoch': 0.01}

05/18/2024 22:06:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.4886, 'learning_rate': 4.9996e-05, 'epoch': 0.02}

05/18/2024 22:06:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.3175, 'learning_rate': 4.9994e-05, 'epoch': 0.02}

05/18/2024 22:06:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2068, 'learning_rate': 4.9991e-05, 'epoch': 0.03}

05/18/2024 22:06:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.1921, 'learning_rate': 4.9989e-05, 'epoch': 0.03}

05/18/2024 22:06:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.2351, 'learning_rate': 4.9986e-05, 'epoch': 0.03}

05/18/2024 22:07:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.1871, 'learning_rate': 4.9982e-05, 'epoch': 0.04}

05/18/2024 22:07:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.2402, 'learning_rate': 4.9978e-05, 'epoch': 0.04}

05/18/2024 22:07:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.1880, 'learning_rate': 4.9974e-05, 'epoch': 0.04}

05/18/2024 22:07:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.1909, 'learning_rate': 4.9969e-05, 'epoch': 0.05}

05/18/2024 22:07:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.1847, 'learning_rate': 4.9964e-05, 'epoch': 0.05}

05/18/2024 22:07:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.2096, 'learning_rate': 4.9959e-05, 'epoch': 0.06}

05/18/2024 22:08:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.1822, 'learning_rate': 4.9953e-05, 'epoch': 0.06}

05/18/2024 22:08:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.1942, 'learning_rate': 4.9947e-05, 'epoch': 0.06}

05/18/2024 22:08:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.2034, 'learning_rate': 4.9941e-05, 'epoch': 0.07}

05/18/2024 22:08:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.1575, 'learning_rate': 4.9934e-05, 'epoch': 0.07}

05/18/2024 22:08:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.2282, 'learning_rate': 4.9927e-05, 'epoch': 0.07}

05/18/2024 22:08:37 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:08:37 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:08:37 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:11:53 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-100

05/18/2024 22:11:54 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:11:54 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:11:54 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-100\tokenizer_config.json

05/18/2024 22:11:54 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-100\special_tokens_map.json

05/18/2024 22:12:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.1422, 'learning_rate': 4.9919e-05, 'epoch': 0.08}

05/18/2024 22:12:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.1651, 'learning_rate': 4.9911e-05, 'epoch': 0.08}

05/18/2024 22:12:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.1370, 'learning_rate': 4.9903e-05, 'epoch': 0.08}

05/18/2024 22:12:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.1565, 'learning_rate': 4.9894e-05, 'epoch': 0.09}

05/18/2024 22:12:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.1732, 'learning_rate': 4.9885e-05, 'epoch': 0.09}

05/18/2024 22:12:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.1258, 'learning_rate': 4.9876e-05, 'epoch': 0.10}

05/18/2024 22:12:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.1264, 'learning_rate': 4.9866e-05, 'epoch': 0.10}

05/18/2024 22:13:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.1696, 'learning_rate': 4.9856e-05, 'epoch': 0.10}

05/18/2024 22:13:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.1542, 'learning_rate': 4.9845e-05, 'epoch': 0.11}

05/18/2024 22:13:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.1512, 'learning_rate': 4.9834e-05, 'epoch': 0.11}

05/18/2024 22:13:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.1443, 'learning_rate': 4.9823e-05, 'epoch': 0.11}

05/18/2024 22:13:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.1001, 'learning_rate': 4.9811e-05, 'epoch': 0.12}

05/18/2024 22:13:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.1128, 'learning_rate': 4.9799e-05, 'epoch': 0.12}

05/18/2024 22:14:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.1208, 'learning_rate': 4.9787e-05, 'epoch': 0.13}

05/18/2024 22:14:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0870, 'learning_rate': 4.9774e-05, 'epoch': 0.13}

05/18/2024 22:14:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.1454, 'learning_rate': 4.9761e-05, 'epoch': 0.13}

05/18/2024 22:14:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.1773, 'learning_rate': 4.9748e-05, 'epoch': 0.14}

05/18/2024 22:14:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.1373, 'learning_rate': 4.9734e-05, 'epoch': 0.14}

05/18/2024 22:14:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.1657, 'learning_rate': 4.9720e-05, 'epoch': 0.14}

05/18/2024 22:14:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.1073, 'learning_rate': 4.9705e-05, 'epoch': 0.15}

05/18/2024 22:14:57 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:14:57 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:14:57 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:18:15 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-200

05/18/2024 22:18:16 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:18:16 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:18:16 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-200\tokenizer_config.json

05/18/2024 22:18:16 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-200\special_tokens_map.json

05/18/2024 22:18:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.1065, 'learning_rate': 4.9690e-05, 'epoch': 0.15}

05/18/2024 22:18:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.1013, 'learning_rate': 4.9675e-05, 'epoch': 0.15}

05/18/2024 22:18:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0997, 'learning_rate': 4.9659e-05, 'epoch': 0.16}

05/18/2024 22:18:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.1336, 'learning_rate': 4.9643e-05, 'epoch': 0.16}

05/18/2024 22:19:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0751, 'learning_rate': 4.9626e-05, 'epoch': 0.17}

05/18/2024 22:19:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.1385, 'learning_rate': 4.9609e-05, 'epoch': 0.17}

05/18/2024 22:19:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.0811, 'learning_rate': 4.9592e-05, 'epoch': 0.17}

05/18/2024 22:19:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.1049, 'learning_rate': 4.9575e-05, 'epoch': 0.18}

05/18/2024 22:19:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.1285, 'learning_rate': 4.9557e-05, 'epoch': 0.18}

05/18/2024 22:19:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.1298, 'learning_rate': 4.9539e-05, 'epoch': 0.18}

05/18/2024 22:20:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.1300, 'learning_rate': 4.9520e-05, 'epoch': 0.19}

05/18/2024 22:20:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0951, 'learning_rate': 4.9501e-05, 'epoch': 0.19}

05/18/2024 22:20:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0747, 'learning_rate': 4.9481e-05, 'epoch': 0.20}

05/18/2024 22:20:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.1084, 'learning_rate': 4.9462e-05, 'epoch': 0.20}

05/18/2024 22:20:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0897, 'learning_rate': 4.9442e-05, 'epoch': 0.20}

05/18/2024 22:20:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.1340, 'learning_rate': 4.9421e-05, 'epoch': 0.21}

05/18/2024 22:21:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.1349, 'learning_rate': 4.9400e-05, 'epoch': 0.21}

05/18/2024 22:21:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.1206, 'learning_rate': 4.9379e-05, 'epoch': 0.21}

05/18/2024 22:21:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.1061, 'learning_rate': 4.9357e-05, 'epoch': 0.22}

05/18/2024 22:21:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0963, 'learning_rate': 4.9335e-05, 'epoch': 0.22}

05/18/2024 22:21:28 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:21:28 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:21:28 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:24:44 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-300

05/18/2024 22:24:44 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:24:44 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:24:44 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-300\tokenizer_config.json

05/18/2024 22:24:44 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-300\special_tokens_map.json

05/18/2024 22:24:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0961, 'learning_rate': 4.9313e-05, 'epoch': 0.23}

05/18/2024 22:25:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0902, 'learning_rate': 4.9291e-05, 'epoch': 0.23}

05/18/2024 22:25:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0687, 'learning_rate': 4.9267e-05, 'epoch': 0.23}

05/18/2024 22:25:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0811, 'learning_rate': 4.9244e-05, 'epoch': 0.24}

05/18/2024 22:25:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0765, 'learning_rate': 4.9220e-05, 'epoch': 0.24}

05/18/2024 22:25:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.1417, 'learning_rate': 4.9196e-05, 'epoch': 0.24}

05/18/2024 22:25:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.1079, 'learning_rate': 4.9172e-05, 'epoch': 0.25}

05/18/2024 22:25:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.1219, 'learning_rate': 4.9147e-05, 'epoch': 0.25}

05/18/2024 22:26:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.1149, 'learning_rate': 4.9122e-05, 'epoch': 0.25}

05/18/2024 22:26:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0792, 'learning_rate': 4.9096e-05, 'epoch': 0.26}

05/18/2024 22:26:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0805, 'learning_rate': 4.9070e-05, 'epoch': 0.26}

05/18/2024 22:26:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0852, 'learning_rate': 4.9044e-05, 'epoch': 0.27}

05/18/2024 22:26:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0926, 'learning_rate': 4.9017e-05, 'epoch': 0.27}

05/18/2024 22:26:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.1002, 'learning_rate': 4.8990e-05, 'epoch': 0.27}

05/18/2024 22:27:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0656, 'learning_rate': 4.8963e-05, 'epoch': 0.28}

05/18/2024 22:27:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0629, 'learning_rate': 4.8935e-05, 'epoch': 0.28}

05/18/2024 22:27:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0518, 'learning_rate': 4.8907e-05, 'epoch': 0.28}

05/18/2024 22:27:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0979, 'learning_rate': 4.8879e-05, 'epoch': 0.29}

05/18/2024 22:27:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0827, 'learning_rate': 4.8850e-05, 'epoch': 0.29}

05/18/2024 22:27:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.1390, 'learning_rate': 4.8821e-05, 'epoch': 0.30}

05/18/2024 22:27:49 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:27:49 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:27:49 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:31:03 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-400

05/18/2024 22:31:04 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:31:04 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:31:04 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-400\tokenizer_config.json

05/18/2024 22:31:04 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-400\special_tokens_map.json

05/18/2024 22:31:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0715, 'learning_rate': 4.8791e-05, 'epoch': 0.30}

05/18/2024 22:31:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0593, 'learning_rate': 4.8761e-05, 'epoch': 0.30}

05/18/2024 22:31:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0730, 'learning_rate': 4.8731e-05, 'epoch': 0.31}

05/18/2024 22:31:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.1437, 'learning_rate': 4.8701e-05, 'epoch': 0.31}

05/18/2024 22:31:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0660, 'learning_rate': 4.8670e-05, 'epoch': 0.31}

05/18/2024 22:32:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0841, 'learning_rate': 4.8638e-05, 'epoch': 0.32}

05/18/2024 22:32:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.1017, 'learning_rate': 4.8607e-05, 'epoch': 0.32}

05/18/2024 22:32:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0934, 'learning_rate': 4.8575e-05, 'epoch': 0.32}

05/18/2024 22:32:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0682, 'learning_rate': 4.8543e-05, 'epoch': 0.33}

05/18/2024 22:32:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.1045, 'learning_rate': 4.8510e-05, 'epoch': 0.33}

05/18/2024 22:32:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0815, 'learning_rate': 4.8477e-05, 'epoch': 0.34}

05/18/2024 22:32:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0692, 'learning_rate': 4.8443e-05, 'epoch': 0.34}

05/18/2024 22:33:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0608, 'learning_rate': 4.8410e-05, 'epoch': 0.34}

05/18/2024 22:33:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.1030, 'learning_rate': 4.8376e-05, 'epoch': 0.35}

05/18/2024 22:33:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0767, 'learning_rate': 4.8341e-05, 'epoch': 0.35}

05/18/2024 22:33:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.1345, 'learning_rate': 4.8306e-05, 'epoch': 0.35}

05/18/2024 22:33:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0739, 'learning_rate': 4.8271e-05, 'epoch': 0.36}

05/18/2024 22:33:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0857, 'learning_rate': 4.8236e-05, 'epoch': 0.36}

05/18/2024 22:34:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0712, 'learning_rate': 4.8200e-05, 'epoch': 0.37}

05/18/2024 22:34:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0778, 'learning_rate': 4.8164e-05, 'epoch': 0.37}

05/18/2024 22:34:10 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:34:10 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:34:10 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:37:25 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-500

05/18/2024 22:37:26 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:37:26 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:37:26 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-500\tokenizer_config.json

05/18/2024 22:37:26 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-500\special_tokens_map.json

05/18/2024 22:37:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0769, 'learning_rate': 4.8127e-05, 'epoch': 0.37}

05/18/2024 22:37:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.1075, 'learning_rate': 4.8091e-05, 'epoch': 0.38}

05/18/2024 22:37:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.1356, 'learning_rate': 4.8053e-05, 'epoch': 0.38}

05/18/2024 22:38:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0762, 'learning_rate': 4.8016e-05, 'epoch': 0.38}

05/18/2024 22:38:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0703, 'learning_rate': 4.7978e-05, 'epoch': 0.39}

05/18/2024 22:38:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0666, 'learning_rate': 4.7940e-05, 'epoch': 0.39}

05/18/2024 22:38:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0744, 'learning_rate': 4.7901e-05, 'epoch': 0.39}

05/18/2024 22:38:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.1018, 'learning_rate': 4.7862e-05, 'epoch': 0.40}

05/18/2024 22:38:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.1088, 'learning_rate': 4.7823e-05, 'epoch': 0.40}

05/18/2024 22:38:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0946, 'learning_rate': 4.7783e-05, 'epoch': 0.41}

05/18/2024 22:39:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0512, 'learning_rate': 4.7743e-05, 'epoch': 0.41}

05/18/2024 22:39:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.1034, 'learning_rate': 4.7703e-05, 'epoch': 0.41}

05/18/2024 22:39:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.1168, 'learning_rate': 4.7662e-05, 'epoch': 0.42}

05/18/2024 22:40:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.0391, 'learning_rate': 4.7621e-05, 'epoch': 0.42}

05/18/2024 22:40:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0533, 'learning_rate': 4.7580e-05, 'epoch': 0.42}

05/18/2024 22:40:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0920, 'learning_rate': 4.7539e-05, 'epoch': 0.43}

05/18/2024 22:41:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0797, 'learning_rate': 4.7497e-05, 'epoch': 0.43}

05/18/2024 22:41:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0658, 'learning_rate': 4.7454e-05, 'epoch': 0.44}

05/18/2024 22:42:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0763, 'learning_rate': 4.7412e-05, 'epoch': 0.44}

05/18/2024 22:42:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0476, 'learning_rate': 4.7369e-05, 'epoch': 0.44}

05/18/2024 22:42:46 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:42:46 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:42:46 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:48:40 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-600

05/18/2024 22:48:41 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:48:41 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:48:41 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-600\tokenizer_config.json

05/18/2024 22:48:41 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-600\special_tokens_map.json

05/18/2024 22:48:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0848, 'learning_rate': 4.7325e-05, 'epoch': 0.45}

05/18/2024 22:48:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0878, 'learning_rate': 4.7282e-05, 'epoch': 0.45}

05/18/2024 22:49:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0932, 'learning_rate': 4.7238e-05, 'epoch': 0.45}

05/18/2024 22:49:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.1041, 'learning_rate': 4.7193e-05, 'epoch': 0.46}

05/18/2024 22:49:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0790, 'learning_rate': 4.7149e-05, 'epoch': 0.46}

05/18/2024 22:49:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0559, 'learning_rate': 4.7104e-05, 'epoch': 0.46}

05/18/2024 22:50:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0824, 'learning_rate': 4.7059e-05, 'epoch': 0.47}

05/18/2024 22:50:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0603, 'learning_rate': 4.7013e-05, 'epoch': 0.47}

05/18/2024 22:51:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.1087, 'learning_rate': 4.6967e-05, 'epoch': 0.48}

05/18/2024 22:51:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0993, 'learning_rate': 4.6921e-05, 'epoch': 0.48}

05/18/2024 22:52:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.1266, 'learning_rate': 4.6874e-05, 'epoch': 0.48}

05/18/2024 22:52:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0786, 'learning_rate': 4.6827e-05, 'epoch': 0.49}

05/18/2024 22:52:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0844, 'learning_rate': 4.6780e-05, 'epoch': 0.49}

05/18/2024 22:53:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0925, 'learning_rate': 4.6732e-05, 'epoch': 0.49}

05/18/2024 22:53:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0648, 'learning_rate': 4.6684e-05, 'epoch': 0.50}

05/18/2024 22:53:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0740, 'learning_rate': 4.6636e-05, 'epoch': 0.50}

05/18/2024 22:53:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0729, 'learning_rate': 4.6588e-05, 'epoch': 0.51}

05/18/2024 22:54:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0886, 'learning_rate': 4.6539e-05, 'epoch': 0.51}

05/18/2024 22:54:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0842, 'learning_rate': 4.6489e-05, 'epoch': 0.51}

05/18/2024 22:54:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.1266, 'learning_rate': 4.6440e-05, 'epoch': 0.52}

05/18/2024 22:54:18 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 22:54:18 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 22:54:18 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 22:58:11 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-700

05/18/2024 22:58:12 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 22:58:12 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 22:58:12 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-700\tokenizer_config.json

05/18/2024 22:58:12 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-700\special_tokens_map.json

05/18/2024 22:58:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.1021, 'learning_rate': 4.6390e-05, 'epoch': 0.52}

05/18/2024 22:58:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.1021, 'learning_rate': 4.6340e-05, 'epoch': 0.52}

05/18/2024 22:58:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0575, 'learning_rate': 4.6289e-05, 'epoch': 0.53}

05/18/2024 22:58:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0933, 'learning_rate': 4.6239e-05, 'epoch': 0.53}

05/18/2024 22:58:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0494, 'learning_rate': 4.6188e-05, 'epoch': 0.54}

05/18/2024 22:59:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0366, 'learning_rate': 4.6136e-05, 'epoch': 0.54}

05/18/2024 22:59:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0825, 'learning_rate': 4.6084e-05, 'epoch': 0.54}

05/18/2024 22:59:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0803, 'learning_rate': 4.6032e-05, 'epoch': 0.55}

05/18/2024 22:59:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0638, 'learning_rate': 4.5980e-05, 'epoch': 0.55}

05/18/2024 22:59:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0742, 'learning_rate': 4.5927e-05, 'epoch': 0.55}

05/18/2024 23:00:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0638, 'learning_rate': 4.5874e-05, 'epoch': 0.56}

05/18/2024 23:00:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0755, 'learning_rate': 4.5821e-05, 'epoch': 0.56}

05/18/2024 23:00:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0750, 'learning_rate': 4.5767e-05, 'epoch': 0.56}

05/18/2024 23:01:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0518, 'learning_rate': 4.5713e-05, 'epoch': 0.57}

05/18/2024 23:01:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0785, 'learning_rate': 4.5659e-05, 'epoch': 0.57}

05/18/2024 23:01:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.1104, 'learning_rate': 4.5605e-05, 'epoch': 0.58}

05/18/2024 23:01:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0626, 'learning_rate': 4.5550e-05, 'epoch': 0.58}

05/18/2024 23:02:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0501, 'learning_rate': 4.5494e-05, 'epoch': 0.58}

05/18/2024 23:02:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0757, 'learning_rate': 4.5439e-05, 'epoch': 0.59}

05/18/2024 23:03:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0819, 'learning_rate': 4.5383e-05, 'epoch': 0.59}

05/18/2024 23:03:00 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 23:03:00 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 23:03:00 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 23:09:16 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-800

05/18/2024 23:09:16 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 23:09:16 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 23:09:16 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-800\tokenizer_config.json

05/18/2024 23:09:16 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-800\special_tokens_map.json

05/18/2024 23:09:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0957, 'learning_rate': 4.5327e-05, 'epoch': 0.59}

05/18/2024 23:09:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0463, 'learning_rate': 4.5271e-05, 'epoch': 0.60}

05/18/2024 23:09:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0821, 'learning_rate': 4.5214e-05, 'epoch': 0.60}

05/18/2024 23:10:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0255, 'learning_rate': 4.5157e-05, 'epoch': 0.61}

05/18/2024 23:10:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0979, 'learning_rate': 4.5100e-05, 'epoch': 0.61}

05/18/2024 23:11:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.1220, 'learning_rate': 4.5042e-05, 'epoch': 0.61}

05/18/2024 23:11:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0765, 'learning_rate': 4.4984e-05, 'epoch': 0.62}

05/18/2024 23:12:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0619, 'learning_rate': 4.4926e-05, 'epoch': 0.62}

05/18/2024 23:12:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0920, 'learning_rate': 4.4868e-05, 'epoch': 0.62}

05/18/2024 23:13:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0864, 'learning_rate': 4.4809e-05, 'epoch': 0.63}

05/18/2024 23:13:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0646, 'learning_rate': 4.4750e-05, 'epoch': 0.63}

05/18/2024 23:14:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0896, 'learning_rate': 4.4690e-05, 'epoch': 0.63}

05/18/2024 23:14:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0708, 'learning_rate': 4.4631e-05, 'epoch': 0.64}

05/18/2024 23:15:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0659, 'learning_rate': 4.4571e-05, 'epoch': 0.64}

05/18/2024 23:15:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0516, 'learning_rate': 4.4510e-05, 'epoch': 0.65}

05/18/2024 23:15:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0942, 'learning_rate': 4.4450e-05, 'epoch': 0.65}

05/18/2024 23:16:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.1266, 'learning_rate': 4.4389e-05, 'epoch': 0.65}

05/18/2024 23:16:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.1257, 'learning_rate': 4.4328e-05, 'epoch': 0.66}

05/18/2024 23:17:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0702, 'learning_rate': 4.4267e-05, 'epoch': 0.66}

05/18/2024 23:17:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.1152, 'learning_rate': 4.4205e-05, 'epoch': 0.66}

05/18/2024 23:17:31 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 23:17:31 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 23:17:31 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 23:26:13 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-900

05/18/2024 23:26:14 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 23:26:14 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 23:26:14 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-900\tokenizer_config.json

05/18/2024 23:26:14 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-900\special_tokens_map.json

05/18/2024 23:26:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0461, 'learning_rate': 4.4143e-05, 'epoch': 0.67}

05/18/2024 23:27:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0812, 'learning_rate': 4.4081e-05, 'epoch': 0.67}

05/18/2024 23:27:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0589, 'learning_rate': 4.4018e-05, 'epoch': 0.68}

05/18/2024 23:28:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0399, 'learning_rate': 4.3955e-05, 'epoch': 0.68}

05/18/2024 23:28:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0561, 'learning_rate': 4.3892e-05, 'epoch': 0.68}

05/18/2024 23:28:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0693, 'learning_rate': 4.3829e-05, 'epoch': 0.69}

05/18/2024 23:29:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.1032, 'learning_rate': 4.3765e-05, 'epoch': 0.69}

05/18/2024 23:29:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.1278, 'learning_rate': 4.3701e-05, 'epoch': 0.69}

05/18/2024 23:30:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0941, 'learning_rate': 4.3637e-05, 'epoch': 0.70}

05/18/2024 23:30:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0806, 'learning_rate': 4.3572e-05, 'epoch': 0.70}

05/18/2024 23:31:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.1019, 'learning_rate': 4.3507e-05, 'epoch': 0.70}

05/18/2024 23:31:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.1000, 'learning_rate': 4.3442e-05, 'epoch': 0.71}

05/18/2024 23:32:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0746, 'learning_rate': 4.3377e-05, 'epoch': 0.71}

05/18/2024 23:33:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0411, 'learning_rate': 4.3311e-05, 'epoch': 0.72}

05/18/2024 23:33:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0721, 'learning_rate': 4.3245e-05, 'epoch': 0.72}

05/18/2024 23:34:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.1192, 'learning_rate': 4.3179e-05, 'epoch': 0.72}

05/18/2024 23:34:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.1015, 'learning_rate': 4.3113e-05, 'epoch': 0.73}

05/18/2024 23:34:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0398, 'learning_rate': 4.3046e-05, 'epoch': 0.73}

05/18/2024 23:35:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.1069, 'learning_rate': 4.2979e-05, 'epoch': 0.73}

05/18/2024 23:35:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0577, 'learning_rate': 4.2912e-05, 'epoch': 0.74}

05/18/2024 23:35:51 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 23:35:51 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 23:35:51 - INFO - transformers.trainer -   Batch size = 1

05/18/2024 23:44:37 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1000

05/18/2024 23:44:38 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/18/2024 23:44:38 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/18/2024 23:44:38 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1000\tokenizer_config.json

05/18/2024 23:44:38 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1000\special_tokens_map.json

05/18/2024 23:44:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0630, 'learning_rate': 4.2844e-05, 'epoch': 0.74}

05/18/2024 23:44:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0712, 'learning_rate': 4.2776e-05, 'epoch': 0.75}

05/18/2024 23:45:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0314, 'learning_rate': 4.2708e-05, 'epoch': 0.75}

05/18/2024 23:45:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.1062, 'learning_rate': 4.2640e-05, 'epoch': 0.75}

05/18/2024 23:46:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0603, 'learning_rate': 4.2571e-05, 'epoch': 0.76}

05/18/2024 23:46:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0576, 'learning_rate': 4.2503e-05, 'epoch': 0.76}

05/18/2024 23:47:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0811, 'learning_rate': 4.2434e-05, 'epoch': 0.76}

05/18/2024 23:47:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0494, 'learning_rate': 4.2364e-05, 'epoch': 0.77}

05/18/2024 23:48:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0717, 'learning_rate': 4.2295e-05, 'epoch': 0.77}

05/18/2024 23:48:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0877, 'learning_rate': 4.2225e-05, 'epoch': 0.77}

05/18/2024 23:49:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0498, 'learning_rate': 4.2154e-05, 'epoch': 0.78}

05/18/2024 23:49:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.1005, 'learning_rate': 4.2084e-05, 'epoch': 0.78}

05/18/2024 23:50:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0481, 'learning_rate': 4.2013e-05, 'epoch': 0.79}

05/18/2024 23:50:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0536, 'learning_rate': 4.1943e-05, 'epoch': 0.79}

05/18/2024 23:51:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0417, 'learning_rate': 4.1871e-05, 'epoch': 0.79}

05/18/2024 23:51:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0943, 'learning_rate': 4.1800e-05, 'epoch': 0.80}

05/18/2024 23:52:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0511, 'learning_rate': 4.1728e-05, 'epoch': 0.80}

05/18/2024 23:52:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.1094, 'learning_rate': 4.1656e-05, 'epoch': 0.80}

05/18/2024 23:52:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0526, 'learning_rate': 4.1584e-05, 'epoch': 0.81}

05/18/2024 23:53:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0614, 'learning_rate': 4.1512e-05, 'epoch': 0.81}

05/18/2024 23:53:25 - INFO - transformers.trainer - ***** Running Evaluation *****

05/18/2024 23:53:25 - INFO - transformers.trainer -   Num examples = 2710

05/18/2024 23:53:25 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 00:02:43 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1100

05/19/2024 00:02:44 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 00:02:44 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 00:02:44 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1100\tokenizer_config.json

05/19/2024 00:02:44 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1100\special_tokens_map.json

05/19/2024 00:03:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0282, 'learning_rate': 4.1439e-05, 'epoch': 0.82}

05/19/2024 00:03:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0766, 'learning_rate': 4.1366e-05, 'epoch': 0.82}

05/19/2024 00:03:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0964, 'learning_rate': 4.1293e-05, 'epoch': 0.82}

05/19/2024 00:04:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0514, 'learning_rate': 4.1220e-05, 'epoch': 0.83}

05/19/2024 00:04:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.1066, 'learning_rate': 4.1146e-05, 'epoch': 0.83}

05/19/2024 00:05:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0611, 'learning_rate': 4.1072e-05, 'epoch': 0.83}

05/19/2024 00:05:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0347, 'learning_rate': 4.0998e-05, 'epoch': 0.84}

05/19/2024 00:06:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0878, 'learning_rate': 4.0924e-05, 'epoch': 0.84}

05/19/2024 00:06:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0419, 'learning_rate': 4.0849e-05, 'epoch': 0.85}

05/19/2024 00:06:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0505, 'learning_rate': 4.0774e-05, 'epoch': 0.85}

05/19/2024 00:06:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0493, 'learning_rate': 4.0699e-05, 'epoch': 0.85}

05/19/2024 00:07:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.1015, 'learning_rate': 4.0624e-05, 'epoch': 0.86}

05/19/2024 00:07:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0829, 'learning_rate': 4.0548e-05, 'epoch': 0.86}

05/19/2024 00:08:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0640, 'learning_rate': 4.0473e-05, 'epoch': 0.86}

05/19/2024 00:08:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0951, 'learning_rate': 4.0397e-05, 'epoch': 0.87}

05/19/2024 00:08:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0938, 'learning_rate': 4.0320e-05, 'epoch': 0.87}

05/19/2024 00:09:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0501, 'learning_rate': 4.0244e-05, 'epoch': 0.87}

05/19/2024 00:09:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0512, 'learning_rate': 4.0167e-05, 'epoch': 0.88}

05/19/2024 00:10:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.1374, 'learning_rate': 4.0090e-05, 'epoch': 0.88}

05/19/2024 00:10:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0480, 'learning_rate': 4.0013e-05, 'epoch': 0.89}

05/19/2024 00:10:39 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 00:10:39 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 00:10:39 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 00:20:32 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1200

05/19/2024 00:20:33 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 00:20:33 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 00:20:34 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1200\tokenizer_config.json

05/19/2024 00:20:34 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1200\special_tokens_map.json

05/19/2024 00:20:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0478, 'learning_rate': 3.9936e-05, 'epoch': 0.89}

05/19/2024 00:21:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0800, 'learning_rate': 3.9858e-05, 'epoch': 0.89}

05/19/2024 00:21:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0979, 'learning_rate': 3.9780e-05, 'epoch': 0.90}

05/19/2024 00:22:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0663, 'learning_rate': 3.9702e-05, 'epoch': 0.90}

05/19/2024 00:22:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0591, 'learning_rate': 3.9624e-05, 'epoch': 0.90}

05/19/2024 00:23:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.0849, 'learning_rate': 3.9546e-05, 'epoch': 0.91}

05/19/2024 00:23:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0571, 'learning_rate': 3.9467e-05, 'epoch': 0.91}

05/19/2024 00:23:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0408, 'learning_rate': 3.9388e-05, 'epoch': 0.92}

05/19/2024 00:24:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0507, 'learning_rate': 3.9309e-05, 'epoch': 0.92}

05/19/2024 00:24:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0972, 'learning_rate': 3.9230e-05, 'epoch': 0.92}

05/19/2024 00:25:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0417, 'learning_rate': 3.9150e-05, 'epoch': 0.93}

05/19/2024 00:25:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0349, 'learning_rate': 3.9070e-05, 'epoch': 0.93}

05/19/2024 00:25:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0505, 'learning_rate': 3.8990e-05, 'epoch': 0.93}

05/19/2024 00:26:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0760, 'learning_rate': 3.8910e-05, 'epoch': 0.94}

05/19/2024 00:26:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0491, 'learning_rate': 3.8830e-05, 'epoch': 0.94}

05/19/2024 00:27:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.1038, 'learning_rate': 3.8749e-05, 'epoch': 0.94}

05/19/2024 00:27:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0557, 'learning_rate': 3.8669e-05, 'epoch': 0.95}

05/19/2024 00:27:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0776, 'learning_rate': 3.8588e-05, 'epoch': 0.95}

05/19/2024 00:28:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0749, 'learning_rate': 3.8506e-05, 'epoch': 0.96}

05/19/2024 00:28:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0435, 'learning_rate': 3.8425e-05, 'epoch': 0.96}

05/19/2024 00:28:47 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 00:28:47 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 00:28:47 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 00:37:57 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1300

05/19/2024 00:37:58 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 00:37:58 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 00:37:58 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1300\tokenizer_config.json

05/19/2024 00:37:58 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1300\special_tokens_map.json

05/19/2024 00:38:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0681, 'learning_rate': 3.8343e-05, 'epoch': 0.96}

05/19/2024 00:38:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0515, 'learning_rate': 3.8262e-05, 'epoch': 0.97}

05/19/2024 00:39:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0475, 'learning_rate': 3.8180e-05, 'epoch': 0.97}

05/19/2024 00:39:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.1170, 'learning_rate': 3.8097e-05, 'epoch': 0.97}

05/19/2024 00:40:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0585, 'learning_rate': 3.8015e-05, 'epoch': 0.98}

05/19/2024 00:40:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.1038, 'learning_rate': 3.7932e-05, 'epoch': 0.98}

05/19/2024 00:40:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0505, 'learning_rate': 3.7850e-05, 'epoch': 0.99}

05/19/2024 00:41:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0783, 'learning_rate': 3.7767e-05, 'epoch': 0.99}

05/19/2024 00:41:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0557, 'learning_rate': 3.7684e-05, 'epoch': 0.99}

05/19/2024 00:42:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.1253, 'learning_rate': 3.7600e-05, 'epoch': 1.00}

05/19/2024 00:42:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0929, 'learning_rate': 3.7517e-05, 'epoch': 1.00}

05/19/2024 00:43:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0576, 'learning_rate': 3.7433e-05, 'epoch': 1.00}

05/19/2024 00:43:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0627, 'learning_rate': 3.7349e-05, 'epoch': 1.01}

05/19/2024 00:43:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0518, 'learning_rate': 3.7265e-05, 'epoch': 1.01}

05/19/2024 00:44:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0950, 'learning_rate': 3.7181e-05, 'epoch': 1.01}

05/19/2024 00:44:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0832, 'learning_rate': 3.7096e-05, 'epoch': 1.02}

05/19/2024 00:45:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0702, 'learning_rate': 3.7012e-05, 'epoch': 1.02}

05/19/2024 00:45:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0771, 'learning_rate': 3.6927e-05, 'epoch': 1.03}

05/19/2024 00:45:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0427, 'learning_rate': 3.6842e-05, 'epoch': 1.03}

05/19/2024 00:46:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0595, 'learning_rate': 3.6757e-05, 'epoch': 1.03}

05/19/2024 00:46:18 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 00:46:18 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 00:46:18 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 00:55:23 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1400

05/19/2024 00:55:24 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 00:55:24 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 00:55:24 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1400\tokenizer_config.json

05/19/2024 00:55:24 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1400\special_tokens_map.json

05/19/2024 00:55:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0773, 'learning_rate': 3.6671e-05, 'epoch': 1.04}

05/19/2024 00:56:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0725, 'learning_rate': 3.6586e-05, 'epoch': 1.04}

05/19/2024 00:56:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0663, 'learning_rate': 3.6500e-05, 'epoch': 1.04}

05/19/2024 00:57:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0495, 'learning_rate': 3.6414e-05, 'epoch': 1.05}

05/19/2024 00:57:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0870, 'learning_rate': 3.6328e-05, 'epoch': 1.05}

05/19/2024 00:57:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0739, 'learning_rate': 3.6242e-05, 'epoch': 1.06}

05/19/2024 00:58:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0887, 'learning_rate': 3.6156e-05, 'epoch': 1.06}

05/19/2024 00:58:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0377, 'learning_rate': 3.6069e-05, 'epoch': 1.06}

05/19/2024 00:59:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0529, 'learning_rate': 3.5982e-05, 'epoch': 1.07}

05/19/2024 00:59:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0560, 'learning_rate': 3.5896e-05, 'epoch': 1.07}

05/19/2024 00:59:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0408, 'learning_rate': 3.5809e-05, 'epoch': 1.07}

05/19/2024 01:00:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0535, 'learning_rate': 3.5721e-05, 'epoch': 1.08}

05/19/2024 01:00:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0429, 'learning_rate': 3.5634e-05, 'epoch': 1.08}

05/19/2024 01:01:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0283, 'learning_rate': 3.5546e-05, 'epoch': 1.08}

05/19/2024 01:01:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.1039, 'learning_rate': 3.5459e-05, 'epoch': 1.09}

05/19/2024 01:02:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0494, 'learning_rate': 3.5371e-05, 'epoch': 1.09}

05/19/2024 01:02:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0462, 'learning_rate': 3.5283e-05, 'epoch': 1.10}

05/19/2024 01:02:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0511, 'learning_rate': 3.5195e-05, 'epoch': 1.10}

05/19/2024 01:03:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0606, 'learning_rate': 3.5107e-05, 'epoch': 1.10}

05/19/2024 01:03:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0215, 'learning_rate': 3.5018e-05, 'epoch': 1.11}

05/19/2024 01:03:43 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 01:03:43 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 01:03:43 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 01:12:51 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1500

05/19/2024 01:12:53 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 01:12:53 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 01:12:53 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1500\tokenizer_config.json

05/19/2024 01:12:53 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1500\special_tokens_map.json

05/19/2024 01:13:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0731, 'learning_rate': 3.4930e-05, 'epoch': 1.11}

05/19/2024 01:13:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0692, 'learning_rate': 3.4841e-05, 'epoch': 1.11}

05/19/2024 01:14:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0730, 'learning_rate': 3.4752e-05, 'epoch': 1.12}

05/19/2024 01:14:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0736, 'learning_rate': 3.4663e-05, 'epoch': 1.12}

05/19/2024 01:15:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0672, 'learning_rate': 3.4574e-05, 'epoch': 1.13}

05/19/2024 01:15:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0638, 'learning_rate': 3.4484e-05, 'epoch': 1.13}

05/19/2024 01:15:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0336, 'learning_rate': 3.4395e-05, 'epoch': 1.13}

05/19/2024 01:16:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0280, 'learning_rate': 3.4305e-05, 'epoch': 1.14}

05/19/2024 01:16:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0280, 'learning_rate': 3.4216e-05, 'epoch': 1.14}

05/19/2024 01:17:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0348, 'learning_rate': 3.4126e-05, 'epoch': 1.14}

05/19/2024 01:17:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0533, 'learning_rate': 3.4036e-05, 'epoch': 1.15}

05/19/2024 01:18:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0362, 'learning_rate': 3.3946e-05, 'epoch': 1.15}

05/19/2024 01:18:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.1237, 'learning_rate': 3.3855e-05, 'epoch': 1.15}

05/19/2024 01:19:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0436, 'learning_rate': 3.3765e-05, 'epoch': 1.16}

05/19/2024 01:19:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0921, 'learning_rate': 3.3674e-05, 'epoch': 1.16}

05/19/2024 01:19:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0592, 'learning_rate': 3.3584e-05, 'epoch': 1.17}

05/19/2024 01:20:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0727, 'learning_rate': 3.3493e-05, 'epoch': 1.17}

05/19/2024 01:20:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0461, 'learning_rate': 3.3402e-05, 'epoch': 1.17}

05/19/2024 01:21:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0405, 'learning_rate': 3.3311e-05, 'epoch': 1.18}

05/19/2024 01:21:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0133, 'learning_rate': 3.3220e-05, 'epoch': 1.18}

05/19/2024 01:21:41 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 01:21:41 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 01:21:41 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 01:31:11 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1600

05/19/2024 01:31:12 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 01:31:12 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 01:31:12 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1600\tokenizer_config.json

05/19/2024 01:31:12 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1600\special_tokens_map.json

05/19/2024 01:31:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0779, 'learning_rate': 3.3129e-05, 'epoch': 1.18}

05/19/2024 01:32:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0665, 'learning_rate': 3.3037e-05, 'epoch': 1.19}

05/19/2024 01:32:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.1175, 'learning_rate': 3.2946e-05, 'epoch': 1.19}

05/19/2024 01:32:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0553, 'learning_rate': 3.2854e-05, 'epoch': 1.20}

05/19/2024 01:33:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0465, 'learning_rate': 3.2762e-05, 'epoch': 1.20}

05/19/2024 01:33:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0517, 'learning_rate': 3.2670e-05, 'epoch': 1.20}

05/19/2024 01:34:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0461, 'learning_rate': 3.2578e-05, 'epoch': 1.21}

05/19/2024 01:34:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0376, 'learning_rate': 3.2486e-05, 'epoch': 1.21}

05/19/2024 01:35:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0611, 'learning_rate': 3.2394e-05, 'epoch': 1.21}

05/19/2024 01:35:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0460, 'learning_rate': 3.2302e-05, 'epoch': 1.22}

05/19/2024 01:36:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0454, 'learning_rate': 3.2209e-05, 'epoch': 1.22}

05/19/2024 01:36:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0788, 'learning_rate': 3.2117e-05, 'epoch': 1.23}

05/19/2024 01:36:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0514, 'learning_rate': 3.2024e-05, 'epoch': 1.23}

05/19/2024 01:37:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0378, 'learning_rate': 3.1931e-05, 'epoch': 1.23}

05/19/2024 01:37:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0588, 'learning_rate': 3.1838e-05, 'epoch': 1.24}

05/19/2024 01:38:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0748, 'learning_rate': 3.1745e-05, 'epoch': 1.24}

05/19/2024 01:38:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0200, 'learning_rate': 3.1652e-05, 'epoch': 1.24}

05/19/2024 01:39:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0458, 'learning_rate': 3.1559e-05, 'epoch': 1.25}

05/19/2024 01:39:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0826, 'learning_rate': 3.1466e-05, 'epoch': 1.25}

05/19/2024 01:40:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0658, 'learning_rate': 3.1372e-05, 'epoch': 1.25}

05/19/2024 01:40:02 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 01:40:02 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 01:40:02 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 01:49:35 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1700

05/19/2024 01:49:36 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 01:49:36 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 01:49:36 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1700\tokenizer_config.json

05/19/2024 01:49:36 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1700\special_tokens_map.json

05/19/2024 01:50:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.0742, 'learning_rate': 3.1279e-05, 'epoch': 1.26}

05/19/2024 01:50:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0464, 'learning_rate': 3.1185e-05, 'epoch': 1.26}

05/19/2024 01:50:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.1035, 'learning_rate': 3.1092e-05, 'epoch': 1.27}

05/19/2024 01:51:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0537, 'learning_rate': 3.0998e-05, 'epoch': 1.27}

05/19/2024 01:51:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0920, 'learning_rate': 3.0904e-05, 'epoch': 1.27}

05/19/2024 01:52:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0434, 'learning_rate': 3.0810e-05, 'epoch': 1.28}

05/19/2024 01:52:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0377, 'learning_rate': 3.0716e-05, 'epoch': 1.28}

05/19/2024 01:53:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0528, 'learning_rate': 3.0622e-05, 'epoch': 1.28}

05/19/2024 01:53:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0502, 'learning_rate': 3.0528e-05, 'epoch': 1.29}

05/19/2024 01:54:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0522, 'learning_rate': 3.0434e-05, 'epoch': 1.29}

05/19/2024 01:54:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0702, 'learning_rate': 3.0339e-05, 'epoch': 1.30}

05/19/2024 01:54:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0567, 'learning_rate': 3.0245e-05, 'epoch': 1.30}

05/19/2024 01:55:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0572, 'learning_rate': 3.0151e-05, 'epoch': 1.30}

05/19/2024 01:55:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0684, 'learning_rate': 3.0056e-05, 'epoch': 1.31}

05/19/2024 01:56:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0258, 'learning_rate': 2.9961e-05, 'epoch': 1.31}

05/19/2024 01:56:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0326, 'learning_rate': 2.9867e-05, 'epoch': 1.31}

05/19/2024 01:57:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.0390, 'learning_rate': 2.9772e-05, 'epoch': 1.32}

05/19/2024 01:57:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0550, 'learning_rate': 2.9677e-05, 'epoch': 1.32}

05/19/2024 01:57:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0836, 'learning_rate': 2.9582e-05, 'epoch': 1.32}

05/19/2024 01:58:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0480, 'learning_rate': 2.9487e-05, 'epoch': 1.33}

05/19/2024 01:58:23 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 01:58:23 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 01:58:23 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 02:07:53 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1800

05/19/2024 02:07:55 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 02:07:55 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 02:07:55 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1800\tokenizer_config.json

05/19/2024 02:07:55 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1800\special_tokens_map.json

05/19/2024 02:08:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0599, 'learning_rate': 2.9392e-05, 'epoch': 1.33}

05/19/2024 02:08:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0279, 'learning_rate': 2.9297e-05, 'epoch': 1.34}

05/19/2024 02:09:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0374, 'learning_rate': 2.9202e-05, 'epoch': 1.34}

05/19/2024 02:09:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0799, 'learning_rate': 2.9106e-05, 'epoch': 1.34}

05/19/2024 02:10:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0459, 'learning_rate': 2.9011e-05, 'epoch': 1.35}

05/19/2024 02:10:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0313, 'learning_rate': 2.8916e-05, 'epoch': 1.35}

05/19/2024 02:10:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0975, 'learning_rate': 2.8820e-05, 'epoch': 1.35}

05/19/2024 02:11:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0397, 'learning_rate': 2.8725e-05, 'epoch': 1.36}

05/19/2024 02:11:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0643, 'learning_rate': 2.8629e-05, 'epoch': 1.36}

05/19/2024 02:12:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0576, 'learning_rate': 2.8534e-05, 'epoch': 1.37}

05/19/2024 02:12:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0732, 'learning_rate': 2.8438e-05, 'epoch': 1.37}

05/19/2024 02:13:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0722, 'learning_rate': 2.8342e-05, 'epoch': 1.37}

05/19/2024 02:13:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0838, 'learning_rate': 2.8246e-05, 'epoch': 1.38}

05/19/2024 02:14:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0458, 'learning_rate': 2.8151e-05, 'epoch': 1.38}

05/19/2024 02:14:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0794, 'learning_rate': 2.8055e-05, 'epoch': 1.38}

05/19/2024 02:14:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0630, 'learning_rate': 2.7959e-05, 'epoch': 1.39}

05/19/2024 02:15:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0815, 'learning_rate': 2.7863e-05, 'epoch': 1.39}

05/19/2024 02:15:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0376, 'learning_rate': 2.7767e-05, 'epoch': 1.39}

05/19/2024 02:16:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.1088, 'learning_rate': 2.7671e-05, 'epoch': 1.40}

05/19/2024 02:16:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0821, 'learning_rate': 2.7575e-05, 'epoch': 1.40}

05/19/2024 02:16:40 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 02:16:40 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 02:16:40 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 02:26:01 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1900

05/19/2024 02:26:02 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 02:26:02 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 02:26:02 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1900\tokenizer_config.json

05/19/2024 02:26:02 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-1900\special_tokens_map.json

05/19/2024 02:26:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0589, 'learning_rate': 2.7479e-05, 'epoch': 1.41}

05/19/2024 02:26:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0635, 'learning_rate': 2.7383e-05, 'epoch': 1.41}

05/19/2024 02:27:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0456, 'learning_rate': 2.7286e-05, 'epoch': 1.41}

05/19/2024 02:27:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0514, 'learning_rate': 2.7190e-05, 'epoch': 1.42}

05/19/2024 02:28:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0478, 'learning_rate': 2.7094e-05, 'epoch': 1.42}

05/19/2024 02:28:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0492, 'learning_rate': 2.6998e-05, 'epoch': 1.42}

05/19/2024 02:29:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0921, 'learning_rate': 2.6901e-05, 'epoch': 1.43}

05/19/2024 02:29:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0833, 'learning_rate': 2.6805e-05, 'epoch': 1.43}

05/19/2024 02:30:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0480, 'learning_rate': 2.6709e-05, 'epoch': 1.44}

05/19/2024 02:30:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0434, 'learning_rate': 2.6612e-05, 'epoch': 1.44}

05/19/2024 02:30:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0811, 'learning_rate': 2.6516e-05, 'epoch': 1.44}

05/19/2024 02:31:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.1041, 'learning_rate': 2.6419e-05, 'epoch': 1.45}

05/19/2024 02:31:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0517, 'learning_rate': 2.6323e-05, 'epoch': 1.45}

05/19/2024 02:32:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0706, 'learning_rate': 2.6226e-05, 'epoch': 1.45}

05/19/2024 02:32:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0549, 'learning_rate': 2.6130e-05, 'epoch': 1.46}

05/19/2024 02:33:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.0928, 'learning_rate': 2.6033e-05, 'epoch': 1.46}

05/19/2024 02:33:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0872, 'learning_rate': 2.5937e-05, 'epoch': 1.46}

05/19/2024 02:33:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0460, 'learning_rate': 2.5840e-05, 'epoch': 1.47}

05/19/2024 02:34:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0484, 'learning_rate': 2.5744e-05, 'epoch': 1.47}

05/19/2024 02:34:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0696, 'learning_rate': 2.5647e-05, 'epoch': 1.48}

05/19/2024 02:34:50 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 02:34:50 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 02:34:50 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 02:44:16 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2000

05/19/2024 02:44:18 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 02:44:18 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 02:44:18 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2000\tokenizer_config.json

05/19/2024 02:44:18 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2000\special_tokens_map.json

05/19/2024 02:44:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0985, 'learning_rate': 2.5551e-05, 'epoch': 1.48}

05/19/2024 02:45:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0562, 'learning_rate': 2.5454e-05, 'epoch': 1.48}

05/19/2024 02:45:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0566, 'learning_rate': 2.5357e-05, 'epoch': 1.49}

05/19/2024 02:46:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0412, 'learning_rate': 2.5261e-05, 'epoch': 1.49}

05/19/2024 02:46:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0674, 'learning_rate': 2.5164e-05, 'epoch': 1.49}

05/19/2024 02:46:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0466, 'learning_rate': 2.5068e-05, 'epoch': 1.50}

05/19/2024 02:47:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0860, 'learning_rate': 2.4971e-05, 'epoch': 1.50}

05/19/2024 02:47:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0144, 'learning_rate': 2.4874e-05, 'epoch': 1.51}

05/19/2024 02:48:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0212, 'learning_rate': 2.4778e-05, 'epoch': 1.51}

05/19/2024 02:48:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0257, 'learning_rate': 2.4681e-05, 'epoch': 1.51}

05/19/2024 02:49:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0553, 'learning_rate': 2.4585e-05, 'epoch': 1.52}

05/19/2024 02:49:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0826, 'learning_rate': 2.4488e-05, 'epoch': 1.52}

05/19/2024 02:50:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0618, 'learning_rate': 2.4391e-05, 'epoch': 1.52}

05/19/2024 02:50:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0427, 'learning_rate': 2.4295e-05, 'epoch': 1.53}

05/19/2024 02:50:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0440, 'learning_rate': 2.4198e-05, 'epoch': 1.53}

05/19/2024 02:51:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0499, 'learning_rate': 2.4102e-05, 'epoch': 1.54}

05/19/2024 02:51:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0740, 'learning_rate': 2.4005e-05, 'epoch': 1.54}

05/19/2024 02:52:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0481, 'learning_rate': 2.3909e-05, 'epoch': 1.54}

05/19/2024 02:52:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0732, 'learning_rate': 2.3812e-05, 'epoch': 1.55}

05/19/2024 02:53:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0634, 'learning_rate': 2.3716e-05, 'epoch': 1.55}

05/19/2024 02:53:07 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 02:53:07 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 02:53:07 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 03:02:40 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2100

05/19/2024 03:02:42 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 03:02:42 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 03:02:42 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2100\tokenizer_config.json

05/19/2024 03:02:42 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2100\special_tokens_map.json

05/19/2024 03:03:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0721, 'learning_rate': 2.3619e-05, 'epoch': 1.55}

05/19/2024 03:03:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0481, 'learning_rate': 2.3523e-05, 'epoch': 1.56}

05/19/2024 03:04:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0915, 'learning_rate': 2.3426e-05, 'epoch': 1.56}

05/19/2024 03:04:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0499, 'learning_rate': 2.3330e-05, 'epoch': 1.56}

05/19/2024 03:05:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0573, 'learning_rate': 2.3234e-05, 'epoch': 1.57}

05/19/2024 03:05:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0485, 'learning_rate': 2.3137e-05, 'epoch': 1.57}

05/19/2024 03:06:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0571, 'learning_rate': 2.3041e-05, 'epoch': 1.58}

05/19/2024 03:06:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.0638, 'learning_rate': 2.2945e-05, 'epoch': 1.58}

05/19/2024 03:06:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0771, 'learning_rate': 2.2848e-05, 'epoch': 1.58}

05/19/2024 03:07:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0849, 'learning_rate': 2.2752e-05, 'epoch': 1.59}

05/19/2024 03:07:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0543, 'learning_rate': 2.2656e-05, 'epoch': 1.59}

05/19/2024 03:08:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0268, 'learning_rate': 2.2560e-05, 'epoch': 1.59}

05/19/2024 03:08:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0713, 'learning_rate': 2.2464e-05, 'epoch': 1.60}

05/19/2024 03:09:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0646, 'learning_rate': 2.2368e-05, 'epoch': 1.60}

05/19/2024 03:09:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0792, 'learning_rate': 2.2272e-05, 'epoch': 1.61}

05/19/2024 03:09:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0718, 'learning_rate': 2.2176e-05, 'epoch': 1.61}

05/19/2024 03:10:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0521, 'learning_rate': 2.2080e-05, 'epoch': 1.61}

05/19/2024 03:10:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0521, 'learning_rate': 2.1984e-05, 'epoch': 1.62}

05/19/2024 03:11:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0592, 'learning_rate': 2.1888e-05, 'epoch': 1.62}

05/19/2024 03:11:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0345, 'learning_rate': 2.1792e-05, 'epoch': 1.62}

05/19/2024 03:11:47 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 03:11:47 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 03:11:47 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 03:21:39 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2200

05/19/2024 03:21:40 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 03:21:40 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 03:21:40 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2200\tokenizer_config.json

05/19/2024 03:21:40 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2200\special_tokens_map.json

05/19/2024 03:22:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0713, 'learning_rate': 2.1696e-05, 'epoch': 1.63}

05/19/2024 03:22:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0499, 'learning_rate': 2.1600e-05, 'epoch': 1.63}

05/19/2024 03:23:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0973, 'learning_rate': 2.1505e-05, 'epoch': 1.63}

05/19/2024 03:23:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0536, 'learning_rate': 2.1409e-05, 'epoch': 1.64}

05/19/2024 03:23:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0356, 'learning_rate': 2.1314e-05, 'epoch': 1.64}

05/19/2024 03:24:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0461, 'learning_rate': 2.1218e-05, 'epoch': 1.65}

05/19/2024 03:24:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0592, 'learning_rate': 2.1123e-05, 'epoch': 1.65}

05/19/2024 03:25:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0371, 'learning_rate': 2.1027e-05, 'epoch': 1.65}

05/19/2024 03:25:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0764, 'learning_rate': 2.0932e-05, 'epoch': 1.66}

05/19/2024 03:26:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0491, 'learning_rate': 2.0836e-05, 'epoch': 1.66}

05/19/2024 03:26:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0609, 'learning_rate': 2.0741e-05, 'epoch': 1.66}

05/19/2024 03:27:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0738, 'learning_rate': 2.0646e-05, 'epoch': 1.67}

05/19/2024 03:27:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0526, 'learning_rate': 2.0551e-05, 'epoch': 1.67}

05/19/2024 03:27:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0467, 'learning_rate': 2.0456e-05, 'epoch': 1.68}

05/19/2024 03:28:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0423, 'learning_rate': 2.0361e-05, 'epoch': 1.68}

05/19/2024 03:28:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0555, 'learning_rate': 2.0266e-05, 'epoch': 1.68}

05/19/2024 03:29:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0625, 'learning_rate': 2.0171e-05, 'epoch': 1.69}

05/19/2024 03:29:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.1081, 'learning_rate': 2.0077e-05, 'epoch': 1.69}

05/19/2024 03:30:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0996, 'learning_rate': 1.9982e-05, 'epoch': 1.69}

05/19/2024 03:30:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0188, 'learning_rate': 1.9887e-05, 'epoch': 1.70}

05/19/2024 03:30:40 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 03:30:40 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 03:30:40 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 03:40:28 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2300

05/19/2024 03:40:29 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 03:40:29 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 03:40:29 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2300\tokenizer_config.json

05/19/2024 03:40:29 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2300\special_tokens_map.json

05/19/2024 03:40:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0287, 'learning_rate': 1.9793e-05, 'epoch': 1.70}

05/19/2024 03:41:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0315, 'learning_rate': 1.9698e-05, 'epoch': 1.70}

05/19/2024 03:41:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0921, 'learning_rate': 1.9604e-05, 'epoch': 1.71}

05/19/2024 03:42:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0145, 'learning_rate': 1.9510e-05, 'epoch': 1.71}

05/19/2024 03:42:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0232, 'learning_rate': 1.9415e-05, 'epoch': 1.72}

05/19/2024 03:43:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0924, 'learning_rate': 1.9321e-05, 'epoch': 1.72}

05/19/2024 03:43:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0671, 'learning_rate': 1.9227e-05, 'epoch': 1.72}

05/19/2024 03:44:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.0911, 'learning_rate': 1.9133e-05, 'epoch': 1.73}

05/19/2024 03:44:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.1335, 'learning_rate': 1.9039e-05, 'epoch': 1.73}

05/19/2024 03:44:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0537, 'learning_rate': 1.8946e-05, 'epoch': 1.73}

05/19/2024 03:45:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0624, 'learning_rate': 1.8852e-05, 'epoch': 1.74}

05/19/2024 03:45:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0953, 'learning_rate': 1.8758e-05, 'epoch': 1.74}

05/19/2024 03:46:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0684, 'learning_rate': 1.8665e-05, 'epoch': 1.75}

05/19/2024 03:46:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0625, 'learning_rate': 1.8572e-05, 'epoch': 1.75}

05/19/2024 03:47:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0623, 'learning_rate': 1.8478e-05, 'epoch': 1.75}

05/19/2024 03:47:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0932, 'learning_rate': 1.8385e-05, 'epoch': 1.76}

05/19/2024 03:48:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0610, 'learning_rate': 1.8292e-05, 'epoch': 1.76}

05/19/2024 03:48:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0632, 'learning_rate': 1.8199e-05, 'epoch': 1.76}

05/19/2024 03:49:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0465, 'learning_rate': 1.8106e-05, 'epoch': 1.77}

05/19/2024 03:49:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0585, 'learning_rate': 1.8013e-05, 'epoch': 1.77}

05/19/2024 03:49:28 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 03:49:28 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 03:49:28 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 03:59:24 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2400

05/19/2024 03:59:26 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 03:59:26 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 03:59:26 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2400\tokenizer_config.json

05/19/2024 03:59:26 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2400\special_tokens_map.json

05/19/2024 03:59:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0649, 'learning_rate': 1.7920e-05, 'epoch': 1.77}

05/19/2024 04:00:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0380, 'learning_rate': 1.7828e-05, 'epoch': 1.78}

05/19/2024 04:00:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0993, 'learning_rate': 1.7735e-05, 'epoch': 1.78}

05/19/2024 04:01:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0435, 'learning_rate': 1.7643e-05, 'epoch': 1.79}

05/19/2024 04:01:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0950, 'learning_rate': 1.7551e-05, 'epoch': 1.79}

05/19/2024 04:02:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0501, 'learning_rate': 1.7459e-05, 'epoch': 1.79}

05/19/2024 04:02:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0583, 'learning_rate': 1.7367e-05, 'epoch': 1.80}

05/19/2024 04:03:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0710, 'learning_rate': 1.7275e-05, 'epoch': 1.80}

05/19/2024 04:03:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0764, 'learning_rate': 1.7183e-05, 'epoch': 1.80}

05/19/2024 04:03:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0427, 'learning_rate': 1.7091e-05, 'epoch': 1.81}

05/19/2024 04:04:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0739, 'learning_rate': 1.6999e-05, 'epoch': 1.81}

05/19/2024 04:04:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0543, 'learning_rate': 1.6908e-05, 'epoch': 1.82}

05/19/2024 04:05:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0183, 'learning_rate': 1.6817e-05, 'epoch': 1.82}

05/19/2024 04:05:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0287, 'learning_rate': 1.6725e-05, 'epoch': 1.82}

05/19/2024 04:06:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0548, 'learning_rate': 1.6634e-05, 'epoch': 1.83}

05/19/2024 04:06:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0679, 'learning_rate': 1.6543e-05, 'epoch': 1.83}

05/19/2024 04:07:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0444, 'learning_rate': 1.6453e-05, 'epoch': 1.83}

05/19/2024 04:07:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0679, 'learning_rate': 1.6362e-05, 'epoch': 1.84}

05/19/2024 04:08:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0530, 'learning_rate': 1.6271e-05, 'epoch': 1.84}

05/19/2024 04:08:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0803, 'learning_rate': 1.6181e-05, 'epoch': 1.85}

05/19/2024 04:08:29 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 04:08:29 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 04:08:29 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 04:18:18 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2500

05/19/2024 04:18:19 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 04:18:19 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 04:18:19 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2500\tokenizer_config.json

05/19/2024 04:18:19 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2500\special_tokens_map.json

05/19/2024 04:18:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0327, 'learning_rate': 1.6090e-05, 'epoch': 1.85}

05/19/2024 04:19:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0449, 'learning_rate': 1.6000e-05, 'epoch': 1.85}

05/19/2024 04:19:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0756, 'learning_rate': 1.5910e-05, 'epoch': 1.86}

05/19/2024 04:20:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0503, 'learning_rate': 1.5820e-05, 'epoch': 1.86}

05/19/2024 04:20:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0662, 'learning_rate': 1.5730e-05, 'epoch': 1.86}

05/19/2024 04:21:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0765, 'learning_rate': 1.5641e-05, 'epoch': 1.87}

05/19/2024 04:21:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0674, 'learning_rate': 1.5551e-05, 'epoch': 1.87}

05/19/2024 04:21:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0562, 'learning_rate': 1.5462e-05, 'epoch': 1.87}

05/19/2024 04:22:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0858, 'learning_rate': 1.5373e-05, 'epoch': 1.88}

05/19/2024 04:22:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0844, 'learning_rate': 1.5284e-05, 'epoch': 1.88}

05/19/2024 04:23:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0403, 'learning_rate': 1.5195e-05, 'epoch': 1.89}

05/19/2024 04:23:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0193, 'learning_rate': 1.5106e-05, 'epoch': 1.89}

05/19/2024 04:24:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0172, 'learning_rate': 1.5017e-05, 'epoch': 1.89}

05/19/2024 04:24:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.1240, 'learning_rate': 1.4929e-05, 'epoch': 1.90}

05/19/2024 04:25:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0271, 'learning_rate': 1.4840e-05, 'epoch': 1.90}

05/19/2024 04:25:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0559, 'learning_rate': 1.4752e-05, 'epoch': 1.90}

05/19/2024 04:26:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0412, 'learning_rate': 1.4664e-05, 'epoch': 1.91}

05/19/2024 04:26:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.0451, 'learning_rate': 1.4576e-05, 'epoch': 1.91}

05/19/2024 04:26:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0347, 'learning_rate': 1.4489e-05, 'epoch': 1.92}

05/19/2024 04:27:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0327, 'learning_rate': 1.4401e-05, 'epoch': 1.92}

05/19/2024 04:27:21 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 04:27:21 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 04:27:21 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 04:37:11 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2600

05/19/2024 04:37:12 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 04:37:12 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 04:37:12 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2600\tokenizer_config.json

05/19/2024 04:37:12 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2600\special_tokens_map.json

05/19/2024 04:37:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0381, 'learning_rate': 1.4314e-05, 'epoch': 1.92}

05/19/2024 04:38:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0247, 'learning_rate': 1.4226e-05, 'epoch': 1.93}

05/19/2024 04:38:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0854, 'learning_rate': 1.4139e-05, 'epoch': 1.93}

05/19/2024 04:39:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0901, 'learning_rate': 1.4052e-05, 'epoch': 1.93}

05/19/2024 04:39:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0403, 'learning_rate': 1.3966e-05, 'epoch': 1.94}

05/19/2024 04:39:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0659, 'learning_rate': 1.3879e-05, 'epoch': 1.94}

05/19/2024 04:40:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0761, 'learning_rate': 1.3792e-05, 'epoch': 1.94}

05/19/2024 04:40:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0168, 'learning_rate': 1.3706e-05, 'epoch': 1.95}

05/19/2024 04:41:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0953, 'learning_rate': 1.3620e-05, 'epoch': 1.95}

05/19/2024 04:41:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0420, 'learning_rate': 1.3534e-05, 'epoch': 1.96}

05/19/2024 04:42:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0744, 'learning_rate': 1.3448e-05, 'epoch': 1.96}

05/19/2024 04:42:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0597, 'learning_rate': 1.3363e-05, 'epoch': 1.96}

05/19/2024 04:43:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0446, 'learning_rate': 1.3277e-05, 'epoch': 1.97}

05/19/2024 04:43:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0396, 'learning_rate': 1.3192e-05, 'epoch': 1.97}

05/19/2024 04:43:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0504, 'learning_rate': 1.3107e-05, 'epoch': 1.97}

05/19/2024 04:44:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0898, 'learning_rate': 1.3022e-05, 'epoch': 1.98}

05/19/2024 04:44:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0354, 'learning_rate': 1.2938e-05, 'epoch': 1.98}

05/19/2024 04:45:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0396, 'learning_rate': 1.2853e-05, 'epoch': 1.99}

05/19/2024 04:45:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0497, 'learning_rate': 1.2769e-05, 'epoch': 1.99}

05/19/2024 04:46:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0285, 'learning_rate': 1.2685e-05, 'epoch': 1.99}

05/19/2024 04:46:13 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 04:46:13 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 04:46:13 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 04:56:07 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2700

05/19/2024 04:56:08 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 04:56:08 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 04:56:08 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2700\tokenizer_config.json

05/19/2024 04:56:08 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2700\special_tokens_map.json

05/19/2024 04:56:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0248, 'learning_rate': 1.2601e-05, 'epoch': 2.00}

05/19/2024 04:57:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0856, 'learning_rate': 1.2517e-05, 'epoch': 2.00}

05/19/2024 04:57:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0362, 'learning_rate': 1.2433e-05, 'epoch': 2.00}

05/19/2024 04:57:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0722, 'learning_rate': 1.2350e-05, 'epoch': 2.01}

05/19/2024 04:58:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0674, 'learning_rate': 1.2266e-05, 'epoch': 2.01}

05/19/2024 04:58:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0307, 'learning_rate': 1.2183e-05, 'epoch': 2.01}

05/19/2024 04:59:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0497, 'learning_rate': 1.2101e-05, 'epoch': 2.02}

05/19/2024 04:59:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0310, 'learning_rate': 1.2018e-05, 'epoch': 2.02}

05/19/2024 05:00:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0192, 'learning_rate': 1.1935e-05, 'epoch': 2.03}

05/19/2024 05:00:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0776, 'learning_rate': 1.1853e-05, 'epoch': 2.03}

05/19/2024 05:01:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0195, 'learning_rate': 1.1771e-05, 'epoch': 2.03}

05/19/2024 05:01:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0492, 'learning_rate': 1.1689e-05, 'epoch': 2.04}

05/19/2024 05:02:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0751, 'learning_rate': 1.1608e-05, 'epoch': 2.04}

05/19/2024 05:02:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0367, 'learning_rate': 1.1526e-05, 'epoch': 2.04}

05/19/2024 05:02:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.0476, 'learning_rate': 1.1445e-05, 'epoch': 2.05}

05/19/2024 05:03:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0222, 'learning_rate': 1.1364e-05, 'epoch': 2.05}

05/19/2024 05:03:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0385, 'learning_rate': 1.1283e-05, 'epoch': 2.06}

05/19/2024 05:04:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0399, 'learning_rate': 1.1202e-05, 'epoch': 2.06}

05/19/2024 05:04:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0592, 'learning_rate': 1.1122e-05, 'epoch': 2.06}

05/19/2024 05:05:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0286, 'learning_rate': 1.1042e-05, 'epoch': 2.07}

05/19/2024 05:05:12 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 05:05:12 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 05:05:12 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 05:15:04 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2800

05/19/2024 05:15:06 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 05:15:06 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 05:15:06 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2800\tokenizer_config.json

05/19/2024 05:15:06 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2800\special_tokens_map.json

05/19/2024 05:15:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0255, 'learning_rate': 1.0962e-05, 'epoch': 2.07}

05/19/2024 05:16:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0542, 'learning_rate': 1.0882e-05, 'epoch': 2.07}

05/19/2024 05:16:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.0743, 'learning_rate': 1.0802e-05, 'epoch': 2.08}

05/19/2024 05:16:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0479, 'learning_rate': 1.0723e-05, 'epoch': 2.08}

05/19/2024 05:17:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0694, 'learning_rate': 1.0643e-05, 'epoch': 2.08}

05/19/2024 05:17:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0837, 'learning_rate': 1.0564e-05, 'epoch': 2.09}

05/19/2024 05:18:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0440, 'learning_rate': 1.0486e-05, 'epoch': 2.09}

05/19/2024 05:18:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0609, 'learning_rate': 1.0407e-05, 'epoch': 2.10}

05/19/2024 05:19:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0258, 'learning_rate': 1.0329e-05, 'epoch': 2.10}

05/19/2024 05:19:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0901, 'learning_rate': 1.0251e-05, 'epoch': 2.10}

05/19/2024 05:20:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0415, 'learning_rate': 1.0173e-05, 'epoch': 2.11}

05/19/2024 05:20:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0392, 'learning_rate': 1.0095e-05, 'epoch': 2.11}

05/19/2024 05:20:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0522, 'learning_rate': 1.0018e-05, 'epoch': 2.11}

05/19/2024 05:21:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0501, 'learning_rate': 9.9405e-06, 'epoch': 2.12}

05/19/2024 05:21:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0347, 'learning_rate': 9.8635e-06, 'epoch': 2.12}

05/19/2024 05:22:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0420, 'learning_rate': 9.7867e-06, 'epoch': 2.13}

05/19/2024 05:22:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0586, 'learning_rate': 9.7102e-06, 'epoch': 2.13}

05/19/2024 05:23:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0273, 'learning_rate': 9.6339e-06, 'epoch': 2.13}

05/19/2024 05:23:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0328, 'learning_rate': 9.5578e-06, 'epoch': 2.14}

05/19/2024 05:24:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0607, 'learning_rate': 9.4819e-06, 'epoch': 2.14}

05/19/2024 05:24:10 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 05:24:10 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 05:24:10 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 05:34:05 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2900

05/19/2024 05:34:07 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 05:34:07 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 05:34:07 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2900\tokenizer_config.json

05/19/2024 05:34:07 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2900\special_tokens_map.json

05/19/2024 05:34:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0444, 'learning_rate': 9.4063e-06, 'epoch': 2.14}

05/19/2024 05:35:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0145, 'learning_rate': 9.3309e-06, 'epoch': 2.15}

05/19/2024 05:35:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0476, 'learning_rate': 9.2557e-06, 'epoch': 2.15}

05/19/2024 05:35:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0799, 'learning_rate': 9.1808e-06, 'epoch': 2.15}

05/19/2024 05:36:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0254, 'learning_rate': 9.1061e-06, 'epoch': 2.16}

05/19/2024 05:36:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0531, 'learning_rate': 9.0317e-06, 'epoch': 2.16}

05/19/2024 05:37:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0458, 'learning_rate': 8.9575e-06, 'epoch': 2.17}

05/19/2024 05:37:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0429, 'learning_rate': 8.8835e-06, 'epoch': 2.17}

05/19/2024 05:38:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0654, 'learning_rate': 8.8098e-06, 'epoch': 2.17}

05/19/2024 05:38:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0457, 'learning_rate': 8.7363e-06, 'epoch': 2.18}

05/19/2024 05:39:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0750, 'learning_rate': 8.6630e-06, 'epoch': 2.18}

05/19/2024 05:39:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0116, 'learning_rate': 8.5900e-06, 'epoch': 2.18}

05/19/2024 05:40:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0609, 'learning_rate': 8.5173e-06, 'epoch': 2.19}

05/19/2024 05:40:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0230, 'learning_rate': 8.4448e-06, 'epoch': 2.19}

05/19/2024 05:40:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0517, 'learning_rate': 8.3725e-06, 'epoch': 2.20}

05/19/2024 05:41:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0567, 'learning_rate': 8.3005e-06, 'epoch': 2.20}

05/19/2024 05:41:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0427, 'learning_rate': 8.2287e-06, 'epoch': 2.20}

05/19/2024 05:42:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0448, 'learning_rate': 8.1572e-06, 'epoch': 2.21}

05/19/2024 05:42:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0318, 'learning_rate': 8.0859e-06, 'epoch': 2.21}

05/19/2024 05:43:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0622, 'learning_rate': 8.0149e-06, 'epoch': 2.21}

05/19/2024 05:43:13 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 05:43:13 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 05:43:13 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 05:53:06 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3000

05/19/2024 05:53:08 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 05:53:08 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 05:53:08 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3000\tokenizer_config.json

05/19/2024 05:53:08 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3000\special_tokens_map.json

05/19/2024 05:53:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0360, 'learning_rate': 7.9442e-06, 'epoch': 2.22}

05/19/2024 05:54:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0475, 'learning_rate': 7.8737e-06, 'epoch': 2.22}

05/19/2024 05:54:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0436, 'learning_rate': 7.8034e-06, 'epoch': 2.23}

05/19/2024 05:54:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0236, 'learning_rate': 7.7334e-06, 'epoch': 2.23}

05/19/2024 05:55:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0572, 'learning_rate': 7.6637e-06, 'epoch': 2.23}

05/19/2024 05:55:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0603, 'learning_rate': 7.5942e-06, 'epoch': 2.24}

05/19/2024 05:56:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0539, 'learning_rate': 7.5250e-06, 'epoch': 2.24}

05/19/2024 05:56:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0179, 'learning_rate': 7.4560e-06, 'epoch': 2.24}

05/19/2024 05:57:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0382, 'learning_rate': 7.3873e-06, 'epoch': 2.25}

05/19/2024 05:57:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0465, 'learning_rate': 7.3189e-06, 'epoch': 2.25}

05/19/2024 05:58:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0462, 'learning_rate': 7.2508e-06, 'epoch': 2.25}

05/19/2024 05:58:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0538, 'learning_rate': 7.1829e-06, 'epoch': 2.26}

05/19/2024 05:59:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0714, 'learning_rate': 7.1152e-06, 'epoch': 2.26}

05/19/2024 05:59:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.0421, 'learning_rate': 7.0479e-06, 'epoch': 2.27}

05/19/2024 05:59:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0558, 'learning_rate': 6.9808e-06, 'epoch': 2.27}

05/19/2024 06:00:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0624, 'learning_rate': 6.9139e-06, 'epoch': 2.27}

05/19/2024 06:00:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0083, 'learning_rate': 6.8474e-06, 'epoch': 2.28}

05/19/2024 06:01:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0389, 'learning_rate': 6.7811e-06, 'epoch': 2.28}

05/19/2024 06:01:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0268, 'learning_rate': 6.7151e-06, 'epoch': 2.28}

05/19/2024 06:02:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0165, 'learning_rate': 6.6493e-06, 'epoch': 2.29}

05/19/2024 06:02:08 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 06:02:08 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 06:02:08 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 06:12:07 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3100

05/19/2024 06:12:08 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 06:12:08 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 06:12:08 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3100\tokenizer_config.json

05/19/2024 06:12:08 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3100\special_tokens_map.json

05/19/2024 06:12:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0652, 'learning_rate': 6.5838e-06, 'epoch': 2.29}

05/19/2024 06:13:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0662, 'learning_rate': 6.5187e-06, 'epoch': 2.30}

05/19/2024 06:13:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0166, 'learning_rate': 6.4537e-06, 'epoch': 2.30}

05/19/2024 06:13:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0256, 'learning_rate': 6.3891e-06, 'epoch': 2.30}

05/19/2024 06:14:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0792, 'learning_rate': 6.3247e-06, 'epoch': 2.31}

05/19/2024 06:14:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0790, 'learning_rate': 6.2606e-06, 'epoch': 2.31}

05/19/2024 06:15:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0737, 'learning_rate': 6.1968e-06, 'epoch': 2.31}

05/19/2024 06:15:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.0534, 'learning_rate': 6.1333e-06, 'epoch': 2.32}

05/19/2024 06:16:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0365, 'learning_rate': 6.0701e-06, 'epoch': 2.32}

05/19/2024 06:16:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0688, 'learning_rate': 6.0071e-06, 'epoch': 2.32}

05/19/2024 06:17:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0364, 'learning_rate': 5.9444e-06, 'epoch': 2.33}

05/19/2024 06:17:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.1010, 'learning_rate': 5.8820e-06, 'epoch': 2.33}

05/19/2024 06:18:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0701, 'learning_rate': 5.8199e-06, 'epoch': 2.34}

05/19/2024 06:18:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0381, 'learning_rate': 5.7581e-06, 'epoch': 2.34}

05/19/2024 06:18:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0895, 'learning_rate': 5.6966e-06, 'epoch': 2.34}

05/19/2024 06:19:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0330, 'learning_rate': 5.6353e-06, 'epoch': 2.35}

05/19/2024 06:19:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0609, 'learning_rate': 5.5744e-06, 'epoch': 2.35}

05/19/2024 06:20:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0893, 'learning_rate': 5.5137e-06, 'epoch': 2.35}

05/19/2024 06:20:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0379, 'learning_rate': 5.4534e-06, 'epoch': 2.36}

05/19/2024 06:21:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0663, 'learning_rate': 5.3933e-06, 'epoch': 2.36}

05/19/2024 06:21:10 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 06:21:10 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 06:21:10 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 06:31:05 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3200

05/19/2024 06:31:07 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 06:31:07 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 06:31:07 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3200\tokenizer_config.json

05/19/2024 06:31:07 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3200\special_tokens_map.json

05/19/2024 06:31:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0249, 'learning_rate': 5.3335e-06, 'epoch': 2.37}

05/19/2024 06:32:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0672, 'learning_rate': 5.2740e-06, 'epoch': 2.37}

05/19/2024 06:32:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0765, 'learning_rate': 5.2148e-06, 'epoch': 2.37}

05/19/2024 06:32:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0209, 'learning_rate': 5.1559e-06, 'epoch': 2.38}

05/19/2024 06:33:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0276, 'learning_rate': 5.0973e-06, 'epoch': 2.38}

05/19/2024 06:33:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0513, 'learning_rate': 5.0390e-06, 'epoch': 2.38}

05/19/2024 06:34:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0378, 'learning_rate': 4.9809e-06, 'epoch': 2.39}

05/19/2024 06:34:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0495, 'learning_rate': 4.9232e-06, 'epoch': 2.39}

05/19/2024 06:35:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0357, 'learning_rate': 4.8658e-06, 'epoch': 2.39}

05/19/2024 06:35:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0409, 'learning_rate': 4.8087e-06, 'epoch': 2.40}

05/19/2024 06:36:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0280, 'learning_rate': 4.7519e-06, 'epoch': 2.40}

05/19/2024 06:36:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0314, 'learning_rate': 4.6954e-06, 'epoch': 2.41}

05/19/2024 06:36:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0136, 'learning_rate': 4.6392e-06, 'epoch': 2.41}

05/19/2024 06:37:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0265, 'learning_rate': 4.5833e-06, 'epoch': 2.41}

05/19/2024 06:37:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0744, 'learning_rate': 4.5277e-06, 'epoch': 2.42}

05/19/2024 06:38:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0178, 'learning_rate': 4.4724e-06, 'epoch': 2.42}

05/19/2024 06:38:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0709, 'learning_rate': 4.4174e-06, 'epoch': 2.42}

05/19/2024 06:39:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0533, 'learning_rate': 4.3627e-06, 'epoch': 2.43}

05/19/2024 06:39:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0304, 'learning_rate': 4.3083e-06, 'epoch': 2.43}

05/19/2024 06:40:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0936, 'learning_rate': 4.2543e-06, 'epoch': 2.44}

05/19/2024 06:40:11 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 06:40:11 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 06:40:11 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 06:50:05 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3300

05/19/2024 06:50:06 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 06:50:06 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 06:50:06 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3300\tokenizer_config.json

05/19/2024 06:50:06 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3300\special_tokens_map.json

05/19/2024 06:50:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0510, 'learning_rate': 4.2005e-06, 'epoch': 2.44}

05/19/2024 06:51:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0486, 'learning_rate': 4.1471e-06, 'epoch': 2.44}

05/19/2024 06:51:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0555, 'learning_rate': 4.0939e-06, 'epoch': 2.45}

05/19/2024 06:51:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0416, 'learning_rate': 4.0411e-06, 'epoch': 2.45}

05/19/2024 06:52:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0303, 'learning_rate': 3.9886e-06, 'epoch': 2.45}

05/19/2024 06:52:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0617, 'learning_rate': 3.9364e-06, 'epoch': 2.46}

05/19/2024 06:53:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0498, 'learning_rate': 3.8846e-06, 'epoch': 2.46}

05/19/2024 06:53:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0376, 'learning_rate': 3.8330e-06, 'epoch': 2.46}

05/19/2024 06:54:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0391, 'learning_rate': 3.7818e-06, 'epoch': 2.47}

05/19/2024 06:54:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0728, 'learning_rate': 3.7308e-06, 'epoch': 2.47}

05/19/2024 06:55:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0546, 'learning_rate': 3.6802e-06, 'epoch': 2.48}

05/19/2024 06:55:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.1073, 'learning_rate': 3.6299e-06, 'epoch': 2.48}

05/19/2024 06:56:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0469, 'learning_rate': 3.5799e-06, 'epoch': 2.48}

05/19/2024 06:56:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0345, 'learning_rate': 3.5303e-06, 'epoch': 2.49}

05/19/2024 06:56:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0323, 'learning_rate': 3.4810e-06, 'epoch': 2.49}

05/19/2024 06:57:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0761, 'learning_rate': 3.4319e-06, 'epoch': 2.49}

05/19/2024 06:57:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0338, 'learning_rate': 3.3833e-06, 'epoch': 2.50}

05/19/2024 06:58:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0793, 'learning_rate': 3.3349e-06, 'epoch': 2.50}

05/19/2024 06:58:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0462, 'learning_rate': 3.2868e-06, 'epoch': 2.51}

05/19/2024 06:59:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0279, 'learning_rate': 3.2391e-06, 'epoch': 2.51}

05/19/2024 06:59:16 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 06:59:16 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 06:59:16 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 07:09:15 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3400

05/19/2024 07:09:17 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 07:09:17 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 07:09:17 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3400\tokenizer_config.json

05/19/2024 07:09:17 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3400\special_tokens_map.json

05/19/2024 07:09:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0554, 'learning_rate': 3.1917e-06, 'epoch': 2.51}

05/19/2024 07:10:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.0729, 'learning_rate': 3.1447e-06, 'epoch': 2.52}

05/19/2024 07:10:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0541, 'learning_rate': 3.0979e-06, 'epoch': 2.52}

05/19/2024 07:11:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0392, 'learning_rate': 3.0515e-06, 'epoch': 2.52}

05/19/2024 07:11:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0188, 'learning_rate': 3.0054e-06, 'epoch': 2.53}

05/19/2024 07:12:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0893, 'learning_rate': 2.9597e-06, 'epoch': 2.53}

05/19/2024 07:12:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0564, 'learning_rate': 2.9142e-06, 'epoch': 2.54}

05/19/2024 07:12:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0456, 'learning_rate': 2.8691e-06, 'epoch': 2.54}

05/19/2024 07:13:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0504, 'learning_rate': 2.8244e-06, 'epoch': 2.54}

05/19/2024 07:13:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0821, 'learning_rate': 2.7799e-06, 'epoch': 2.55}

05/19/2024 07:14:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.0482, 'learning_rate': 2.7358e-06, 'epoch': 2.55}

05/19/2024 07:14:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0444, 'learning_rate': 2.6920e-06, 'epoch': 2.55}

05/19/2024 07:15:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0472, 'learning_rate': 2.6486e-06, 'epoch': 2.56}

05/19/2024 07:15:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0667, 'learning_rate': 2.6055e-06, 'epoch': 2.56}

05/19/2024 07:16:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0802, 'learning_rate': 2.5627e-06, 'epoch': 2.56}

05/19/2024 07:16:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0344, 'learning_rate': 2.5203e-06, 'epoch': 2.57}

05/19/2024 07:17:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0552, 'learning_rate': 2.4782e-06, 'epoch': 2.57}

05/19/2024 07:17:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.0275, 'learning_rate': 2.4364e-06, 'epoch': 2.58}

05/19/2024 07:17:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0412, 'learning_rate': 2.3950e-06, 'epoch': 2.58}

05/19/2024 07:18:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0301, 'learning_rate': 2.3539e-06, 'epoch': 2.58}

05/19/2024 07:18:24 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 07:18:24 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 07:18:24 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 07:28:25 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3500

05/19/2024 07:28:27 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 07:28:27 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 07:28:27 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3500\tokenizer_config.json

05/19/2024 07:28:27 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3500\special_tokens_map.json

05/19/2024 07:28:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.0354, 'learning_rate': 2.3131e-06, 'epoch': 2.59}

05/19/2024 07:29:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0137, 'learning_rate': 2.2727e-06, 'epoch': 2.59}

05/19/2024 07:29:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0277, 'learning_rate': 2.2326e-06, 'epoch': 2.59}

05/19/2024 07:30:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0526, 'learning_rate': 2.1929e-06, 'epoch': 2.60}

05/19/2024 07:30:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.0791, 'learning_rate': 2.1535e-06, 'epoch': 2.60}

05/19/2024 07:31:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.1120, 'learning_rate': 2.1145e-06, 'epoch': 2.61}

05/19/2024 07:31:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.0353, 'learning_rate': 2.0757e-06, 'epoch': 2.61}

05/19/2024 07:32:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.0301, 'learning_rate': 2.0374e-06, 'epoch': 2.61}

05/19/2024 07:32:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0801, 'learning_rate': 1.9993e-06, 'epoch': 2.62}

05/19/2024 07:33:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0288, 'learning_rate': 1.9617e-06, 'epoch': 2.62}

05/19/2024 07:33:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0369, 'learning_rate': 1.9243e-06, 'epoch': 2.62}

05/19/2024 07:33:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0946, 'learning_rate': 1.8873e-06, 'epoch': 2.63}

05/19/2024 07:34:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0413, 'learning_rate': 1.8507e-06, 'epoch': 2.63}

05/19/2024 07:34:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.0567, 'learning_rate': 1.8144e-06, 'epoch': 2.63}

05/19/2024 07:35:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0718, 'learning_rate': 1.7784e-06, 'epoch': 2.64}

05/19/2024 07:35:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0481, 'learning_rate': 1.7428e-06, 'epoch': 2.64}

05/19/2024 07:36:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0932, 'learning_rate': 1.7075e-06, 'epoch': 2.65}

05/19/2024 07:36:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0290, 'learning_rate': 1.6726e-06, 'epoch': 2.65}

05/19/2024 07:37:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0415, 'learning_rate': 1.6381e-06, 'epoch': 2.65}

05/19/2024 07:37:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0397, 'learning_rate': 1.6038e-06, 'epoch': 2.66}

05/19/2024 07:37:37 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 07:37:37 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 07:37:37 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 07:47:37 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3600

05/19/2024 07:47:38 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 07:47:38 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 07:47:38 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3600\tokenizer_config.json

05/19/2024 07:47:38 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3600\special_tokens_map.json

05/19/2024 07:48:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0514, 'learning_rate': 1.5700e-06, 'epoch': 2.66}

05/19/2024 07:48:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0438, 'learning_rate': 1.5365e-06, 'epoch': 2.66}

05/19/2024 07:48:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0566, 'learning_rate': 1.5033e-06, 'epoch': 2.67}

05/19/2024 07:49:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0349, 'learning_rate': 1.4705e-06, 'epoch': 2.67}

05/19/2024 07:49:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0672, 'learning_rate': 1.4380e-06, 'epoch': 2.68}

05/19/2024 07:50:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0527, 'learning_rate': 1.4059e-06, 'epoch': 2.68}

05/19/2024 07:50:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0607, 'learning_rate': 1.3741e-06, 'epoch': 2.68}

05/19/2024 07:51:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0287, 'learning_rate': 1.3427e-06, 'epoch': 2.69}

05/19/2024 07:51:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0243, 'learning_rate': 1.3117e-06, 'epoch': 2.69}

05/19/2024 07:52:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0467, 'learning_rate': 1.2809e-06, 'epoch': 2.69}

05/19/2024 07:52:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0322, 'learning_rate': 1.2506e-06, 'epoch': 2.70}

05/19/2024 07:53:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0729, 'learning_rate': 1.2206e-06, 'epoch': 2.70}

05/19/2024 07:53:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0360, 'learning_rate': 1.1910e-06, 'epoch': 2.70}

05/19/2024 07:54:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0174, 'learning_rate': 1.1617e-06, 'epoch': 2.71}

05/19/2024 07:54:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0595, 'learning_rate': 1.1328e-06, 'epoch': 2.71}

05/19/2024 07:54:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0974, 'learning_rate': 1.1042e-06, 'epoch': 2.72}

05/19/2024 07:55:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0445, 'learning_rate': 1.0760e-06, 'epoch': 2.72}

05/19/2024 07:55:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0576, 'learning_rate': 1.0481e-06, 'epoch': 2.72}

05/19/2024 07:56:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0457, 'learning_rate': 1.0206e-06, 'epoch': 2.73}

05/19/2024 07:56:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0382, 'learning_rate': 9.9347e-07, 'epoch': 2.73}

05/19/2024 07:56:44 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 07:56:44 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 07:56:44 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 08:06:42 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3700

05/19/2024 08:06:43 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 08:06:43 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 08:06:43 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3700\tokenizer_config.json

05/19/2024 08:06:43 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3700\special_tokens_map.json

05/19/2024 08:07:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0424, 'learning_rate': 9.6668e-07, 'epoch': 2.73}

05/19/2024 08:07:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.0649, 'learning_rate': 9.4026e-07, 'epoch': 2.74}

05/19/2024 08:08:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0169, 'learning_rate': 9.1419e-07, 'epoch': 2.74}

05/19/2024 08:08:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0590, 'learning_rate': 8.8849e-07, 'epoch': 2.75}

05/19/2024 08:09:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0395, 'learning_rate': 8.6314e-07, 'epoch': 2.75}

05/19/2024 08:09:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0491, 'learning_rate': 8.3816e-07, 'epoch': 2.75}

05/19/2024 08:09:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.0427, 'learning_rate': 8.1353e-07, 'epoch': 2.76}

05/19/2024 08:10:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.0332, 'learning_rate': 7.8927e-07, 'epoch': 2.76}

05/19/2024 08:10:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0760, 'learning_rate': 7.6537e-07, 'epoch': 2.76}

05/19/2024 08:11:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0342, 'learning_rate': 7.4183e-07, 'epoch': 2.77}

05/19/2024 08:11:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.0465, 'learning_rate': 7.1865e-07, 'epoch': 2.77}

05/19/2024 08:12:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0484, 'learning_rate': 6.9584e-07, 'epoch': 2.77}

05/19/2024 08:12:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0470, 'learning_rate': 6.7338e-07, 'epoch': 2.78}

05/19/2024 08:13:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.0853, 'learning_rate': 6.5129e-07, 'epoch': 2.78}

05/19/2024 08:13:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0541, 'learning_rate': 6.2957e-07, 'epoch': 2.79}

05/19/2024 08:14:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.0332, 'learning_rate': 6.0821e-07, 'epoch': 2.79}

05/19/2024 08:14:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.0693, 'learning_rate': 5.8721e-07, 'epoch': 2.79}

05/19/2024 08:15:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.0489, 'learning_rate': 5.6658e-07, 'epoch': 2.80}

05/19/2024 08:15:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.0296, 'learning_rate': 5.4631e-07, 'epoch': 2.80}

05/19/2024 08:15:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0642, 'learning_rate': 5.2641e-07, 'epoch': 2.80}

05/19/2024 08:15:57 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 08:15:57 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 08:15:57 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 08:25:54 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3800

05/19/2024 08:25:56 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 08:25:56 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 08:25:56 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3800\tokenizer_config.json

05/19/2024 08:25:56 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3800\special_tokens_map.json

05/19/2024 08:26:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.0252, 'learning_rate': 5.0687e-07, 'epoch': 2.81}

05/19/2024 08:26:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.0500, 'learning_rate': 4.8770e-07, 'epoch': 2.81}

05/19/2024 08:27:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.0308, 'learning_rate': 4.6889e-07, 'epoch': 2.82}

05/19/2024 08:27:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0370, 'learning_rate': 4.5045e-07, 'epoch': 2.82}

05/19/2024 08:28:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0709, 'learning_rate': 4.3238e-07, 'epoch': 2.82}

05/19/2024 08:28:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0177, 'learning_rate': 4.1468e-07, 'epoch': 2.83}

05/19/2024 08:29:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0430, 'learning_rate': 3.9734e-07, 'epoch': 2.83}

05/19/2024 08:29:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.0793, 'learning_rate': 3.8037e-07, 'epoch': 2.83}

05/19/2024 08:30:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0319, 'learning_rate': 3.6376e-07, 'epoch': 2.84}

05/19/2024 08:30:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.0450, 'learning_rate': 3.4753e-07, 'epoch': 2.84}

05/19/2024 08:31:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.0413, 'learning_rate': 3.3166e-07, 'epoch': 2.85}

05/19/2024 08:31:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0395, 'learning_rate': 3.1616e-07, 'epoch': 2.85}

05/19/2024 08:31:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.0681, 'learning_rate': 3.0103e-07, 'epoch': 2.85}

05/19/2024 08:32:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0550, 'learning_rate': 2.8627e-07, 'epoch': 2.86}

05/19/2024 08:32:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0814, 'learning_rate': 2.7187e-07, 'epoch': 2.86}

05/19/2024 08:33:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.0555, 'learning_rate': 2.5785e-07, 'epoch': 2.86}

05/19/2024 08:33:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.0385, 'learning_rate': 2.4420e-07, 'epoch': 2.87}

05/19/2024 08:34:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.0320, 'learning_rate': 2.3091e-07, 'epoch': 2.87}

05/19/2024 08:34:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0431, 'learning_rate': 2.1800e-07, 'epoch': 2.87}

05/19/2024 08:35:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0658, 'learning_rate': 2.0545e-07, 'epoch': 2.88}

05/19/2024 08:35:07 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 08:35:07 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 08:35:07 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 08:45:05 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3900

05/19/2024 08:45:06 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 08:45:06 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 08:45:06 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3900\tokenizer_config.json

05/19/2024 08:45:06 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-3900\special_tokens_map.json

05/19/2024 08:45:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.0692, 'learning_rate': 1.9328e-07, 'epoch': 2.88}

05/19/2024 08:46:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.0522, 'learning_rate': 1.8147e-07, 'epoch': 2.89}

05/19/2024 08:46:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.0554, 'learning_rate': 1.7004e-07, 'epoch': 2.89}

05/19/2024 08:46:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.0753, 'learning_rate': 1.5898e-07, 'epoch': 2.89}

05/19/2024 08:47:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0282, 'learning_rate': 1.4828e-07, 'epoch': 2.90}

05/19/2024 08:47:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0159, 'learning_rate': 1.3796e-07, 'epoch': 2.90}

05/19/2024 08:48:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.0319, 'learning_rate': 1.2801e-07, 'epoch': 2.90}

05/19/2024 08:48:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.0088, 'learning_rate': 1.1844e-07, 'epoch': 2.91}

05/19/2024 08:49:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.0504, 'learning_rate': 1.0923e-07, 'epoch': 2.91}

05/19/2024 08:49:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.0597, 'learning_rate': 1.0040e-07, 'epoch': 2.92}

05/19/2024 08:50:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.0329, 'learning_rate': 9.1932e-08, 'epoch': 2.92}

05/19/2024 08:50:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.0611, 'learning_rate': 8.3841e-08, 'epoch': 2.92}

05/19/2024 08:51:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.0782, 'learning_rate': 7.6122e-08, 'epoch': 2.93}

05/19/2024 08:51:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.0416, 'learning_rate': 6.8775e-08, 'epoch': 2.93}

05/19/2024 08:51:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.0363, 'learning_rate': 6.1801e-08, 'epoch': 2.93}

05/19/2024 08:52:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.0647, 'learning_rate': 5.5198e-08, 'epoch': 2.94}

05/19/2024 08:52:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.0298, 'learning_rate': 4.8969e-08, 'epoch': 2.94}

05/19/2024 08:53:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.0279, 'learning_rate': 4.3111e-08, 'epoch': 2.94}

05/19/2024 08:53:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.0340, 'learning_rate': 3.7627e-08, 'epoch': 2.95}

05/19/2024 08:54:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.0881, 'learning_rate': 3.2515e-08, 'epoch': 2.95}

05/19/2024 08:54:17 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 08:54:17 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 08:54:17 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 09:04:19 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-4000

05/19/2024 09:04:20 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 09:04:20 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 09:04:20 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-4000\tokenizer_config.json

05/19/2024 09:04:20 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-4000\special_tokens_map.json

05/19/2024 09:04:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.0279, 'learning_rate': 2.7776e-08, 'epoch': 2.96}

05/19/2024 09:05:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.0345, 'learning_rate': 2.3410e-08, 'epoch': 2.96}

05/19/2024 09:05:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.0697, 'learning_rate': 1.9417e-08, 'epoch': 2.96}

05/19/2024 09:06:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.0780, 'learning_rate': 1.5796e-08, 'epoch': 2.97}

05/19/2024 09:06:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.0740, 'learning_rate': 1.2549e-08, 'epoch': 2.97}

05/19/2024 09:07:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.0320, 'learning_rate': 9.6753e-09, 'epoch': 2.97}

05/19/2024 09:07:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.0646, 'learning_rate': 7.1745e-09, 'epoch': 2.98}

05/19/2024 09:08:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.0682, 'learning_rate': 5.0469e-09, 'epoch': 2.98}

05/19/2024 09:08:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.0368, 'learning_rate': 3.2924e-09, 'epoch': 2.99}

05/19/2024 09:08:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.0929, 'learning_rate': 2.1576e-09, 'epoch': 2.99}

05/19/2024 09:09:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.0350, 'learning_rate': 1.0751e-09, 'epoch': 2.99}

05/19/2024 09:09:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.0430, 'learning_rate': 3.6583e-10, 'epoch': 3.00}

05/19/2024 09:10:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.0313, 'learning_rate': 2.9864e-11, 'epoch': 3.00}

05/19/2024 09:10:18 - INFO - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)



05/19/2024 09:10:18 - INFO - transformers.trainer - Loading best model from saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\checkpoint-2600 (score: 0.06363344937562943).

05/19/2024 09:10:18 - INFO - transformers.trainer - Saving model checkpoint to saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35

05/19/2024 09:10:20 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--Qwen--Qwen1.5-0.5B\snapshots\8f445e3628f3500ee69f24e1303c9f10f5342a39\config.json

05/19/2024 09:10:20 - INFO - transformers.configuration_utils - Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 2816,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "num_key_value_heads": 16,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}


05/19/2024 09:10:20 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\tokenizer_config.json

05/19/2024 09:10:20 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves\Qwen1.5-0.5B\lora\train_2024-05-18-22-00-35\special_tokens_map.json

05/19/2024 09:10:21 - INFO - transformers.trainer - ***** Running Evaluation *****

05/19/2024 09:10:21 - INFO - transformers.trainer -   Num examples = 2710

05/19/2024 09:10:21 - INFO - transformers.trainer -   Batch size = 1

05/19/2024 09:20:17 - INFO - transformers.modelcard - Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

