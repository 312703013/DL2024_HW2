{
  "best_metric": 0.06363344937562943,
  "best_model_checkpoint": "saves\\Qwen1.5-0.5B\\lora\\train_2024-05-18-22-00-35\\checkpoint-2600",
  "epoch": 3.0,
  "eval_steps": 100,
  "global_step": 4065,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0036900369003690036,
      "grad_norm": 4.94627046585083,
      "learning_rate": 4.9999813349943033e-05,
      "loss": 2.4632,
      "step": 5
    },
    {
      "epoch": 0.007380073800738007,
      "grad_norm": 6.703662395477295,
      "learning_rate": 4.999925340255922e-05,
      "loss": 1.9044,
      "step": 10
    },
    {
      "epoch": 0.01107011070110701,
      "grad_norm": 7.243518829345703,
      "learning_rate": 4.9998320166209664e-05,
      "loss": 1.4153,
      "step": 15
    },
    {
      "epoch": 0.014760147601476014,
      "grad_norm": 6.769618988037109,
      "learning_rate": 4.9997013654829474e-05,
      "loss": 0.8808,
      "step": 20
    },
    {
      "epoch": 0.01845018450184502,
      "grad_norm": 4.192803382873535,
      "learning_rate": 4.999569970062593e-05,
      "loss": 0.4886,
      "step": 25
    },
    {
      "epoch": 0.02214022140221402,
      "grad_norm": 1.9989279508590698,
      "learning_rate": 4.9993721347098225e-05,
      "loss": 0.3175,
      "step": 30
    },
    {
      "epoch": 0.025830258302583026,
      "grad_norm": 1.2140971422195435,
      "learning_rate": 4.999136978720946e-05,
      "loss": 0.2068,
      "step": 35
    },
    {
      "epoch": 0.02952029520295203,
      "grad_norm": 1.4622883796691895,
      "learning_rate": 4.998864505607316e-05,
      "loss": 0.1921,
      "step": 40
    },
    {
      "epoch": 0.033210332103321034,
      "grad_norm": 1.6065845489501953,
      "learning_rate": 4.998554719437501e-05,
      "loss": 0.2351,
      "step": 45
    },
    {
      "epoch": 0.03690036900369004,
      "grad_norm": 1.359602451324463,
      "learning_rate": 4.99820762483723e-05,
      "loss": 0.1871,
      "step": 50
    },
    {
      "epoch": 0.04059040590405904,
      "grad_norm": 1.9380890130996704,
      "learning_rate": 4.99782322698932e-05,
      "loss": 0.2402,
      "step": 55
    },
    {
      "epoch": 0.04428044280442804,
      "grad_norm": 0.759818434715271,
      "learning_rate": 4.997401531633602e-05,
      "loss": 0.188,
      "step": 60
    },
    {
      "epoch": 0.04797047970479705,
      "grad_norm": 0.8228976726531982,
      "learning_rate": 4.9969425450668336e-05,
      "loss": 0.1909,
      "step": 65
    },
    {
      "epoch": 0.05166051660516605,
      "grad_norm": 1.033195972442627,
      "learning_rate": 4.996446274142604e-05,
      "loss": 0.1847,
      "step": 70
    },
    {
      "epoch": 0.055350553505535055,
      "grad_norm": 0.9259718656539917,
      "learning_rate": 4.9959127262712325e-05,
      "loss": 0.2096,
      "step": 75
    },
    {
      "epoch": 0.05904059040590406,
      "grad_norm": 1.0260528326034546,
      "learning_rate": 4.995341909419658e-05,
      "loss": 0.1822,
      "step": 80
    },
    {
      "epoch": 0.06273062730627306,
      "grad_norm": 1.785470724105835,
      "learning_rate": 4.994733832111322e-05,
      "loss": 0.1942,
      "step": 85
    },
    {
      "epoch": 0.06642066420664207,
      "grad_norm": 1.7327080965042114,
      "learning_rate": 4.994088503426036e-05,
      "loss": 0.2034,
      "step": 90
    },
    {
      "epoch": 0.07011070110701106,
      "grad_norm": 1.5648040771484375,
      "learning_rate": 4.9934059329998515e-05,
      "loss": 0.1575,
      "step": 95
    },
    {
      "epoch": 0.07380073800738007,
      "grad_norm": 1.7372171878814697,
      "learning_rate": 4.992686131024913e-05,
      "loss": 0.2282,
      "step": 100
    },
    {
      "epoch": 0.07380073800738007,
      "eval_loss": 0.16989228129386902,
      "eval_runtime": 196.1726,
      "eval_samples_per_second": 13.814,
      "eval_steps_per_second": 13.814,
      "step": 100
    },
    {
      "epoch": 0.07749077490774908,
      "grad_norm": 2.208786964416504,
      "learning_rate": 4.991929108249307e-05,
      "loss": 0.1422,
      "step": 105
    },
    {
      "epoch": 0.08118081180811808,
      "grad_norm": 2.6032023429870605,
      "learning_rate": 4.991134875976901e-05,
      "loss": 0.1651,
      "step": 110
    },
    {
      "epoch": 0.08487084870848709,
      "grad_norm": 2.0802555084228516,
      "learning_rate": 4.990303446067175e-05,
      "loss": 0.137,
      "step": 115
    },
    {
      "epoch": 0.08856088560885608,
      "grad_norm": 1.7503399848937988,
      "learning_rate": 4.9894348309350445e-05,
      "loss": 0.1565,
      "step": 120
    },
    {
      "epoch": 0.09225092250922509,
      "grad_norm": 2.5313222408294678,
      "learning_rate": 4.9885290435506734e-05,
      "loss": 0.1732,
      "step": 125
    },
    {
      "epoch": 0.0959409594095941,
      "grad_norm": 1.4015700817108154,
      "learning_rate": 4.9875860974392854e-05,
      "loss": 0.1258,
      "step": 130
    },
    {
      "epoch": 0.0996309963099631,
      "grad_norm": 2.413283348083496,
      "learning_rate": 4.986606006680953e-05,
      "loss": 0.1264,
      "step": 135
    },
    {
      "epoch": 0.1033210332103321,
      "grad_norm": 2.0730559825897217,
      "learning_rate": 4.985588785910399e-05,
      "loss": 0.1696,
      "step": 140
    },
    {
      "epoch": 0.1070110701107011,
      "grad_norm": 2.890709400177002,
      "learning_rate": 4.984534450316766e-05,
      "loss": 0.1542,
      "step": 145
    },
    {
      "epoch": 0.11070110701107011,
      "grad_norm": 2.9042632579803467,
      "learning_rate": 4.9834430156433996e-05,
      "loss": 0.1512,
      "step": 150
    },
    {
      "epoch": 0.11439114391143912,
      "grad_norm": 1.8177059888839722,
      "learning_rate": 4.982314498187606e-05,
      "loss": 0.1443,
      "step": 155
    },
    {
      "epoch": 0.11808118081180811,
      "grad_norm": 1.7571330070495605,
      "learning_rate": 4.981148914800414e-05,
      "loss": 0.1001,
      "step": 160
    },
    {
      "epoch": 0.12177121771217712,
      "grad_norm": 2.8381965160369873,
      "learning_rate": 4.97994628288632e-05,
      "loss": 0.1128,
      "step": 165
    },
    {
      "epoch": 0.12546125461254612,
      "grad_norm": 1.763360857963562,
      "learning_rate": 4.978706620403029e-05,
      "loss": 0.1208,
      "step": 170
    },
    {
      "epoch": 0.12915129151291513,
      "grad_norm": 2.071685552597046,
      "learning_rate": 4.9774299458611874e-05,
      "loss": 0.087,
      "step": 175
    },
    {
      "epoch": 0.13284132841328414,
      "grad_norm": 3.74094820022583,
      "learning_rate": 4.9761162783241036e-05,
      "loss": 0.1454,
      "step": 180
    },
    {
      "epoch": 0.13653136531365315,
      "grad_norm": 3.2037253379821777,
      "learning_rate": 4.9747656374074684e-05,
      "loss": 0.1773,
      "step": 185
    },
    {
      "epoch": 0.14022140221402213,
      "grad_norm": 3.379774570465088,
      "learning_rate": 4.973378043279059e-05,
      "loss": 0.1373,
      "step": 190
    },
    {
      "epoch": 0.14391143911439114,
      "grad_norm": 1.201870322227478,
      "learning_rate": 4.971953516658435e-05,
      "loss": 0.1657,
      "step": 195
    },
    {
      "epoch": 0.14760147601476015,
      "grad_norm": 2.751113176345825,
      "learning_rate": 4.970492078816637e-05,
      "loss": 0.1073,
      "step": 200
    },
    {
      "epoch": 0.14760147601476015,
      "eval_loss": 0.13233429193496704,
      "eval_runtime": 197.8657,
      "eval_samples_per_second": 13.696,
      "eval_steps_per_second": 13.696,
      "step": 200
    },
    {
      "epoch": 0.15129151291512916,
      "grad_norm": 3.149019241333008,
      "learning_rate": 4.9689937515758586e-05,
      "loss": 0.1065,
      "step": 205
    },
    {
      "epoch": 0.15498154981549817,
      "grad_norm": 1.4684934616088867,
      "learning_rate": 4.9674585573091324e-05,
      "loss": 0.1013,
      "step": 210
    },
    {
      "epoch": 0.15867158671586715,
      "grad_norm": 1.8998422622680664,
      "learning_rate": 4.965886518939983e-05,
      "loss": 0.0997,
      "step": 215
    },
    {
      "epoch": 0.16236162361623616,
      "grad_norm": 2.2578155994415283,
      "learning_rate": 4.964277659942097e-05,
      "loss": 0.1336,
      "step": 220
    },
    {
      "epoch": 0.16605166051660517,
      "grad_norm": 1.6814990043640137,
      "learning_rate": 4.9626320043389626e-05,
      "loss": 0.0751,
      "step": 225
    },
    {
      "epoch": 0.16974169741697417,
      "grad_norm": 3.341754674911499,
      "learning_rate": 4.960949576703518e-05,
      "loss": 0.1385,
      "step": 230
    },
    {
      "epoch": 0.17343173431734318,
      "grad_norm": 1.0394611358642578,
      "learning_rate": 4.9592304021577785e-05,
      "loss": 0.0811,
      "step": 235
    },
    {
      "epoch": 0.17712177121771217,
      "grad_norm": 1.9510372877120972,
      "learning_rate": 4.957474506372468e-05,
      "loss": 0.1049,
      "step": 240
    },
    {
      "epoch": 0.18081180811808117,
      "grad_norm": 2.4936656951904297,
      "learning_rate": 4.95568191556663e-05,
      "loss": 0.1285,
      "step": 245
    },
    {
      "epoch": 0.18450184501845018,
      "grad_norm": 3.481959581375122,
      "learning_rate": 4.9538526565072374e-05,
      "loss": 0.1298,
      "step": 250
    },
    {
      "epoch": 0.1881918819188192,
      "grad_norm": 2.161989212036133,
      "learning_rate": 4.9519867565087964e-05,
      "loss": 0.13,
      "step": 255
    },
    {
      "epoch": 0.1918819188191882,
      "grad_norm": 1.5866082906723022,
      "learning_rate": 4.9500842434329344e-05,
      "loss": 0.0951,
      "step": 260
    },
    {
      "epoch": 0.19557195571955718,
      "grad_norm": 1.9207550287246704,
      "learning_rate": 4.948145145687984e-05,
      "loss": 0.0747,
      "step": 265
    },
    {
      "epoch": 0.1992619926199262,
      "grad_norm": 1.1467530727386475,
      "learning_rate": 4.946169492228561e-05,
      "loss": 0.1084,
      "step": 270
    },
    {
      "epoch": 0.2029520295202952,
      "grad_norm": 0.7493331432342529,
      "learning_rate": 4.944157312555134e-05,
      "loss": 0.0897,
      "step": 275
    },
    {
      "epoch": 0.2066420664206642,
      "grad_norm": 1.6505969762802124,
      "learning_rate": 4.942108636713578e-05,
      "loss": 0.134,
      "step": 280
    },
    {
      "epoch": 0.21033210332103322,
      "grad_norm": 5.439265251159668,
      "learning_rate": 4.94002349529473e-05,
      "loss": 0.1349,
      "step": 285
    },
    {
      "epoch": 0.2140221402214022,
      "grad_norm": 2.382354259490967,
      "learning_rate": 4.937901919433932e-05,
      "loss": 0.1206,
      "step": 290
    },
    {
      "epoch": 0.2177121771217712,
      "grad_norm": 1.187608003616333,
      "learning_rate": 4.935743940810562e-05,
      "loss": 0.1061,
      "step": 295
    },
    {
      "epoch": 0.22140221402214022,
      "grad_norm": 1.0698562860488892,
      "learning_rate": 4.933549591647569e-05,
      "loss": 0.0963,
      "step": 300
    },
    {
      "epoch": 0.22140221402214022,
      "eval_loss": 0.10822387039661407,
      "eval_runtime": 195.4431,
      "eval_samples_per_second": 13.866,
      "eval_steps_per_second": 13.866,
      "step": 300
    },
    {
      "epoch": 0.22509225092250923,
      "grad_norm": 1.5526279211044312,
      "learning_rate": 4.931318904710984e-05,
      "loss": 0.0961,
      "step": 305
    },
    {
      "epoch": 0.22878228782287824,
      "grad_norm": 1.8319251537322998,
      "learning_rate": 4.9290519133094345e-05,
      "loss": 0.0902,
      "step": 310
    },
    {
      "epoch": 0.23247232472324722,
      "grad_norm": 2.2331221103668213,
      "learning_rate": 4.9267486512936465e-05,
      "loss": 0.0687,
      "step": 315
    },
    {
      "epoch": 0.23616236162361623,
      "grad_norm": 0.9618192911148071,
      "learning_rate": 4.9244091530559395e-05,
      "loss": 0.0811,
      "step": 320
    },
    {
      "epoch": 0.23985239852398524,
      "grad_norm": 1.9074680805206299,
      "learning_rate": 4.922033453529711e-05,
      "loss": 0.0765,
      "step": 325
    },
    {
      "epoch": 0.24354243542435425,
      "grad_norm": 3.110597848892212,
      "learning_rate": 4.919621588188916e-05,
      "loss": 0.1417,
      "step": 330
    },
    {
      "epoch": 0.24723247232472326,
      "grad_norm": 2.734280824661255,
      "learning_rate": 4.917173593047541e-05,
      "loss": 0.1079,
      "step": 335
    },
    {
      "epoch": 0.25092250922509224,
      "grad_norm": 2.5761168003082275,
      "learning_rate": 4.9146895046590595e-05,
      "loss": 0.1219,
      "step": 340
    },
    {
      "epoch": 0.25461254612546125,
      "grad_norm": 1.4235038757324219,
      "learning_rate": 4.9121693601158915e-05,
      "loss": 0.1149,
      "step": 345
    },
    {
      "epoch": 0.25830258302583026,
      "grad_norm": 0.5558786988258362,
      "learning_rate": 4.909613197048845e-05,
      "loss": 0.0792,
      "step": 350
    },
    {
      "epoch": 0.26199261992619927,
      "grad_norm": 1.6744509935379028,
      "learning_rate": 4.90702105362656e-05,
      "loss": 0.0805,
      "step": 355
    },
    {
      "epoch": 0.2656826568265683,
      "grad_norm": 2.0841176509857178,
      "learning_rate": 4.904392968554934e-05,
      "loss": 0.0852,
      "step": 360
    },
    {
      "epoch": 0.2693726937269373,
      "grad_norm": 2.3085265159606934,
      "learning_rate": 4.901728981076545e-05,
      "loss": 0.0926,
      "step": 365
    },
    {
      "epoch": 0.2730627306273063,
      "grad_norm": 2.4399254322052,
      "learning_rate": 4.8990291309700656e-05,
      "loss": 0.1002,
      "step": 370
    },
    {
      "epoch": 0.2767527675276753,
      "grad_norm": 0.9124851226806641,
      "learning_rate": 4.89629345854967e-05,
      "loss": 0.0656,
      "step": 375
    },
    {
      "epoch": 0.28044280442804426,
      "grad_norm": 3.227536201477051,
      "learning_rate": 4.893522004664432e-05,
      "loss": 0.0629,
      "step": 380
    },
    {
      "epoch": 0.28413284132841327,
      "grad_norm": 0.9394928216934204,
      "learning_rate": 4.890714810697713e-05,
      "loss": 0.0518,
      "step": 385
    },
    {
      "epoch": 0.2878228782287823,
      "grad_norm": 3.626730442047119,
      "learning_rate": 4.887871918566547e-05,
      "loss": 0.0979,
      "step": 390
    },
    {
      "epoch": 0.2915129151291513,
      "grad_norm": 2.4595489501953125,
      "learning_rate": 4.884993370721011e-05,
      "loss": 0.0827,
      "step": 395
    },
    {
      "epoch": 0.2952029520295203,
      "grad_norm": 2.051029682159424,
      "learning_rate": 4.882079210143595e-05,
      "loss": 0.139,
      "step": 400
    },
    {
      "epoch": 0.2952029520295203,
      "eval_loss": 0.10336843878030777,
      "eval_runtime": 193.8918,
      "eval_samples_per_second": 13.977,
      "eval_steps_per_second": 13.977,
      "step": 400
    },
    {
      "epoch": 0.2988929889298893,
      "grad_norm": 2.0238990783691406,
      "learning_rate": 4.8791294803485586e-05,
      "loss": 0.0715,
      "step": 405
    },
    {
      "epoch": 0.3025830258302583,
      "grad_norm": 0.47095349431037903,
      "learning_rate": 4.87614422538128e-05,
      "loss": 0.0593,
      "step": 410
    },
    {
      "epoch": 0.3062730627306273,
      "grad_norm": 1.2718703746795654,
      "learning_rate": 4.8731234898175995e-05,
      "loss": 0.073,
      "step": 415
    },
    {
      "epoch": 0.30996309963099633,
      "grad_norm": 2.1407699584960938,
      "learning_rate": 4.870067318763155e-05,
      "loss": 0.1437,
      "step": 420
    },
    {
      "epoch": 0.31365313653136534,
      "grad_norm": 3.275007724761963,
      "learning_rate": 4.866975757852706e-05,
      "loss": 0.066,
      "step": 425
    },
    {
      "epoch": 0.3173431734317343,
      "grad_norm": 0.7160467505455017,
      "learning_rate": 4.863848853249456e-05,
      "loss": 0.0841,
      "step": 430
    },
    {
      "epoch": 0.3210332103321033,
      "grad_norm": 1.8515266180038452,
      "learning_rate": 4.860686651644356e-05,
      "loss": 0.1017,
      "step": 435
    },
    {
      "epoch": 0.3247232472324723,
      "grad_norm": 1.5423015356063843,
      "learning_rate": 4.8574892002554177e-05,
      "loss": 0.0934,
      "step": 440
    },
    {
      "epoch": 0.3284132841328413,
      "grad_norm": 1.7803648710250854,
      "learning_rate": 4.8542565468269964e-05,
      "loss": 0.0682,
      "step": 445
    },
    {
      "epoch": 0.33210332103321033,
      "grad_norm": 3.3997933864593506,
      "learning_rate": 4.850988739629091e-05,
      "loss": 0.1045,
      "step": 450
    },
    {
      "epoch": 0.33579335793357934,
      "grad_norm": 3.540281057357788,
      "learning_rate": 4.847685827456612e-05,
      "loss": 0.0815,
      "step": 455
    },
    {
      "epoch": 0.33948339483394835,
      "grad_norm": 1.1141449213027954,
      "learning_rate": 4.844347859628658e-05,
      "loss": 0.0692,
      "step": 460
    },
    {
      "epoch": 0.34317343173431736,
      "grad_norm": 3.82431960105896,
      "learning_rate": 4.840974885987782e-05,
      "loss": 0.0608,
      "step": 465
    },
    {
      "epoch": 0.34686346863468637,
      "grad_norm": 0.666356086730957,
      "learning_rate": 4.8375669568992404e-05,
      "loss": 0.103,
      "step": 470
    },
    {
      "epoch": 0.3505535055350554,
      "grad_norm": 2.995213508605957,
      "learning_rate": 4.834124123250246e-05,
      "loss": 0.0767,
      "step": 475
    },
    {
      "epoch": 0.35424354243542433,
      "grad_norm": 0.8928066492080688,
      "learning_rate": 4.830646436449208e-05,
      "loss": 0.1345,
      "step": 480
    },
    {
      "epoch": 0.35793357933579334,
      "grad_norm": 2.5604970455169678,
      "learning_rate": 4.827133948424959e-05,
      "loss": 0.0739,
      "step": 485
    },
    {
      "epoch": 0.36162361623616235,
      "grad_norm": 2.3464417457580566,
      "learning_rate": 4.823586711625987e-05,
      "loss": 0.0857,
      "step": 490
    },
    {
      "epoch": 0.36531365313653136,
      "grad_norm": 2.006227731704712,
      "learning_rate": 4.82000477901965e-05,
      "loss": 0.0712,
      "step": 495
    },
    {
      "epoch": 0.36900369003690037,
      "grad_norm": 2.0398683547973633,
      "learning_rate": 4.81638820409138e-05,
      "loss": 0.0778,
      "step": 500
    },
    {
      "epoch": 0.36900369003690037,
      "eval_loss": 0.09080471843481064,
      "eval_runtime": 195.6854,
      "eval_samples_per_second": 13.849,
      "eval_steps_per_second": 13.849,
      "step": 500
    },
    {
      "epoch": 0.3726937269372694,
      "grad_norm": 1.1010535955429077,
      "learning_rate": 4.812737040843891e-05,
      "loss": 0.0769,
      "step": 505
    },
    {
      "epoch": 0.3763837638376384,
      "grad_norm": 1.3110332489013672,
      "learning_rate": 4.809051343796368e-05,
      "loss": 0.1075,
      "step": 510
    },
    {
      "epoch": 0.3800738007380074,
      "grad_norm": 2.937333822250366,
      "learning_rate": 4.8053311679836585e-05,
      "loss": 0.1356,
      "step": 515
    },
    {
      "epoch": 0.3837638376383764,
      "grad_norm": 2.1186447143554688,
      "learning_rate": 4.801576568955444e-05,
      "loss": 0.0762,
      "step": 520
    },
    {
      "epoch": 0.3874538745387454,
      "grad_norm": 1.8030424118041992,
      "learning_rate": 4.797787602775412e-05,
      "loss": 0.0703,
      "step": 525
    },
    {
      "epoch": 0.39114391143911437,
      "grad_norm": 2.0122230052948,
      "learning_rate": 4.7939643260204254e-05,
      "loss": 0.0666,
      "step": 530
    },
    {
      "epoch": 0.3948339483394834,
      "grad_norm": 2.4154179096221924,
      "learning_rate": 4.7901067957796685e-05,
      "loss": 0.0744,
      "step": 535
    },
    {
      "epoch": 0.3985239852398524,
      "grad_norm": 1.7408277988433838,
      "learning_rate": 4.786215069653802e-05,
      "loss": 0.1018,
      "step": 540
    },
    {
      "epoch": 0.4022140221402214,
      "grad_norm": 1.6673647165298462,
      "learning_rate": 4.782289205754097e-05,
      "loss": 0.1088,
      "step": 545
    },
    {
      "epoch": 0.4059040590405904,
      "grad_norm": 2.1561391353607178,
      "learning_rate": 4.778329262701571e-05,
      "loss": 0.0946,
      "step": 550
    },
    {
      "epoch": 0.4095940959409594,
      "grad_norm": 2.2807483673095703,
      "learning_rate": 4.774335299626113e-05,
      "loss": 0.0512,
      "step": 555
    },
    {
      "epoch": 0.4132841328413284,
      "grad_norm": 2.9649996757507324,
      "learning_rate": 4.7703073761655957e-05,
      "loss": 0.1034,
      "step": 560
    },
    {
      "epoch": 0.41697416974169743,
      "grad_norm": 3.334836483001709,
      "learning_rate": 4.766245552464993e-05,
      "loss": 0.1168,
      "step": 565
    },
    {
      "epoch": 0.42066420664206644,
      "grad_norm": 1.3222633600234985,
      "learning_rate": 4.7621498891754734e-05,
      "loss": 0.0391,
      "step": 570
    },
    {
      "epoch": 0.42435424354243545,
      "grad_norm": 1.0706510543823242,
      "learning_rate": 4.7580204474534996e-05,
      "loss": 0.0533,
      "step": 575
    },
    {
      "epoch": 0.4280442804428044,
      "grad_norm": 4.413570880889893,
      "learning_rate": 4.7538572889599154e-05,
      "loss": 0.092,
      "step": 580
    },
    {
      "epoch": 0.4317343173431734,
      "grad_norm": 1.0979690551757812,
      "learning_rate": 4.749660475859021e-05,
      "loss": 0.0797,
      "step": 585
    },
    {
      "epoch": 0.4354243542435424,
      "grad_norm": 2.1730899810791016,
      "learning_rate": 4.74543007081765e-05,
      "loss": 0.0658,
      "step": 590
    },
    {
      "epoch": 0.43911439114391143,
      "grad_norm": 1.7181613445281982,
      "learning_rate": 4.74116613700423e-05,
      "loss": 0.0763,
      "step": 595
    },
    {
      "epoch": 0.44280442804428044,
      "grad_norm": 1.7883188724517822,
      "learning_rate": 4.7368687380878384e-05,
      "loss": 0.0476,
      "step": 600
    },
    {
      "epoch": 0.44280442804428044,
      "eval_loss": 0.08724594116210938,
      "eval_runtime": 353.9333,
      "eval_samples_per_second": 7.657,
      "eval_steps_per_second": 7.657,
      "step": 600
    },
    {
      "epoch": 0.44649446494464945,
      "grad_norm": 2.361884355545044,
      "learning_rate": 4.732537938237256e-05,
      "loss": 0.0848,
      "step": 605
    },
    {
      "epoch": 0.45018450184501846,
      "grad_norm": 2.9406251907348633,
      "learning_rate": 4.728173802120007e-05,
      "loss": 0.0878,
      "step": 610
    },
    {
      "epoch": 0.45387453874538747,
      "grad_norm": 1.4948304891586304,
      "learning_rate": 4.7237763949013914e-05,
      "loss": 0.0932,
      "step": 615
    },
    {
      "epoch": 0.4575645756457565,
      "grad_norm": 1.8841195106506348,
      "learning_rate": 4.7193457822435135e-05,
      "loss": 0.1041,
      "step": 620
    },
    {
      "epoch": 0.4612546125461255,
      "grad_norm": 2.414829730987549,
      "learning_rate": 4.714882030304301e-05,
      "loss": 0.079,
      "step": 625
    },
    {
      "epoch": 0.46494464944649444,
      "grad_norm": 3.595311403274536,
      "learning_rate": 4.7103852057365195e-05,
      "loss": 0.0559,
      "step": 630
    },
    {
      "epoch": 0.46863468634686345,
      "grad_norm": 1.3022663593292236,
      "learning_rate": 4.705855375686773e-05,
      "loss": 0.0824,
      "step": 635
    },
    {
      "epoch": 0.47232472324723246,
      "grad_norm": 1.056434154510498,
      "learning_rate": 4.7012926077945045e-05,
      "loss": 0.0603,
      "step": 640
    },
    {
      "epoch": 0.47601476014760147,
      "grad_norm": 4.614291667938232,
      "learning_rate": 4.696696970190986e-05,
      "loss": 0.1087,
      "step": 645
    },
    {
      "epoch": 0.4797047970479705,
      "grad_norm": 0.557102382183075,
      "learning_rate": 4.692068531498298e-05,
      "loss": 0.0993,
      "step": 650
    },
    {
      "epoch": 0.4833948339483395,
      "grad_norm": 1.0002151727676392,
      "learning_rate": 4.687407360828308e-05,
      "loss": 0.1266,
      "step": 655
    },
    {
      "epoch": 0.4870848708487085,
      "grad_norm": 1.5529142618179321,
      "learning_rate": 4.682713527781639e-05,
      "loss": 0.0786,
      "step": 660
    },
    {
      "epoch": 0.4907749077490775,
      "grad_norm": 1.7926300764083862,
      "learning_rate": 4.6779871024466265e-05,
      "loss": 0.0844,
      "step": 665
    },
    {
      "epoch": 0.4944649446494465,
      "grad_norm": 1.423504114151001,
      "learning_rate": 4.673228155398275e-05,
      "loss": 0.0925,
      "step": 670
    },
    {
      "epoch": 0.4981549815498155,
      "grad_norm": 1.5487571954727173,
      "learning_rate": 4.668436757697204e-05,
      "loss": 0.0648,
      "step": 675
    },
    {
      "epoch": 0.5018450184501845,
      "grad_norm": 1.3938411474227905,
      "learning_rate": 4.663612980888586e-05,
      "loss": 0.074,
      "step": 680
    },
    {
      "epoch": 0.5055350553505535,
      "grad_norm": 2.2797648906707764,
      "learning_rate": 4.6587568970010774e-05,
      "loss": 0.0729,
      "step": 685
    },
    {
      "epoch": 0.5092250922509225,
      "grad_norm": 1.7761476039886475,
      "learning_rate": 4.653868578545746e-05,
      "loss": 0.0886,
      "step": 690
    },
    {
      "epoch": 0.5129151291512916,
      "grad_norm": 2.381948947906494,
      "learning_rate": 4.6489480985149836e-05,
      "loss": 0.0842,
      "step": 695
    },
    {
      "epoch": 0.5166051660516605,
      "grad_norm": 1.8850492238998413,
      "learning_rate": 4.6439955303814224e-05,
      "loss": 0.1266,
      "step": 700
    },
    {
      "epoch": 0.5166051660516605,
      "eval_loss": 0.08133482933044434,
      "eval_runtime": 233.2809,
      "eval_samples_per_second": 11.617,
      "eval_steps_per_second": 11.617,
      "step": 700
    },
    {
      "epoch": 0.5202952029520295,
      "grad_norm": 1.2690844535827637,
      "learning_rate": 4.639010948096831e-05,
      "loss": 0.1021,
      "step": 705
    },
    {
      "epoch": 0.5239852398523985,
      "grad_norm": 0.8629671335220337,
      "learning_rate": 4.633994426091016e-05,
      "loss": 0.1021,
      "step": 710
    },
    {
      "epoch": 0.5276752767527675,
      "grad_norm": 2.12872576713562,
      "learning_rate": 4.6289460392707055e-05,
      "loss": 0.0575,
      "step": 715
    },
    {
      "epoch": 0.5313653136531366,
      "grad_norm": 2.8971612453460693,
      "learning_rate": 4.623865863018435e-05,
      "loss": 0.0933,
      "step": 720
    },
    {
      "epoch": 0.5350553505535055,
      "grad_norm": 1.3011207580566406,
      "learning_rate": 4.6187539731914195e-05,
      "loss": 0.0494,
      "step": 725
    },
    {
      "epoch": 0.5387453874538746,
      "grad_norm": 0.8905510306358337,
      "learning_rate": 4.613610446120422e-05,
      "loss": 0.0366,
      "step": 730
    },
    {
      "epoch": 0.5424354243542435,
      "grad_norm": 2.8017430305480957,
      "learning_rate": 4.608435358608612e-05,
      "loss": 0.0825,
      "step": 735
    },
    {
      "epoch": 0.5461254612546126,
      "grad_norm": 2.0186073780059814,
      "learning_rate": 4.603228787930418e-05,
      "loss": 0.0803,
      "step": 740
    },
    {
      "epoch": 0.5498154981549815,
      "grad_norm": 1.856310486793518,
      "learning_rate": 4.597990811830378e-05,
      "loss": 0.0638,
      "step": 745
    },
    {
      "epoch": 0.5535055350553506,
      "grad_norm": 5.698829650878906,
      "learning_rate": 4.5927215085219765e-05,
      "loss": 0.0742,
      "step": 750
    },
    {
      "epoch": 0.5571955719557196,
      "grad_norm": 1.4929744005203247,
      "learning_rate": 4.5874209566864725e-05,
      "loss": 0.0638,
      "step": 755
    },
    {
      "epoch": 0.5608856088560885,
      "grad_norm": 3.5216805934906006,
      "learning_rate": 4.582089235471731e-05,
      "loss": 0.0755,
      "step": 760
    },
    {
      "epoch": 0.5645756457564576,
      "grad_norm": 2.8101253509521484,
      "learning_rate": 4.576726424491038e-05,
      "loss": 0.075,
      "step": 765
    },
    {
      "epoch": 0.5682656826568265,
      "grad_norm": 0.8701581358909607,
      "learning_rate": 4.571332603821911e-05,
      "loss": 0.0518,
      "step": 770
    },
    {
      "epoch": 0.5719557195571956,
      "grad_norm": 1.8367493152618408,
      "learning_rate": 4.565907854004904e-05,
      "loss": 0.0785,
      "step": 775
    },
    {
      "epoch": 0.5756457564575646,
      "grad_norm": 0.5594192743301392,
      "learning_rate": 4.5604522560424076e-05,
      "loss": 0.1104,
      "step": 780
    },
    {
      "epoch": 0.5793357933579336,
      "grad_norm": 1.6818407773971558,
      "learning_rate": 4.554965891397435e-05,
      "loss": 0.0626,
      "step": 785
    },
    {
      "epoch": 0.5830258302583026,
      "grad_norm": 1.4909456968307495,
      "learning_rate": 4.549448841992406e-05,
      "loss": 0.0501,
      "step": 790
    },
    {
      "epoch": 0.5867158671586716,
      "grad_norm": 2.0474958419799805,
      "learning_rate": 4.54390119020793e-05,
      "loss": 0.0757,
      "step": 795
    },
    {
      "epoch": 0.5904059040590406,
      "grad_norm": 1.8924707174301147,
      "learning_rate": 4.5383230188815684e-05,
      "loss": 0.0819,
      "step": 800
    },
    {
      "epoch": 0.5904059040590406,
      "eval_loss": 0.08069638162851334,
      "eval_runtime": 375.8737,
      "eval_samples_per_second": 7.21,
      "eval_steps_per_second": 7.21,
      "step": 800
    },
    {
      "epoch": 0.5940959409594095,
      "grad_norm": 2.1351335048675537,
      "learning_rate": 4.5327144113066e-05,
      "loss": 0.0957,
      "step": 805
    },
    {
      "epoch": 0.5977859778597786,
      "grad_norm": 2.7482683658599854,
      "learning_rate": 4.527075451230779e-05,
      "loss": 0.0463,
      "step": 810
    },
    {
      "epoch": 0.6014760147601476,
      "grad_norm": 0.8193476796150208,
      "learning_rate": 4.521406222855082e-05,
      "loss": 0.0821,
      "step": 815
    },
    {
      "epoch": 0.6051660516605166,
      "grad_norm": 0.29834482073783875,
      "learning_rate": 4.515706810832455e-05,
      "loss": 0.0255,
      "step": 820
    },
    {
      "epoch": 0.6088560885608856,
      "grad_norm": 3.4459614753723145,
      "learning_rate": 4.5099773002665434e-05,
      "loss": 0.0979,
      "step": 825
    },
    {
      "epoch": 0.6125461254612546,
      "grad_norm": 0.37220320105552673,
      "learning_rate": 4.5042177767104246e-05,
      "loss": 0.122,
      "step": 830
    },
    {
      "epoch": 0.6162361623616236,
      "grad_norm": 1.5704607963562012,
      "learning_rate": 4.4984283261653305e-05,
      "loss": 0.0765,
      "step": 835
    },
    {
      "epoch": 0.6199261992619927,
      "grad_norm": 0.9693787693977356,
      "learning_rate": 4.4926090350793636e-05,
      "loss": 0.0619,
      "step": 840
    },
    {
      "epoch": 0.6236162361623616,
      "grad_norm": 3.6951680183410645,
      "learning_rate": 4.4867599903462046e-05,
      "loss": 0.092,
      "step": 845
    },
    {
      "epoch": 0.6273062730627307,
      "grad_norm": 1.785950779914856,
      "learning_rate": 4.4808812793038164e-05,
      "loss": 0.0864,
      "step": 850
    },
    {
      "epoch": 0.6309963099630996,
      "grad_norm": 1.6279369592666626,
      "learning_rate": 4.4749729897331385e-05,
      "loss": 0.0646,
      "step": 855
    },
    {
      "epoch": 0.6346863468634686,
      "grad_norm": 2.0092387199401855,
      "learning_rate": 4.469035209856779e-05,
      "loss": 0.0896,
      "step": 860
    },
    {
      "epoch": 0.6383763837638377,
      "grad_norm": 0.8709149956703186,
      "learning_rate": 4.463068028337691e-05,
      "loss": 0.0708,
      "step": 865
    },
    {
      "epoch": 0.6420664206642066,
      "grad_norm": 4.057400703430176,
      "learning_rate": 4.4570715342778605e-05,
      "loss": 0.0659,
      "step": 870
    },
    {
      "epoch": 0.6457564575645757,
      "grad_norm": 0.39518192410469055,
      "learning_rate": 4.451045817216961e-05,
      "loss": 0.0516,
      "step": 875
    },
    {
      "epoch": 0.6494464944649446,
      "grad_norm": 0.9392526149749756,
      "learning_rate": 4.4449909671310285e-05,
      "loss": 0.0942,
      "step": 880
    },
    {
      "epoch": 0.6531365313653137,
      "grad_norm": 2.4408040046691895,
      "learning_rate": 4.438907074431111e-05,
      "loss": 0.1266,
      "step": 885
    },
    {
      "epoch": 0.6568265682656826,
      "grad_norm": 3.163364887237549,
      "learning_rate": 4.432794229961923e-05,
      "loss": 0.1257,
      "step": 890
    },
    {
      "epoch": 0.6605166051660517,
      "grad_norm": 2.5367069244384766,
      "learning_rate": 4.426652525000485e-05,
      "loss": 0.0702,
      "step": 895
    },
    {
      "epoch": 0.6642066420664207,
      "grad_norm": 1.2334405183792114,
      "learning_rate": 4.420482051254764e-05,
      "loss": 0.1152,
      "step": 900
    },
    {
      "epoch": 0.6642066420664207,
      "eval_loss": 0.07467745989561081,
      "eval_runtime": 521.5752,
      "eval_samples_per_second": 5.196,
      "eval_steps_per_second": 5.196,
      "step": 900
    },
    {
      "epoch": 0.6678966789667896,
      "grad_norm": 0.7528893351554871,
      "learning_rate": 4.414282900862303e-05,
      "loss": 0.0461,
      "step": 905
    },
    {
      "epoch": 0.6715867158671587,
      "grad_norm": 3.3271546363830566,
      "learning_rate": 4.4080551663888424e-05,
      "loss": 0.0812,
      "step": 910
    },
    {
      "epoch": 0.6752767527675276,
      "grad_norm": 1.3228857517242432,
      "learning_rate": 4.401798940826943e-05,
      "loss": 0.0589,
      "step": 915
    },
    {
      "epoch": 0.6789667896678967,
      "grad_norm": 2.321857452392578,
      "learning_rate": 4.395514317594592e-05,
      "loss": 0.0399,
      "step": 920
    },
    {
      "epoch": 0.6826568265682657,
      "grad_norm": 1.5343118906021118,
      "learning_rate": 4.3892013905338136e-05,
      "loss": 0.0561,
      "step": 925
    },
    {
      "epoch": 0.6863468634686347,
      "grad_norm": 0.6305597424507141,
      "learning_rate": 4.382860253909262e-05,
      "loss": 0.0693,
      "step": 930
    },
    {
      "epoch": 0.6900369003690037,
      "grad_norm": 2.008657455444336,
      "learning_rate": 4.3764910024068196e-05,
      "loss": 0.1032,
      "step": 935
    },
    {
      "epoch": 0.6937269372693727,
      "grad_norm": 2.0301947593688965,
      "learning_rate": 4.370093731132178e-05,
      "loss": 0.1278,
      "step": 940
    },
    {
      "epoch": 0.6974169741697417,
      "grad_norm": 1.9413042068481445,
      "learning_rate": 4.363668535609421e-05,
      "loss": 0.0941,
      "step": 945
    },
    {
      "epoch": 0.7011070110701108,
      "grad_norm": 2.1937735080718994,
      "learning_rate": 4.357215511779598e-05,
      "loss": 0.0806,
      "step": 950
    },
    {
      "epoch": 0.7047970479704797,
      "grad_norm": 2.049740791320801,
      "learning_rate": 4.350734755999289e-05,
      "loss": 0.1019,
      "step": 955
    },
    {
      "epoch": 0.7084870848708487,
      "grad_norm": 0.9807661175727844,
      "learning_rate": 4.3442263650391715e-05,
      "loss": 0.1,
      "step": 960
    },
    {
      "epoch": 0.7121771217712177,
      "grad_norm": 1.0169445276260376,
      "learning_rate": 4.337690436082565e-05,
      "loss": 0.0746,
      "step": 965
    },
    {
      "epoch": 0.7158671586715867,
      "grad_norm": 0.4690466821193695,
      "learning_rate": 4.331127066723994e-05,
      "loss": 0.0411,
      "step": 970
    },
    {
      "epoch": 0.7195571955719557,
      "grad_norm": 2.1571977138519287,
      "learning_rate": 4.324536354967719e-05,
      "loss": 0.0721,
      "step": 975
    },
    {
      "epoch": 0.7232472324723247,
      "grad_norm": 2.3142616748809814,
      "learning_rate": 4.317918399226276e-05,
      "loss": 0.1192,
      "step": 980
    },
    {
      "epoch": 0.7269372693726938,
      "grad_norm": 4.349612236022949,
      "learning_rate": 4.311273298319012e-05,
      "loss": 0.1015,
      "step": 985
    },
    {
      "epoch": 0.7306273062730627,
      "grad_norm": 2.219831943511963,
      "learning_rate": 4.304601151470604e-05,
      "loss": 0.0398,
      "step": 990
    },
    {
      "epoch": 0.7343173431734318,
      "grad_norm": 2.058281183242798,
      "learning_rate": 4.2979020583095784e-05,
      "loss": 0.1069,
      "step": 995
    },
    {
      "epoch": 0.7380073800738007,
      "grad_norm": 0.8697874546051025,
      "learning_rate": 4.291176118866825e-05,
      "loss": 0.0577,
      "step": 1000
    },
    {
      "epoch": 0.7380073800738007,
      "eval_loss": 0.07473781704902649,
      "eval_runtime": 525.9654,
      "eval_samples_per_second": 5.152,
      "eval_steps_per_second": 5.152,
      "step": 1000
    },
    {
      "epoch": 0.7416974169741697,
      "grad_norm": 0.6474995017051697,
      "learning_rate": 4.284423433574102e-05,
      "loss": 0.063,
      "step": 1005
    },
    {
      "epoch": 0.7453874538745388,
      "grad_norm": 0.9229964017868042,
      "learning_rate": 4.277644103262538e-05,
      "loss": 0.0712,
      "step": 1010
    },
    {
      "epoch": 0.7490774907749077,
      "grad_norm": 0.6055892705917358,
      "learning_rate": 4.2708382291611235e-05,
      "loss": 0.0314,
      "step": 1015
    },
    {
      "epoch": 0.7527675276752768,
      "grad_norm": 3.145562171936035,
      "learning_rate": 4.264005912895202e-05,
      "loss": 0.1062,
      "step": 1020
    },
    {
      "epoch": 0.7564575645756457,
      "grad_norm": 1.7842416763305664,
      "learning_rate": 4.25714725648495e-05,
      "loss": 0.0603,
      "step": 1025
    },
    {
      "epoch": 0.7601476014760148,
      "grad_norm": 1.442065715789795,
      "learning_rate": 4.2502623623438575e-05,
      "loss": 0.0576,
      "step": 1030
    },
    {
      "epoch": 0.7638376383763837,
      "grad_norm": 2.173708200454712,
      "learning_rate": 4.243351333277194e-05,
      "loss": 0.0811,
      "step": 1035
    },
    {
      "epoch": 0.7675276752767528,
      "grad_norm": 0.7095981240272522,
      "learning_rate": 4.2364142724804776e-05,
      "loss": 0.0494,
      "step": 1040
    },
    {
      "epoch": 0.7712177121771218,
      "grad_norm": 4.40350341796875,
      "learning_rate": 4.2294512835379325e-05,
      "loss": 0.0717,
      "step": 1045
    },
    {
      "epoch": 0.7749077490774908,
      "grad_norm": 1.3125948905944824,
      "learning_rate": 4.2224624704209404e-05,
      "loss": 0.0877,
      "step": 1050
    },
    {
      "epoch": 0.7785977859778598,
      "grad_norm": 1.5785281658172607,
      "learning_rate": 4.21544793748649e-05,
      "loss": 0.0498,
      "step": 1055
    },
    {
      "epoch": 0.7822878228782287,
      "grad_norm": 4.0140700340271,
      "learning_rate": 4.20840778947562e-05,
      "loss": 0.1005,
      "step": 1060
    },
    {
      "epoch": 0.7859778597785978,
      "grad_norm": 1.7981646060943604,
      "learning_rate": 4.2013421315118534e-05,
      "loss": 0.0481,
      "step": 1065
    },
    {
      "epoch": 0.7896678966789668,
      "grad_norm": 1.5535390377044678,
      "learning_rate": 4.194251069099626e-05,
      "loss": 0.0536,
      "step": 1070
    },
    {
      "epoch": 0.7933579335793358,
      "grad_norm": 0.2741527259349823,
      "learning_rate": 4.187134708122714e-05,
      "loss": 0.0417,
      "step": 1075
    },
    {
      "epoch": 0.7970479704797048,
      "grad_norm": 1.8359622955322266,
      "learning_rate": 4.1799931548426516e-05,
      "loss": 0.0943,
      "step": 1080
    },
    {
      "epoch": 0.8007380073800738,
      "grad_norm": 1.0914995670318604,
      "learning_rate": 4.172826515897146e-05,
      "loss": 0.0511,
      "step": 1085
    },
    {
      "epoch": 0.8044280442804428,
      "grad_norm": 4.069861888885498,
      "learning_rate": 4.165634898298482e-05,
      "loss": 0.1094,
      "step": 1090
    },
    {
      "epoch": 0.8081180811808119,
      "grad_norm": 0.39945900440216064,
      "learning_rate": 4.158418409431926e-05,
      "loss": 0.0526,
      "step": 1095
    },
    {
      "epoch": 0.8118081180811808,
      "grad_norm": 3.9353511333465576,
      "learning_rate": 4.151177157054123e-05,
      "loss": 0.0614,
      "step": 1100
    },
    {
      "epoch": 0.8118081180811808,
      "eval_loss": 0.07323210686445236,
      "eval_runtime": 558.6512,
      "eval_samples_per_second": 4.851,
      "eval_steps_per_second": 4.851,
      "step": 1100
    },
    {
      "epoch": 0.8154981549815498,
      "grad_norm": 1.7402257919311523,
      "learning_rate": 4.143911249291487e-05,
      "loss": 0.0282,
      "step": 1105
    },
    {
      "epoch": 0.8191881918819188,
      "grad_norm": 1.3636904954910278,
      "learning_rate": 4.136620794638585e-05,
      "loss": 0.0766,
      "step": 1110
    },
    {
      "epoch": 0.8228782287822878,
      "grad_norm": 2.745384454727173,
      "learning_rate": 4.1293059019565195e-05,
      "loss": 0.0964,
      "step": 1115
    },
    {
      "epoch": 0.8265682656826568,
      "grad_norm": 1.7776963710784912,
      "learning_rate": 4.121966680471302e-05,
      "loss": 0.0514,
      "step": 1120
    },
    {
      "epoch": 0.8302583025830258,
      "grad_norm": 2.0300512313842773,
      "learning_rate": 4.11460323977222e-05,
      "loss": 0.1066,
      "step": 1125
    },
    {
      "epoch": 0.8339483394833949,
      "grad_norm": 0.30840110778808594,
      "learning_rate": 4.107215689810204e-05,
      "loss": 0.0611,
      "step": 1130
    },
    {
      "epoch": 0.8376383763837638,
      "grad_norm": 0.5848278999328613,
      "learning_rate": 4.099804140896184e-05,
      "loss": 0.0347,
      "step": 1135
    },
    {
      "epoch": 0.8413284132841329,
      "grad_norm": 2.9324073791503906,
      "learning_rate": 4.0923687036994415e-05,
      "loss": 0.0878,
      "step": 1140
    },
    {
      "epoch": 0.8450184501845018,
      "grad_norm": 1.3620103597640991,
      "learning_rate": 4.0849094892459594e-05,
      "loss": 0.0419,
      "step": 1145
    },
    {
      "epoch": 0.8487084870848709,
      "grad_norm": 2.8939876556396484,
      "learning_rate": 4.077426608916761e-05,
      "loss": 0.0505,
      "step": 1150
    },
    {
      "epoch": 0.8523985239852399,
      "grad_norm": 1.2769368886947632,
      "learning_rate": 4.069920174446251e-05,
      "loss": 0.0493,
      "step": 1155
    },
    {
      "epoch": 0.8560885608856088,
      "grad_norm": 2.794187307357788,
      "learning_rate": 4.0623902979205406e-05,
      "loss": 0.1015,
      "step": 1160
    },
    {
      "epoch": 0.8597785977859779,
      "grad_norm": 0.5148488879203796,
      "learning_rate": 4.054837091775783e-05,
      "loss": 0.0829,
      "step": 1165
    },
    {
      "epoch": 0.8634686346863468,
      "grad_norm": 1.1908856630325317,
      "learning_rate": 4.047260668796486e-05,
      "loss": 0.064,
      "step": 1170
    },
    {
      "epoch": 0.8671586715867159,
      "grad_norm": 2.076178550720215,
      "learning_rate": 4.039661142113831e-05,
      "loss": 0.0951,
      "step": 1175
    },
    {
      "epoch": 0.8708487084870848,
      "grad_norm": 2.1391959190368652,
      "learning_rate": 4.032038625203987e-05,
      "loss": 0.0938,
      "step": 1180
    },
    {
      "epoch": 0.8745387453874539,
      "grad_norm": 0.9089421629905701,
      "learning_rate": 4.0243932318864086e-05,
      "loss": 0.0501,
      "step": 1185
    },
    {
      "epoch": 0.8782287822878229,
      "grad_norm": 0.9371378421783447,
      "learning_rate": 4.016725076322146e-05,
      "loss": 0.0512,
      "step": 1190
    },
    {
      "epoch": 0.8819188191881919,
      "grad_norm": 2.32944393157959,
      "learning_rate": 4.009034273012131e-05,
      "loss": 0.1374,
      "step": 1195
    },
    {
      "epoch": 0.8856088560885609,
      "grad_norm": 1.3917385339736938,
      "learning_rate": 4.001320936795476e-05,
      "loss": 0.048,
      "step": 1200
    },
    {
      "epoch": 0.8856088560885609,
      "eval_loss": 0.07087213546037674,
      "eval_runtime": 592.9246,
      "eval_samples_per_second": 4.571,
      "eval_steps_per_second": 4.571,
      "step": 1200
    },
    {
      "epoch": 0.8892988929889298,
      "grad_norm": 1.590252161026001,
      "learning_rate": 3.993585182847751e-05,
      "loss": 0.0478,
      "step": 1205
    },
    {
      "epoch": 0.8929889298892989,
      "grad_norm": 1.5079383850097656,
      "learning_rate": 3.985827126679269e-05,
      "loss": 0.08,
      "step": 1210
    },
    {
      "epoch": 0.8966789667896679,
      "grad_norm": 1.198887586593628,
      "learning_rate": 3.978046884133361e-05,
      "loss": 0.0979,
      "step": 1215
    },
    {
      "epoch": 0.9003690036900369,
      "grad_norm": 1.0894006490707397,
      "learning_rate": 3.970244571384644e-05,
      "loss": 0.0663,
      "step": 1220
    },
    {
      "epoch": 0.9040590405904059,
      "grad_norm": 1.9337049722671509,
      "learning_rate": 3.9624203049372874e-05,
      "loss": 0.0591,
      "step": 1225
    },
    {
      "epoch": 0.9077490774907749,
      "grad_norm": 1.148148536682129,
      "learning_rate": 3.954574201623273e-05,
      "loss": 0.0849,
      "step": 1230
    },
    {
      "epoch": 0.9114391143911439,
      "grad_norm": 1.5554649829864502,
      "learning_rate": 3.946706378600651e-05,
      "loss": 0.0571,
      "step": 1235
    },
    {
      "epoch": 0.915129151291513,
      "grad_norm": 2.3239572048187256,
      "learning_rate": 3.9388169533517924e-05,
      "loss": 0.0408,
      "step": 1240
    },
    {
      "epoch": 0.9188191881918819,
      "grad_norm": 3.395260810852051,
      "learning_rate": 3.9309060436816283e-05,
      "loss": 0.0507,
      "step": 1245
    },
    {
      "epoch": 0.922509225092251,
      "grad_norm": 0.7893214821815491,
      "learning_rate": 3.922973767715901e-05,
      "loss": 0.0972,
      "step": 1250
    },
    {
      "epoch": 0.9261992619926199,
      "grad_norm": 1.8841376304626465,
      "learning_rate": 3.915020243899388e-05,
      "loss": 0.0417,
      "step": 1255
    },
    {
      "epoch": 0.9298892988929889,
      "grad_norm": 1.1459367275238037,
      "learning_rate": 3.907045590994146e-05,
      "loss": 0.0349,
      "step": 1260
    },
    {
      "epoch": 0.933579335793358,
      "grad_norm": 2.2586941719055176,
      "learning_rate": 3.899049928077727e-05,
      "loss": 0.0505,
      "step": 1265
    },
    {
      "epoch": 0.9372693726937269,
      "grad_norm": 1.1792553663253784,
      "learning_rate": 3.8910333745414075e-05,
      "loss": 0.076,
      "step": 1270
    },
    {
      "epoch": 0.940959409594096,
      "grad_norm": 1.3749823570251465,
      "learning_rate": 3.8829960500884e-05,
      "loss": 0.0491,
      "step": 1275
    },
    {
      "epoch": 0.9446494464944649,
      "grad_norm": 3.813992500305176,
      "learning_rate": 3.8749380747320694e-05,
      "loss": 0.1038,
      "step": 1280
    },
    {
      "epoch": 0.948339483394834,
      "grad_norm": 0.11266804486513138,
      "learning_rate": 3.866859568794142e-05,
      "loss": 0.0557,
      "step": 1285
    },
    {
      "epoch": 0.9520295202952029,
      "grad_norm": 1.4554435014724731,
      "learning_rate": 3.858760652902905e-05,
      "loss": 0.0776,
      "step": 1290
    },
    {
      "epoch": 0.955719557195572,
      "grad_norm": 1.3622822761535645,
      "learning_rate": 3.850641447991407e-05,
      "loss": 0.0749,
      "step": 1295
    },
    {
      "epoch": 0.959409594095941,
      "grad_norm": 1.9956707954406738,
      "learning_rate": 3.842502075295653e-05,
      "loss": 0.0435,
      "step": 1300
    },
    {
      "epoch": 0.959409594095941,
      "eval_loss": 0.06832287460565567,
      "eval_runtime": 549.9457,
      "eval_samples_per_second": 4.928,
      "eval_steps_per_second": 4.928,
      "step": 1300
    },
    {
      "epoch": 0.9630996309963099,
      "grad_norm": 1.4983346462249756,
      "learning_rate": 3.834342656352792e-05,
      "loss": 0.0681,
      "step": 1305
    },
    {
      "epoch": 0.966789667896679,
      "grad_norm": 0.33271855115890503,
      "learning_rate": 3.8261633129993066e-05,
      "loss": 0.0515,
      "step": 1310
    },
    {
      "epoch": 0.9704797047970479,
      "grad_norm": 2.0531558990478516,
      "learning_rate": 3.817964167369188e-05,
      "loss": 0.0475,
      "step": 1315
    },
    {
      "epoch": 0.974169741697417,
      "grad_norm": 4.243657112121582,
      "learning_rate": 3.809745341892116e-05,
      "loss": 0.117,
      "step": 1320
    },
    {
      "epoch": 0.977859778597786,
      "grad_norm": 4.783182144165039,
      "learning_rate": 3.801506959291631e-05,
      "loss": 0.0585,
      "step": 1325
    },
    {
      "epoch": 0.981549815498155,
      "grad_norm": 3.7978274822235107,
      "learning_rate": 3.793249142583299e-05,
      "loss": 0.1038,
      "step": 1330
    },
    {
      "epoch": 0.985239852398524,
      "grad_norm": 2.879340171813965,
      "learning_rate": 3.784972015072876e-05,
      "loss": 0.0505,
      "step": 1335
    },
    {
      "epoch": 0.988929889298893,
      "grad_norm": 1.2599915266036987,
      "learning_rate": 3.776675700354469e-05,
      "loss": 0.0783,
      "step": 1340
    },
    {
      "epoch": 0.992619926199262,
      "grad_norm": 1.0569040775299072,
      "learning_rate": 3.768360322308686e-05,
      "loss": 0.0557,
      "step": 1345
    },
    {
      "epoch": 0.996309963099631,
      "grad_norm": 2.9684317111968994,
      "learning_rate": 3.760026005100792e-05,
      "loss": 0.1253,
      "step": 1350
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9226775169372559,
      "learning_rate": 3.7516728731788466e-05,
      "loss": 0.0929,
      "step": 1355
    },
    {
      "epoch": 1.003690036900369,
      "grad_norm": 1.23222815990448,
      "learning_rate": 3.743301051271855e-05,
      "loss": 0.0576,
      "step": 1360
    },
    {
      "epoch": 1.007380073800738,
      "grad_norm": 0.9135705828666687,
      "learning_rate": 3.734910664387901e-05,
      "loss": 0.0627,
      "step": 1365
    },
    {
      "epoch": 1.011070110701107,
      "grad_norm": 0.31171417236328125,
      "learning_rate": 3.7265018378122784e-05,
      "loss": 0.0518,
      "step": 1370
    },
    {
      "epoch": 1.014760147601476,
      "grad_norm": 1.1268291473388672,
      "learning_rate": 3.7180746971056244e-05,
      "loss": 0.095,
      "step": 1375
    },
    {
      "epoch": 1.018450184501845,
      "grad_norm": 1.3367884159088135,
      "learning_rate": 3.7096293681020426e-05,
      "loss": 0.0832,
      "step": 1380
    },
    {
      "epoch": 1.022140221402214,
      "grad_norm": 1.2853268384933472,
      "learning_rate": 3.701165976907224e-05,
      "loss": 0.0702,
      "step": 1385
    },
    {
      "epoch": 1.0258302583025831,
      "grad_norm": 1.8284287452697754,
      "learning_rate": 3.692684649896565e-05,
      "loss": 0.0771,
      "step": 1390
    },
    {
      "epoch": 1.029520295202952,
      "grad_norm": 0.7600541710853577,
      "learning_rate": 3.6841855137132785e-05,
      "loss": 0.0427,
      "step": 1395
    },
    {
      "epoch": 1.033210332103321,
      "grad_norm": 2.20447039604187,
      "learning_rate": 3.6756686952665045e-05,
      "loss": 0.0595,
      "step": 1400
    },
    {
      "epoch": 1.033210332103321,
      "eval_loss": 0.06928567588329315,
      "eval_runtime": 544.5213,
      "eval_samples_per_second": 4.977,
      "eval_steps_per_second": 4.977,
      "step": 1400
    },
    {
      "epoch": 1.03690036900369,
      "grad_norm": 1.1380562782287598,
      "learning_rate": 3.667134321729416e-05,
      "loss": 0.0773,
      "step": 1405
    },
    {
      "epoch": 1.040590405904059,
      "grad_norm": 1.6360478401184082,
      "learning_rate": 3.6585825205373167e-05,
      "loss": 0.0725,
      "step": 1410
    },
    {
      "epoch": 1.044280442804428,
      "grad_norm": 1.054566502571106,
      "learning_rate": 3.650013419385741e-05,
      "loss": 0.0663,
      "step": 1415
    },
    {
      "epoch": 1.047970479704797,
      "grad_norm": 1.7800077199935913,
      "learning_rate": 3.6414271462285465e-05,
      "loss": 0.0495,
      "step": 1420
    },
    {
      "epoch": 1.051660516605166,
      "grad_norm": 1.0056285858154297,
      "learning_rate": 3.632823829276004e-05,
      "loss": 0.087,
      "step": 1425
    },
    {
      "epoch": 1.055350553505535,
      "grad_norm": 2.0425143241882324,
      "learning_rate": 3.624203596992879e-05,
      "loss": 0.0739,
      "step": 1430
    },
    {
      "epoch": 1.0590405904059041,
      "grad_norm": 1.1351499557495117,
      "learning_rate": 3.615566578096521e-05,
      "loss": 0.0887,
      "step": 1435
    },
    {
      "epoch": 1.062730627306273,
      "grad_norm": 1.1877275705337524,
      "learning_rate": 3.6069129015549366e-05,
      "loss": 0.0377,
      "step": 1440
    },
    {
      "epoch": 1.066420664206642,
      "grad_norm": 4.840137004852295,
      "learning_rate": 3.5982426965848604e-05,
      "loss": 0.0529,
      "step": 1445
    },
    {
      "epoch": 1.070110701107011,
      "grad_norm": 1.0912177562713623,
      "learning_rate": 3.589556092649835e-05,
      "loss": 0.056,
      "step": 1450
    },
    {
      "epoch": 1.07380073800738,
      "grad_norm": 0.4298003017902374,
      "learning_rate": 3.5808532194582694e-05,
      "loss": 0.0408,
      "step": 1455
    },
    {
      "epoch": 1.0774907749077491,
      "grad_norm": 1.2224483489990234,
      "learning_rate": 3.5721342069615044e-05,
      "loss": 0.0535,
      "step": 1460
    },
    {
      "epoch": 1.081180811808118,
      "grad_norm": 1.5327374935150146,
      "learning_rate": 3.563399185351877e-05,
      "loss": 0.0429,
      "step": 1465
    },
    {
      "epoch": 1.084870848708487,
      "grad_norm": 0.7800693511962891,
      "learning_rate": 3.5546482850607663e-05,
      "loss": 0.0283,
      "step": 1470
    },
    {
      "epoch": 1.088560885608856,
      "grad_norm": 2.0185577869415283,
      "learning_rate": 3.5458816367566584e-05,
      "loss": 0.1039,
      "step": 1475
    },
    {
      "epoch": 1.0922509225092252,
      "grad_norm": 1.6751095056533813,
      "learning_rate": 3.537099371343184e-05,
      "loss": 0.0494,
      "step": 1480
    },
    {
      "epoch": 1.0959409594095941,
      "grad_norm": 0.1652858853340149,
      "learning_rate": 3.528301619957171e-05,
      "loss": 0.0462,
      "step": 1485
    },
    {
      "epoch": 1.099630996309963,
      "grad_norm": 1.5950359106063843,
      "learning_rate": 3.5194885139666825e-05,
      "loss": 0.0511,
      "step": 1490
    },
    {
      "epoch": 1.103321033210332,
      "grad_norm": 3.3886585235595703,
      "learning_rate": 3.5106601849690574e-05,
      "loss": 0.0606,
      "step": 1495
    },
    {
      "epoch": 1.1070110701107012,
      "grad_norm": 1.5114494562149048,
      "learning_rate": 3.5018167647889456e-05,
      "loss": 0.0215,
      "step": 1500
    },
    {
      "epoch": 1.1070110701107012,
      "eval_loss": 0.06803146004676819,
      "eval_runtime": 548.2757,
      "eval_samples_per_second": 4.943,
      "eval_steps_per_second": 4.943,
      "step": 1500
    },
    {
      "epoch": 1.1107011070110702,
      "grad_norm": 1.6622076034545898,
      "learning_rate": 3.492958385476336e-05,
      "loss": 0.0731,
      "step": 1505
    },
    {
      "epoch": 1.1143911439114391,
      "grad_norm": 2.6526076793670654,
      "learning_rate": 3.4840851793045906e-05,
      "loss": 0.0692,
      "step": 1510
    },
    {
      "epoch": 1.118081180811808,
      "grad_norm": 1.9092832803726196,
      "learning_rate": 3.475197278768462e-05,
      "loss": 0.073,
      "step": 1515
    },
    {
      "epoch": 1.121771217712177,
      "grad_norm": 1.8728662729263306,
      "learning_rate": 3.466294816582124e-05,
      "loss": 0.0736,
      "step": 1520
    },
    {
      "epoch": 1.1254612546125462,
      "grad_norm": 1.1259312629699707,
      "learning_rate": 3.457377925677182e-05,
      "loss": 0.0672,
      "step": 1525
    },
    {
      "epoch": 1.1291512915129152,
      "grad_norm": 1.4777958393096924,
      "learning_rate": 3.448446739200689e-05,
      "loss": 0.0638,
      "step": 1530
    },
    {
      "epoch": 1.132841328413284,
      "grad_norm": 0.37853577733039856,
      "learning_rate": 3.4395013905131656e-05,
      "loss": 0.0336,
      "step": 1535
    },
    {
      "epoch": 1.136531365313653,
      "grad_norm": 1.3967417478561401,
      "learning_rate": 3.430542013186598e-05,
      "loss": 0.028,
      "step": 1540
    },
    {
      "epoch": 1.140221402214022,
      "grad_norm": 1.1686761379241943,
      "learning_rate": 3.421568741002448e-05,
      "loss": 0.028,
      "step": 1545
    },
    {
      "epoch": 1.1439114391143912,
      "grad_norm": 2.1377146244049072,
      "learning_rate": 3.4125817079496594e-05,
      "loss": 0.0348,
      "step": 1550
    },
    {
      "epoch": 1.1476014760147601,
      "grad_norm": 4.320023059844971,
      "learning_rate": 3.403581048222648e-05,
      "loss": 0.0533,
      "step": 1555
    },
    {
      "epoch": 1.151291512915129,
      "grad_norm": 1.3077486753463745,
      "learning_rate": 3.394566896219307e-05,
      "loss": 0.0362,
      "step": 1560
    },
    {
      "epoch": 1.1549815498154983,
      "grad_norm": 1.9980788230895996,
      "learning_rate": 3.3855393865389956e-05,
      "loss": 0.1237,
      "step": 1565
    },
    {
      "epoch": 1.1586715867158672,
      "grad_norm": 3.632483720779419,
      "learning_rate": 3.376498653980529e-05,
      "loss": 0.0436,
      "step": 1570
    },
    {
      "epoch": 1.1623616236162362,
      "grad_norm": 2.4333572387695312,
      "learning_rate": 3.3674448335401664e-05,
      "loss": 0.0921,
      "step": 1575
    },
    {
      "epoch": 1.1660516605166051,
      "grad_norm": 3.328217029571533,
      "learning_rate": 3.358378060409597e-05,
      "loss": 0.0592,
      "step": 1580
    },
    {
      "epoch": 1.169741697416974,
      "grad_norm": 1.033312201499939,
      "learning_rate": 3.349298469973918e-05,
      "loss": 0.0727,
      "step": 1585
    },
    {
      "epoch": 1.1734317343173433,
      "grad_norm": 1.6954416036605835,
      "learning_rate": 3.340206197809615e-05,
      "loss": 0.0461,
      "step": 1590
    },
    {
      "epoch": 1.1771217712177122,
      "grad_norm": 2.3180766105651855,
      "learning_rate": 3.3311013796825377e-05,
      "loss": 0.0405,
      "step": 1595
    },
    {
      "epoch": 1.1808118081180812,
      "grad_norm": 1.0317797660827637,
      "learning_rate": 3.321984151545871e-05,
      "loss": 0.0133,
      "step": 1600
    },
    {
      "epoch": 1.1808118081180812,
      "eval_loss": 0.07233479619026184,
      "eval_runtime": 570.62,
      "eval_samples_per_second": 4.749,
      "eval_steps_per_second": 4.749,
      "step": 1600
    },
    {
      "epoch": 1.1845018450184501,
      "grad_norm": 1.1238235235214233,
      "learning_rate": 3.3128546495381086e-05,
      "loss": 0.0779,
      "step": 1605
    },
    {
      "epoch": 1.188191881918819,
      "grad_norm": 0.45757001638412476,
      "learning_rate": 3.3037130099810146e-05,
      "loss": 0.0665,
      "step": 1610
    },
    {
      "epoch": 1.1918819188191883,
      "grad_norm": 1.5244241952896118,
      "learning_rate": 3.2945593693775935e-05,
      "loss": 0.1175,
      "step": 1615
    },
    {
      "epoch": 1.1955719557195572,
      "grad_norm": 3.869534969329834,
      "learning_rate": 3.285393864410048e-05,
      "loss": 0.0553,
      "step": 1620
    },
    {
      "epoch": 1.1992619926199262,
      "grad_norm": 0.20777887105941772,
      "learning_rate": 3.2762166319377394e-05,
      "loss": 0.0465,
      "step": 1625
    },
    {
      "epoch": 1.2029520295202951,
      "grad_norm": 1.4760408401489258,
      "learning_rate": 3.267027808995146e-05,
      "loss": 0.0517,
      "step": 1630
    },
    {
      "epoch": 1.2066420664206643,
      "grad_norm": 1.2818872928619385,
      "learning_rate": 3.257827532789815e-05,
      "loss": 0.0461,
      "step": 1635
    },
    {
      "epoch": 1.2103321033210332,
      "grad_norm": 1.2581677436828613,
      "learning_rate": 3.248615940700309e-05,
      "loss": 0.0376,
      "step": 1640
    },
    {
      "epoch": 1.2140221402214022,
      "grad_norm": 2.0116584300994873,
      "learning_rate": 3.239393170274165e-05,
      "loss": 0.0611,
      "step": 1645
    },
    {
      "epoch": 1.2177121771217712,
      "grad_norm": 2.7792930603027344,
      "learning_rate": 3.230159359225834e-05,
      "loss": 0.046,
      "step": 1650
    },
    {
      "epoch": 1.2214022140221403,
      "grad_norm": 0.20544317364692688,
      "learning_rate": 3.220914645434623e-05,
      "loss": 0.0454,
      "step": 1655
    },
    {
      "epoch": 1.2250922509225093,
      "grad_norm": 1.6343120336532593,
      "learning_rate": 3.211659166942642e-05,
      "loss": 0.0788,
      "step": 1660
    },
    {
      "epoch": 1.2287822878228782,
      "grad_norm": 0.4056205749511719,
      "learning_rate": 3.2023930619527364e-05,
      "loss": 0.0514,
      "step": 1665
    },
    {
      "epoch": 1.2324723247232472,
      "grad_norm": 1.9369014501571655,
      "learning_rate": 3.19311646882643e-05,
      "loss": 0.0378,
      "step": 1670
    },
    {
      "epoch": 1.2361623616236161,
      "grad_norm": 1.702188491821289,
      "learning_rate": 3.1838295260818523e-05,
      "loss": 0.0588,
      "step": 1675
    },
    {
      "epoch": 1.2398523985239853,
      "grad_norm": 1.6596091985702515,
      "learning_rate": 3.174532372391674e-05,
      "loss": 0.0748,
      "step": 1680
    },
    {
      "epoch": 1.2435424354243543,
      "grad_norm": 0.5801753401756287,
      "learning_rate": 3.165225146581038e-05,
      "loss": 0.02,
      "step": 1685
    },
    {
      "epoch": 1.2472324723247232,
      "grad_norm": 1.5011470317840576,
      "learning_rate": 3.155907987625483e-05,
      "loss": 0.0458,
      "step": 1690
    },
    {
      "epoch": 1.2509225092250922,
      "grad_norm": 3.458390474319458,
      "learning_rate": 3.146581034648865e-05,
      "loss": 0.0826,
      "step": 1695
    },
    {
      "epoch": 1.2546125461254611,
      "grad_norm": 0.6191288232803345,
      "learning_rate": 3.1372444269212934e-05,
      "loss": 0.0658,
      "step": 1700
    },
    {
      "epoch": 1.2546125461254611,
      "eval_loss": 0.07052934169769287,
      "eval_runtime": 573.4519,
      "eval_samples_per_second": 4.726,
      "eval_steps_per_second": 4.726,
      "step": 1700
    },
    {
      "epoch": 1.2583025830258303,
      "grad_norm": 1.6361932754516602,
      "learning_rate": 3.1278983038570353e-05,
      "loss": 0.0742,
      "step": 1705
    },
    {
      "epoch": 1.2619926199261993,
      "grad_norm": 2.499321937561035,
      "learning_rate": 3.118542805012441e-05,
      "loss": 0.0464,
      "step": 1710
    },
    {
      "epoch": 1.2656826568265682,
      "grad_norm": 2.303081512451172,
      "learning_rate": 3.109178070083866e-05,
      "loss": 0.1035,
      "step": 1715
    },
    {
      "epoch": 1.2693726937269374,
      "grad_norm": 1.5995107889175415,
      "learning_rate": 3.09980423890557e-05,
      "loss": 0.0537,
      "step": 1720
    },
    {
      "epoch": 1.2730627306273063,
      "grad_norm": 2.156438112258911,
      "learning_rate": 3.090421451447647e-05,
      "loss": 0.092,
      "step": 1725
    },
    {
      "epoch": 1.2767527675276753,
      "grad_norm": 2.0839202404022217,
      "learning_rate": 3.0810298478139196e-05,
      "loss": 0.0434,
      "step": 1730
    },
    {
      "epoch": 1.2804428044280443,
      "grad_norm": 1.848191738128662,
      "learning_rate": 3.071629568239858e-05,
      "loss": 0.0377,
      "step": 1735
    },
    {
      "epoch": 1.2841328413284132,
      "grad_norm": 3.5247561931610107,
      "learning_rate": 3.062220753090478e-05,
      "loss": 0.0528,
      "step": 1740
    },
    {
      "epoch": 1.2878228782287824,
      "grad_norm": 0.5490167737007141,
      "learning_rate": 3.05280354285825e-05,
      "loss": 0.0502,
      "step": 1745
    },
    {
      "epoch": 1.2915129151291513,
      "grad_norm": 2.759477376937866,
      "learning_rate": 3.043378078161002e-05,
      "loss": 0.0522,
      "step": 1750
    },
    {
      "epoch": 1.2952029520295203,
      "grad_norm": 0.9689191579818726,
      "learning_rate": 3.033944499739814e-05,
      "loss": 0.0702,
      "step": 1755
    },
    {
      "epoch": 1.2988929889298892,
      "grad_norm": 3.4592809677124023,
      "learning_rate": 3.024502948456923e-05,
      "loss": 0.0567,
      "step": 1760
    },
    {
      "epoch": 1.3025830258302582,
      "grad_norm": 1.7704365253448486,
      "learning_rate": 3.0150535652936162e-05,
      "loss": 0.0572,
      "step": 1765
    },
    {
      "epoch": 1.3062730627306274,
      "grad_norm": 4.604969501495361,
      "learning_rate": 3.005596491348125e-05,
      "loss": 0.0684,
      "step": 1770
    },
    {
      "epoch": 1.3099630996309963,
      "grad_norm": 1.0457733869552612,
      "learning_rate": 2.996131867833521e-05,
      "loss": 0.0258,
      "step": 1775
    },
    {
      "epoch": 1.3136531365313653,
      "grad_norm": 1.3425709009170532,
      "learning_rate": 2.986659836075605e-05,
      "loss": 0.0326,
      "step": 1780
    },
    {
      "epoch": 1.3173431734317342,
      "grad_norm": 0.2796865999698639,
      "learning_rate": 2.9771805375107997e-05,
      "loss": 0.039,
      "step": 1785
    },
    {
      "epoch": 1.3210332103321032,
      "grad_norm": 3.3025128841400146,
      "learning_rate": 2.967694113684034e-05,
      "loss": 0.055,
      "step": 1790
    },
    {
      "epoch": 1.3247232472324724,
      "grad_norm": 3.2605316638946533,
      "learning_rate": 2.9582007062466306e-05,
      "loss": 0.0836,
      "step": 1795
    },
    {
      "epoch": 1.3284132841328413,
      "grad_norm": 1.6396101713180542,
      "learning_rate": 2.9487004569541943e-05,
      "loss": 0.048,
      "step": 1800
    },
    {
      "epoch": 1.3284132841328413,
      "eval_loss": 0.06804138422012329,
      "eval_runtime": 569.9606,
      "eval_samples_per_second": 4.755,
      "eval_steps_per_second": 4.755,
      "step": 1800
    },
    {
      "epoch": 1.3321033210332103,
      "grad_norm": 1.7053577899932861,
      "learning_rate": 2.93919350766449e-05,
      "loss": 0.0599,
      "step": 1805
    },
    {
      "epoch": 1.3357933579335795,
      "grad_norm": 0.42948785424232483,
      "learning_rate": 2.9296800003353275e-05,
      "loss": 0.0279,
      "step": 1810
    },
    {
      "epoch": 1.3394833948339484,
      "grad_norm": 0.5347057580947876,
      "learning_rate": 2.9201600770224425e-05,
      "loss": 0.0374,
      "step": 1815
    },
    {
      "epoch": 1.3431734317343174,
      "grad_norm": 1.6696206331253052,
      "learning_rate": 2.9106338798773723e-05,
      "loss": 0.0799,
      "step": 1820
    },
    {
      "epoch": 1.3468634686346863,
      "grad_norm": 1.205281138420105,
      "learning_rate": 2.9011015511453366e-05,
      "loss": 0.0459,
      "step": 1825
    },
    {
      "epoch": 1.3505535055350553,
      "grad_norm": 1.272951364517212,
      "learning_rate": 2.8915632331631114e-05,
      "loss": 0.0313,
      "step": 1830
    },
    {
      "epoch": 1.3542435424354244,
      "grad_norm": 2.1514945030212402,
      "learning_rate": 2.8820190683569044e-05,
      "loss": 0.0975,
      "step": 1835
    },
    {
      "epoch": 1.3579335793357934,
      "grad_norm": 1.9268314838409424,
      "learning_rate": 2.872469199240228e-05,
      "loss": 0.0397,
      "step": 1840
    },
    {
      "epoch": 1.3616236162361623,
      "grad_norm": 1.7532072067260742,
      "learning_rate": 2.8629137684117708e-05,
      "loss": 0.0643,
      "step": 1845
    },
    {
      "epoch": 1.3653136531365313,
      "grad_norm": 0.33578404784202576,
      "learning_rate": 2.8533529185532704e-05,
      "loss": 0.0576,
      "step": 1850
    },
    {
      "epoch": 1.3690036900369003,
      "grad_norm": 1.8330188989639282,
      "learning_rate": 2.8437867924273798e-05,
      "loss": 0.0732,
      "step": 1855
    },
    {
      "epoch": 1.3726937269372694,
      "grad_norm": 1.144234538078308,
      "learning_rate": 2.8342155328755376e-05,
      "loss": 0.0722,
      "step": 1860
    },
    {
      "epoch": 1.3763837638376384,
      "grad_norm": 1.186797022819519,
      "learning_rate": 2.824639282815836e-05,
      "loss": 0.0838,
      "step": 1865
    },
    {
      "epoch": 1.3800738007380073,
      "grad_norm": 0.4919196665287018,
      "learning_rate": 2.815058185240884e-05,
      "loss": 0.0458,
      "step": 1870
    },
    {
      "epoch": 1.3837638376383765,
      "grad_norm": 1.7849491834640503,
      "learning_rate": 2.8054723832156742e-05,
      "loss": 0.0794,
      "step": 1875
    },
    {
      "epoch": 1.3874538745387455,
      "grad_norm": 0.2671055495738983,
      "learning_rate": 2.795882019875446e-05,
      "loss": 0.063,
      "step": 1880
    },
    {
      "epoch": 1.3911439114391144,
      "grad_norm": 1.8723702430725098,
      "learning_rate": 2.7862872384235487e-05,
      "loss": 0.0815,
      "step": 1885
    },
    {
      "epoch": 1.3948339483394834,
      "grad_norm": 0.5393820405006409,
      "learning_rate": 2.7766881821293034e-05,
      "loss": 0.0376,
      "step": 1890
    },
    {
      "epoch": 1.3985239852398523,
      "grad_norm": 2.227513313293457,
      "learning_rate": 2.7670849943258604e-05,
      "loss": 0.1088,
      "step": 1895
    },
    {
      "epoch": 1.4022140221402215,
      "grad_norm": 1.170660138130188,
      "learning_rate": 2.757477818408066e-05,
      "loss": 0.0821,
      "step": 1900
    },
    {
      "epoch": 1.4022140221402215,
      "eval_loss": 0.06599080562591553,
      "eval_runtime": 560.8657,
      "eval_samples_per_second": 4.832,
      "eval_steps_per_second": 4.832,
      "step": 1900
    },
    {
      "epoch": 1.4059040590405905,
      "grad_norm": 1.1461701393127441,
      "learning_rate": 2.7478667978303137e-05,
      "loss": 0.0589,
      "step": 1905
    },
    {
      "epoch": 1.4095940959409594,
      "grad_norm": 1.6656529903411865,
      "learning_rate": 2.7382520761044065e-05,
      "loss": 0.0635,
      "step": 1910
    },
    {
      "epoch": 1.4132841328413284,
      "grad_norm": 0.3413958251476288,
      "learning_rate": 2.7286337967974135e-05,
      "loss": 0.0456,
      "step": 1915
    },
    {
      "epoch": 1.4169741697416973,
      "grad_norm": 1.3452194929122925,
      "learning_rate": 2.7190121035295246e-05,
      "loss": 0.0514,
      "step": 1920
    },
    {
      "epoch": 1.4206642066420665,
      "grad_norm": 0.7122889757156372,
      "learning_rate": 2.7093871399719072e-05,
      "loss": 0.0478,
      "step": 1925
    },
    {
      "epoch": 1.4243542435424354,
      "grad_norm": 0.49668359756469727,
      "learning_rate": 2.6997590498445625e-05,
      "loss": 0.0492,
      "step": 1930
    },
    {
      "epoch": 1.4280442804428044,
      "grad_norm": 1.2995778322219849,
      "learning_rate": 2.6901279769141748e-05,
      "loss": 0.0921,
      "step": 1935
    },
    {
      "epoch": 1.4317343173431734,
      "grad_norm": 0.46270668506622314,
      "learning_rate": 2.68049406499197e-05,
      "loss": 0.0833,
      "step": 1940
    },
    {
      "epoch": 1.4354243542435423,
      "grad_norm": 1.4475654363632202,
      "learning_rate": 2.670857457931563e-05,
      "loss": 0.048,
      "step": 1945
    },
    {
      "epoch": 1.4391143911439115,
      "grad_norm": 1.1613999605178833,
      "learning_rate": 2.6612182996268165e-05,
      "loss": 0.0434,
      "step": 1950
    },
    {
      "epoch": 1.4428044280442804,
      "grad_norm": 1.1785250902175903,
      "learning_rate": 2.6515767340096858e-05,
      "loss": 0.0811,
      "step": 1955
    },
    {
      "epoch": 1.4464944649446494,
      "grad_norm": 2.5099539756774902,
      "learning_rate": 2.6419329050480706e-05,
      "loss": 0.1041,
      "step": 1960
    },
    {
      "epoch": 1.4501845018450186,
      "grad_norm": 0.6226081252098083,
      "learning_rate": 2.6322869567436713e-05,
      "loss": 0.0517,
      "step": 1965
    },
    {
      "epoch": 1.4538745387453875,
      "grad_norm": 1.3704713582992554,
      "learning_rate": 2.6226390331298316e-05,
      "loss": 0.0706,
      "step": 1970
    },
    {
      "epoch": 1.4575645756457565,
      "grad_norm": 0.5301734805107117,
      "learning_rate": 2.61298927826939e-05,
      "loss": 0.0549,
      "step": 1975
    },
    {
      "epoch": 1.4612546125461254,
      "grad_norm": 2.229698896408081,
      "learning_rate": 2.60333783625253e-05,
      "loss": 0.0928,
      "step": 1980
    },
    {
      "epoch": 1.4649446494464944,
      "grad_norm": 1.0254179239273071,
      "learning_rate": 2.5936848511946286e-05,
      "loss": 0.0872,
      "step": 1985
    },
    {
      "epoch": 1.4686346863468636,
      "grad_norm": 0.48256441950798035,
      "learning_rate": 2.584030467234102e-05,
      "loss": 0.046,
      "step": 1990
    },
    {
      "epoch": 1.4723247232472325,
      "grad_norm": 1.344295859336853,
      "learning_rate": 2.574374828530256e-05,
      "loss": 0.0484,
      "step": 1995
    },
    {
      "epoch": 1.4760147601476015,
      "grad_norm": 0.5107486844062805,
      "learning_rate": 2.5647180792611307e-05,
      "loss": 0.0696,
      "step": 2000
    },
    {
      "epoch": 1.4760147601476015,
      "eval_loss": 0.06664016842842102,
      "eval_runtime": 566.4947,
      "eval_samples_per_second": 4.784,
      "eval_steps_per_second": 4.784,
      "step": 2000
    },
    {
      "epoch": 1.4797047970479704,
      "grad_norm": 1.9695314168930054,
      "learning_rate": 2.5550603636213515e-05,
      "loss": 0.0985,
      "step": 2005
    },
    {
      "epoch": 1.4833948339483394,
      "grad_norm": 2.023163318634033,
      "learning_rate": 2.5454018258199718e-05,
      "loss": 0.0562,
      "step": 2010
    },
    {
      "epoch": 1.4870848708487086,
      "grad_norm": 1.1033262014389038,
      "learning_rate": 2.5357426100783222e-05,
      "loss": 0.0566,
      "step": 2015
    },
    {
      "epoch": 1.4907749077490775,
      "grad_norm": 0.44780561327934265,
      "learning_rate": 2.5260828606278557e-05,
      "loss": 0.0412,
      "step": 2020
    },
    {
      "epoch": 1.4944649446494465,
      "grad_norm": 1.3030720949172974,
      "learning_rate": 2.5164227217079968e-05,
      "loss": 0.0674,
      "step": 2025
    },
    {
      "epoch": 1.4981549815498156,
      "grad_norm": 0.21664226055145264,
      "learning_rate": 2.5067623375639803e-05,
      "loss": 0.0466,
      "step": 2030
    },
    {
      "epoch": 1.5018450184501844,
      "grad_norm": 2.4712467193603516,
      "learning_rate": 2.4971018524447095e-05,
      "loss": 0.086,
      "step": 2035
    },
    {
      "epoch": 1.5055350553505535,
      "grad_norm": 0.7198724746704102,
      "learning_rate": 2.4874414106005918e-05,
      "loss": 0.0144,
      "step": 2040
    },
    {
      "epoch": 1.5092250922509225,
      "grad_norm": 0.3513696491718292,
      "learning_rate": 2.4777811562813876e-05,
      "loss": 0.0212,
      "step": 2045
    },
    {
      "epoch": 1.5129151291512914,
      "grad_norm": 0.32097670435905457,
      "learning_rate": 2.468121233734059e-05,
      "loss": 0.0257,
      "step": 2050
    },
    {
      "epoch": 1.5166051660516606,
      "grad_norm": 5.911133289337158,
      "learning_rate": 2.4584617872006136e-05,
      "loss": 0.0553,
      "step": 2055
    },
    {
      "epoch": 1.5202952029520294,
      "grad_norm": 1.7664576768875122,
      "learning_rate": 2.4488029609159512e-05,
      "loss": 0.0826,
      "step": 2060
    },
    {
      "epoch": 1.5239852398523985,
      "grad_norm": 1.2474279403686523,
      "learning_rate": 2.4391448991057096e-05,
      "loss": 0.0618,
      "step": 2065
    },
    {
      "epoch": 1.5276752767527675,
      "grad_norm": 1.1465954780578613,
      "learning_rate": 2.429487745984112e-05,
      "loss": 0.0427,
      "step": 2070
    },
    {
      "epoch": 1.5313653136531364,
      "grad_norm": 1.979305624961853,
      "learning_rate": 2.4198316457518127e-05,
      "loss": 0.044,
      "step": 2075
    },
    {
      "epoch": 1.5350553505535056,
      "grad_norm": 0.615767776966095,
      "learning_rate": 2.410176742593744e-05,
      "loss": 0.0499,
      "step": 2080
    },
    {
      "epoch": 1.5387453874538746,
      "grad_norm": 2.137544631958008,
      "learning_rate": 2.400523180676964e-05,
      "loss": 0.074,
      "step": 2085
    },
    {
      "epoch": 1.5424354243542435,
      "grad_norm": 1.760016679763794,
      "learning_rate": 2.3908711041485038e-05,
      "loss": 0.0481,
      "step": 2090
    },
    {
      "epoch": 1.5461254612546127,
      "grad_norm": 1.8861744403839111,
      "learning_rate": 2.3812206571332144e-05,
      "loss": 0.0732,
      "step": 2095
    },
    {
      "epoch": 1.5498154981549814,
      "grad_norm": 2.988640308380127,
      "learning_rate": 2.3715719837316134e-05,
      "loss": 0.0634,
      "step": 2100
    },
    {
      "epoch": 1.5498154981549814,
      "eval_loss": 0.06542418897151947,
      "eval_runtime": 573.1541,
      "eval_samples_per_second": 4.728,
      "eval_steps_per_second": 4.728,
      "step": 2100
    },
    {
      "epoch": 1.5535055350553506,
      "grad_norm": 2.2473413944244385,
      "learning_rate": 2.3619252280177365e-05,
      "loss": 0.0721,
      "step": 2105
    },
    {
      "epoch": 1.5571955719557196,
      "grad_norm": 1.5424590110778809,
      "learning_rate": 2.3522805340369847e-05,
      "loss": 0.0481,
      "step": 2110
    },
    {
      "epoch": 1.5608856088560885,
      "grad_norm": 2.1510961055755615,
      "learning_rate": 2.342638045803971e-05,
      "loss": 0.0915,
      "step": 2115
    },
    {
      "epoch": 1.5645756457564577,
      "grad_norm": 0.7612193822860718,
      "learning_rate": 2.332997907300374e-05,
      "loss": 0.0499,
      "step": 2120
    },
    {
      "epoch": 1.5682656826568264,
      "grad_norm": 1.517332673072815,
      "learning_rate": 2.323360262472787e-05,
      "loss": 0.0573,
      "step": 2125
    },
    {
      "epoch": 1.5719557195571956,
      "grad_norm": 1.5554600954055786,
      "learning_rate": 2.3137252552305656e-05,
      "loss": 0.0485,
      "step": 2130
    },
    {
      "epoch": 1.5756457564575646,
      "grad_norm": 2.191467046737671,
      "learning_rate": 2.304093029443682e-05,
      "loss": 0.0571,
      "step": 2135
    },
    {
      "epoch": 1.5793357933579335,
      "grad_norm": 1.3763842582702637,
      "learning_rate": 2.2944637289405756e-05,
      "loss": 0.0638,
      "step": 2140
    },
    {
      "epoch": 1.5830258302583027,
      "grad_norm": 1.902206301689148,
      "learning_rate": 2.2848374975060048e-05,
      "loss": 0.0771,
      "step": 2145
    },
    {
      "epoch": 1.5867158671586716,
      "grad_norm": 0.645244836807251,
      "learning_rate": 2.275214478878902e-05,
      "loss": 0.0849,
      "step": 2150
    },
    {
      "epoch": 1.5904059040590406,
      "grad_norm": 0.8614996671676636,
      "learning_rate": 2.2655948167502245e-05,
      "loss": 0.0543,
      "step": 2155
    },
    {
      "epoch": 1.5940959409594095,
      "grad_norm": 1.4130003452301025,
      "learning_rate": 2.2559786547608117e-05,
      "loss": 0.0268,
      "step": 2160
    },
    {
      "epoch": 1.5977859778597785,
      "grad_norm": 1.9484902620315552,
      "learning_rate": 2.2463661364992385e-05,
      "loss": 0.0713,
      "step": 2165
    },
    {
      "epoch": 1.6014760147601477,
      "grad_norm": 0.8544361591339111,
      "learning_rate": 2.2367574054996705e-05,
      "loss": 0.0646,
      "step": 2170
    },
    {
      "epoch": 1.6051660516605166,
      "grad_norm": 1.3142199516296387,
      "learning_rate": 2.2271526052397234e-05,
      "loss": 0.0792,
      "step": 2175
    },
    {
      "epoch": 1.6088560885608856,
      "grad_norm": 1.0492339134216309,
      "learning_rate": 2.2175518791383185e-05,
      "loss": 0.0718,
      "step": 2180
    },
    {
      "epoch": 1.6125461254612548,
      "grad_norm": 2.4238808155059814,
      "learning_rate": 2.2079553705535398e-05,
      "loss": 0.0521,
      "step": 2185
    },
    {
      "epoch": 1.6162361623616235,
      "grad_norm": 1.8102617263793945,
      "learning_rate": 2.1983632227805003e-05,
      "loss": 0.0521,
      "step": 2190
    },
    {
      "epoch": 1.6199261992619927,
      "grad_norm": 1.248416781425476,
      "learning_rate": 2.1887755790491924e-05,
      "loss": 0.0592,
      "step": 2195
    },
    {
      "epoch": 1.6236162361623616,
      "grad_norm": 2.615079402923584,
      "learning_rate": 2.1791925825223562e-05,
      "loss": 0.0345,
      "step": 2200
    },
    {
      "epoch": 1.6236162361623616,
      "eval_loss": 0.06587780267000198,
      "eval_runtime": 591.4176,
      "eval_samples_per_second": 4.582,
      "eval_steps_per_second": 4.582,
      "step": 2200
    },
    {
      "epoch": 1.6273062730627306,
      "grad_norm": 2.2395389080047607,
      "learning_rate": 2.169614376293341e-05,
      "loss": 0.0713,
      "step": 2205
    },
    {
      "epoch": 1.6309963099630997,
      "grad_norm": 2.9175398349761963,
      "learning_rate": 2.1600411033839628e-05,
      "loss": 0.0499,
      "step": 2210
    },
    {
      "epoch": 1.6346863468634685,
      "grad_norm": 1.8670539855957031,
      "learning_rate": 2.1504729067423786e-05,
      "loss": 0.0973,
      "step": 2215
    },
    {
      "epoch": 1.6383763837638377,
      "grad_norm": 2.0024917125701904,
      "learning_rate": 2.1409099292409436e-05,
      "loss": 0.0536,
      "step": 2220
    },
    {
      "epoch": 1.6420664206642066,
      "grad_norm": 0.6031743884086609,
      "learning_rate": 2.1313523136740816e-05,
      "loss": 0.0356,
      "step": 2225
    },
    {
      "epoch": 1.6457564575645756,
      "grad_norm": 2.1399378776550293,
      "learning_rate": 2.1218002027561515e-05,
      "loss": 0.0461,
      "step": 2230
    },
    {
      "epoch": 1.6494464944649447,
      "grad_norm": 2.2530410289764404,
      "learning_rate": 2.1122537391193182e-05,
      "loss": 0.0592,
      "step": 2235
    },
    {
      "epoch": 1.6531365313653137,
      "grad_norm": 0.5324140787124634,
      "learning_rate": 2.1027130653114186e-05,
      "loss": 0.0371,
      "step": 2240
    },
    {
      "epoch": 1.6568265682656826,
      "grad_norm": 2.1897308826446533,
      "learning_rate": 2.093178323793838e-05,
      "loss": 0.0764,
      "step": 2245
    },
    {
      "epoch": 1.6605166051660518,
      "grad_norm": 1.4017630815505981,
      "learning_rate": 2.0836496569393814e-05,
      "loss": 0.0491,
      "step": 2250
    },
    {
      "epoch": 1.6642066420664205,
      "grad_norm": 2.905689001083374,
      "learning_rate": 2.0741272070301425e-05,
      "loss": 0.0609,
      "step": 2255
    },
    {
      "epoch": 1.6678966789667897,
      "grad_norm": 1.509293794631958,
      "learning_rate": 2.0646111162553912e-05,
      "loss": 0.0738,
      "step": 2260
    },
    {
      "epoch": 1.6715867158671587,
      "grad_norm": 2.3219189643859863,
      "learning_rate": 2.055101526709435e-05,
      "loss": 0.0526,
      "step": 2265
    },
    {
      "epoch": 1.6752767527675276,
      "grad_norm": 5.046220779418945,
      "learning_rate": 2.0455985803895095e-05,
      "loss": 0.0467,
      "step": 2270
    },
    {
      "epoch": 1.6789667896678968,
      "grad_norm": 1.60061776638031,
      "learning_rate": 2.0361024191936523e-05,
      "loss": 0.0423,
      "step": 2275
    },
    {
      "epoch": 1.6826568265682655,
      "grad_norm": 1.9089301824569702,
      "learning_rate": 2.026613184918586e-05,
      "loss": 0.0555,
      "step": 2280
    },
    {
      "epoch": 1.6863468634686347,
      "grad_norm": 2.837414026260376,
      "learning_rate": 2.017131019257598e-05,
      "loss": 0.0625,
      "step": 2285
    },
    {
      "epoch": 1.6900369003690037,
      "grad_norm": 1.1778579950332642,
      "learning_rate": 2.007656063798433e-05,
      "loss": 0.1081,
      "step": 2290
    },
    {
      "epoch": 1.6937269372693726,
      "grad_norm": 3.09744930267334,
      "learning_rate": 1.9981884600211662e-05,
      "loss": 0.0996,
      "step": 2295
    },
    {
      "epoch": 1.6974169741697418,
      "grad_norm": 0.3208942711353302,
      "learning_rate": 1.988728349296101e-05,
      "loss": 0.0188,
      "step": 2300
    },
    {
      "epoch": 1.6974169741697418,
      "eval_loss": 0.06460066884756088,
      "eval_runtime": 587.7597,
      "eval_samples_per_second": 4.611,
      "eval_steps_per_second": 4.611,
      "step": 2300
    },
    {
      "epoch": 1.7011070110701108,
      "grad_norm": 1.7106599807739258,
      "learning_rate": 1.9792758728816543e-05,
      "loss": 0.0287,
      "step": 2305
    },
    {
      "epoch": 1.7047970479704797,
      "grad_norm": 0.11198779195547104,
      "learning_rate": 1.9698311719222463e-05,
      "loss": 0.0315,
      "step": 2310
    },
    {
      "epoch": 1.7084870848708487,
      "grad_norm": 0.4434213936328888,
      "learning_rate": 1.960394387446195e-05,
      "loss": 0.0921,
      "step": 2315
    },
    {
      "epoch": 1.7121771217712176,
      "grad_norm": 0.15194106101989746,
      "learning_rate": 1.950965660363609e-05,
      "loss": 0.0145,
      "step": 2320
    },
    {
      "epoch": 1.7158671586715868,
      "grad_norm": 2.681520938873291,
      "learning_rate": 1.9415451314642846e-05,
      "loss": 0.0232,
      "step": 2325
    },
    {
      "epoch": 1.7195571955719557,
      "grad_norm": 0.9697707295417786,
      "learning_rate": 1.9321329414156028e-05,
      "loss": 0.0924,
      "step": 2330
    },
    {
      "epoch": 1.7232472324723247,
      "grad_norm": 2.4252216815948486,
      "learning_rate": 1.922729230760427e-05,
      "loss": 0.0671,
      "step": 2335
    },
    {
      "epoch": 1.7269372693726939,
      "grad_norm": 1.7604128122329712,
      "learning_rate": 1.9133341399150077e-05,
      "loss": 0.0911,
      "step": 2340
    },
    {
      "epoch": 1.7306273062730626,
      "grad_norm": 2.084365129470825,
      "learning_rate": 1.9039478091668853e-05,
      "loss": 0.1335,
      "step": 2345
    },
    {
      "epoch": 1.7343173431734318,
      "grad_norm": 1.7834571599960327,
      "learning_rate": 1.8945703786727924e-05,
      "loss": 0.0537,
      "step": 2350
    },
    {
      "epoch": 1.7380073800738007,
      "grad_norm": 1.4630458354949951,
      "learning_rate": 1.885201988456563e-05,
      "loss": 0.0624,
      "step": 2355
    },
    {
      "epoch": 1.7416974169741697,
      "grad_norm": 1.7528870105743408,
      "learning_rate": 1.8758427784070448e-05,
      "loss": 0.0953,
      "step": 2360
    },
    {
      "epoch": 1.7453874538745389,
      "grad_norm": 1.8470308780670166,
      "learning_rate": 1.8664928882760036e-05,
      "loss": 0.0684,
      "step": 2365
    },
    {
      "epoch": 1.7490774907749076,
      "grad_norm": 1.5966577529907227,
      "learning_rate": 1.857152457676041e-05,
      "loss": 0.0625,
      "step": 2370
    },
    {
      "epoch": 1.7527675276752768,
      "grad_norm": 2.5439517498016357,
      "learning_rate": 1.8478216260785106e-05,
      "loss": 0.0623,
      "step": 2375
    },
    {
      "epoch": 1.7564575645756457,
      "grad_norm": 1.3547279834747314,
      "learning_rate": 1.8385005328114307e-05,
      "loss": 0.0932,
      "step": 2380
    },
    {
      "epoch": 1.7601476014760147,
      "grad_norm": 0.6051035523414612,
      "learning_rate": 1.8291893170574093e-05,
      "loss": 0.061,
      "step": 2385
    },
    {
      "epoch": 1.7638376383763839,
      "grad_norm": 0.13846872746944427,
      "learning_rate": 1.8198881178515627e-05,
      "loss": 0.0632,
      "step": 2390
    },
    {
      "epoch": 1.7675276752767528,
      "grad_norm": 0.9384050965309143,
      "learning_rate": 1.8105970740794393e-05,
      "loss": 0.0465,
      "step": 2395
    },
    {
      "epoch": 1.7712177121771218,
      "grad_norm": 0.9992460012435913,
      "learning_rate": 1.801316324474948e-05,
      "loss": 0.0585,
      "step": 2400
    },
    {
      "epoch": 1.7712177121771218,
      "eval_loss": 0.0639498233795166,
      "eval_runtime": 596.801,
      "eval_samples_per_second": 4.541,
      "eval_steps_per_second": 4.541,
      "step": 2400
    },
    {
      "epoch": 1.774907749077491,
      "grad_norm": 2.9456491470336914,
      "learning_rate": 1.7920460076182834e-05,
      "loss": 0.0649,
      "step": 2405
    },
    {
      "epoch": 1.7785977859778597,
      "grad_norm": 1.3448048830032349,
      "learning_rate": 1.7827862619338587e-05,
      "loss": 0.038,
      "step": 2410
    },
    {
      "epoch": 1.7822878228782288,
      "grad_norm": 1.7371410131454468,
      "learning_rate": 1.7735372256882392e-05,
      "loss": 0.0993,
      "step": 2415
    },
    {
      "epoch": 1.7859778597785978,
      "grad_norm": 0.20828081667423248,
      "learning_rate": 1.7642990369880755e-05,
      "loss": 0.0435,
      "step": 2420
    },
    {
      "epoch": 1.7896678966789668,
      "grad_norm": 3.550438642501831,
      "learning_rate": 1.7550718337780446e-05,
      "loss": 0.095,
      "step": 2425
    },
    {
      "epoch": 1.793357933579336,
      "grad_norm": 0.7029083371162415,
      "learning_rate": 1.745855753838787e-05,
      "loss": 0.0501,
      "step": 2430
    },
    {
      "epoch": 1.7970479704797047,
      "grad_norm": 1.549548864364624,
      "learning_rate": 1.736650934784849e-05,
      "loss": 0.0583,
      "step": 2435
    },
    {
      "epoch": 1.8007380073800738,
      "grad_norm": 2.986330270767212,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 0.071,
      "step": 2440
    },
    {
      "epoch": 1.8044280442804428,
      "grad_norm": 2.339939594268799,
      "learning_rate": 1.718275628948335e-05,
      "loss": 0.0764,
      "step": 2445
    },
    {
      "epoch": 1.8081180811808117,
      "grad_norm": 0.5564755797386169,
      "learning_rate": 1.7091054165459078e-05,
      "loss": 0.0427,
      "step": 2450
    },
    {
      "epoch": 1.811808118081181,
      "grad_norm": 1.4224809408187866,
      "learning_rate": 1.6999470137850062e-05,
      "loss": 0.0739,
      "step": 2455
    },
    {
      "epoch": 1.8154981549815496,
      "grad_norm": 2.2485733032226562,
      "learning_rate": 1.69080055741894e-05,
      "loss": 0.0543,
      "step": 2460
    },
    {
      "epoch": 1.8191881918819188,
      "grad_norm": 0.5229928493499756,
      "learning_rate": 1.681666184022638e-05,
      "loss": 0.0183,
      "step": 2465
    },
    {
      "epoch": 1.8228782287822878,
      "grad_norm": 1.5491924285888672,
      "learning_rate": 1.672544029990606e-05,
      "loss": 0.0287,
      "step": 2470
    },
    {
      "epoch": 1.8265682656826567,
      "grad_norm": 0.5510640740394592,
      "learning_rate": 1.6634342315348878e-05,
      "loss": 0.0548,
      "step": 2475
    },
    {
      "epoch": 1.830258302583026,
      "grad_norm": 1.4093683958053589,
      "learning_rate": 1.654336924683037e-05,
      "loss": 0.0679,
      "step": 2480
    },
    {
      "epoch": 1.8339483394833949,
      "grad_norm": 1.2900657653808594,
      "learning_rate": 1.64525224527608e-05,
      "loss": 0.0444,
      "step": 2485
    },
    {
      "epoch": 1.8376383763837638,
      "grad_norm": 0.691888153553009,
      "learning_rate": 1.6361803289664917e-05,
      "loss": 0.0679,
      "step": 2490
    },
    {
      "epoch": 1.841328413284133,
      "grad_norm": 0.3542804718017578,
      "learning_rate": 1.6271213112161677e-05,
      "loss": 0.053,
      "step": 2495
    },
    {
      "epoch": 1.8450184501845017,
      "grad_norm": 3.3139867782592773,
      "learning_rate": 1.618075327294403e-05,
      "loss": 0.0803,
      "step": 2500
    },
    {
      "epoch": 1.8450184501845017,
      "eval_loss": 0.06486159563064575,
      "eval_runtime": 588.2274,
      "eval_samples_per_second": 4.607,
      "eval_steps_per_second": 4.607,
      "step": 2500
    },
    {
      "epoch": 1.848708487084871,
      "grad_norm": 1.159657597541809,
      "learning_rate": 1.6090425122758692e-05,
      "loss": 0.0327,
      "step": 2505
    },
    {
      "epoch": 1.8523985239852399,
      "grad_norm": 2.54899263381958,
      "learning_rate": 1.6000230010386025e-05,
      "loss": 0.0449,
      "step": 2510
    },
    {
      "epoch": 1.8560885608856088,
      "grad_norm": 2.2778759002685547,
      "learning_rate": 1.591016928261986e-05,
      "loss": 0.0756,
      "step": 2515
    },
    {
      "epoch": 1.859778597785978,
      "grad_norm": 0.4970366656780243,
      "learning_rate": 1.582024428424737e-05,
      "loss": 0.0503,
      "step": 2520
    },
    {
      "epoch": 1.8634686346863467,
      "grad_norm": 2.1535563468933105,
      "learning_rate": 1.5730456358029074e-05,
      "loss": 0.0662,
      "step": 2525
    },
    {
      "epoch": 1.867158671586716,
      "grad_norm": 2.917880058288574,
      "learning_rate": 1.5640806844678678e-05,
      "loss": 0.0765,
      "step": 2530
    },
    {
      "epoch": 1.8708487084870848,
      "grad_norm": 2.987576961517334,
      "learning_rate": 1.5551297082843125e-05,
      "loss": 0.0674,
      "step": 2535
    },
    {
      "epoch": 1.8745387453874538,
      "grad_norm": 2.4554333686828613,
      "learning_rate": 1.546192840908259e-05,
      "loss": 0.0562,
      "step": 2540
    },
    {
      "epoch": 1.878228782287823,
      "grad_norm": 2.208893299102783,
      "learning_rate": 1.537270215785052e-05,
      "loss": 0.0858,
      "step": 2545
    },
    {
      "epoch": 1.881918819188192,
      "grad_norm": 1.9316470623016357,
      "learning_rate": 1.5283619661473683e-05,
      "loss": 0.0844,
      "step": 2550
    },
    {
      "epoch": 1.8856088560885609,
      "grad_norm": 0.7733399271965027,
      "learning_rate": 1.5194682250132356e-05,
      "loss": 0.0403,
      "step": 2555
    },
    {
      "epoch": 1.8892988929889298,
      "grad_norm": 0.9179968237876892,
      "learning_rate": 1.5105891251840343e-05,
      "loss": 0.0193,
      "step": 2560
    },
    {
      "epoch": 1.8929889298892988,
      "grad_norm": 0.4339173138141632,
      "learning_rate": 1.5017247992425249e-05,
      "loss": 0.0172,
      "step": 2565
    },
    {
      "epoch": 1.896678966789668,
      "grad_norm": 2.0401687622070312,
      "learning_rate": 1.4928753795508626e-05,
      "loss": 0.124,
      "step": 2570
    },
    {
      "epoch": 1.900369003690037,
      "grad_norm": 2.437347173690796,
      "learning_rate": 1.4840409982486217e-05,
      "loss": 0.0271,
      "step": 2575
    },
    {
      "epoch": 1.9040590405904059,
      "grad_norm": 2.313161611557007,
      "learning_rate": 1.4752217872508245e-05,
      "loss": 0.0559,
      "step": 2580
    },
    {
      "epoch": 1.907749077490775,
      "grad_norm": 2.4586055278778076,
      "learning_rate": 1.4664178782459698e-05,
      "loss": 0.0412,
      "step": 2585
    },
    {
      "epoch": 1.9114391143911438,
      "grad_norm": 0.8669052124023438,
      "learning_rate": 1.4576294026940679e-05,
      "loss": 0.0451,
      "step": 2590
    },
    {
      "epoch": 1.915129151291513,
      "grad_norm": 0.6269888877868652,
      "learning_rate": 1.448856491824674e-05,
      "loss": 0.0347,
      "step": 2595
    },
    {
      "epoch": 1.918819188191882,
      "grad_norm": 0.6721287369728088,
      "learning_rate": 1.4400992766349352e-05,
      "loss": 0.0327,
      "step": 2600
    },
    {
      "epoch": 1.918819188191882,
      "eval_loss": 0.06363344937562943,
      "eval_runtime": 589.6946,
      "eval_samples_per_second": 4.596,
      "eval_steps_per_second": 4.596,
      "step": 2600
    },
    {
      "epoch": 1.9225092250922509,
      "grad_norm": 1.6737703084945679,
      "learning_rate": 1.4313578878876276e-05,
      "loss": 0.0381,
      "step": 2605
    },
    {
      "epoch": 1.92619926199262,
      "grad_norm": 0.5208994150161743,
      "learning_rate": 1.4226324561092067e-05,
      "loss": 0.0247,
      "step": 2610
    },
    {
      "epoch": 1.9298892988929888,
      "grad_norm": 0.4629512131214142,
      "learning_rate": 1.4139231115878618e-05,
      "loss": 0.0854,
      "step": 2615
    },
    {
      "epoch": 1.933579335793358,
      "grad_norm": 2.392482042312622,
      "learning_rate": 1.4052299843715627e-05,
      "loss": 0.0901,
      "step": 2620
    },
    {
      "epoch": 1.937269372693727,
      "grad_norm": 1.3687798976898193,
      "learning_rate": 1.3965532042661261e-05,
      "loss": 0.0403,
      "step": 2625
    },
    {
      "epoch": 1.9409594095940959,
      "grad_norm": 0.15286991000175476,
      "learning_rate": 1.3878929008332725e-05,
      "loss": 0.0659,
      "step": 2630
    },
    {
      "epoch": 1.944649446494465,
      "grad_norm": 2.5726511478424072,
      "learning_rate": 1.3792492033886914e-05,
      "loss": 0.0761,
      "step": 2635
    },
    {
      "epoch": 1.948339483394834,
      "grad_norm": 0.47805503010749817,
      "learning_rate": 1.3706222410001115e-05,
      "loss": 0.0168,
      "step": 2640
    },
    {
      "epoch": 1.952029520295203,
      "grad_norm": 2.0792133808135986,
      "learning_rate": 1.3620121424853765e-05,
      "loss": 0.0953,
      "step": 2645
    },
    {
      "epoch": 1.9557195571955721,
      "grad_norm": 1.6478005647659302,
      "learning_rate": 1.3534190364105148e-05,
      "loss": 0.042,
      "step": 2650
    },
    {
      "epoch": 1.9594095940959408,
      "grad_norm": 1.3254081010818481,
      "learning_rate": 1.3448430510878263e-05,
      "loss": 0.0744,
      "step": 2655
    },
    {
      "epoch": 1.96309963099631,
      "grad_norm": 1.8684905767440796,
      "learning_rate": 1.336284314573964e-05,
      "loss": 0.0597,
      "step": 2660
    },
    {
      "epoch": 1.966789667896679,
      "grad_norm": 0.8300767540931702,
      "learning_rate": 1.3277429546680192e-05,
      "loss": 0.0446,
      "step": 2665
    },
    {
      "epoch": 1.970479704797048,
      "grad_norm": 1.6724882125854492,
      "learning_rate": 1.3192190989096164e-05,
      "loss": 0.0396,
      "step": 2670
    },
    {
      "epoch": 1.974169741697417,
      "grad_norm": 2.0974936485290527,
      "learning_rate": 1.3107128745770106e-05,
      "loss": 0.0504,
      "step": 2675
    },
    {
      "epoch": 1.9778597785977858,
      "grad_norm": 1.7003815174102783,
      "learning_rate": 1.302224408685181e-05,
      "loss": 0.0898,
      "step": 2680
    },
    {
      "epoch": 1.981549815498155,
      "grad_norm": 0.17643238604068756,
      "learning_rate": 1.2937538279839379e-05,
      "loss": 0.0354,
      "step": 2685
    },
    {
      "epoch": 1.985239852398524,
      "grad_norm": 0.20699866116046906,
      "learning_rate": 1.2853012589560325e-05,
      "loss": 0.0396,
      "step": 2690
    },
    {
      "epoch": 1.988929889298893,
      "grad_norm": 1.1350589990615845,
      "learning_rate": 1.2768668278152646e-05,
      "loss": 0.0497,
      "step": 2695
    },
    {
      "epoch": 1.992619926199262,
      "grad_norm": 0.7264536023139954,
      "learning_rate": 1.268450660504597e-05,
      "loss": 0.0285,
      "step": 2700
    },
    {
      "epoch": 1.992619926199262,
      "eval_loss": 0.06500259786844254,
      "eval_runtime": 593.8778,
      "eval_samples_per_second": 4.563,
      "eval_steps_per_second": 4.563,
      "step": 2700
    },
    {
      "epoch": 1.996309963099631,
      "grad_norm": 1.12884521484375,
      "learning_rate": 1.2600528826942783e-05,
      "loss": 0.0248,
      "step": 2705
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.445352077484131,
      "learning_rate": 1.2516736197799666e-05,
      "loss": 0.0856,
      "step": 2710
    },
    {
      "epoch": 2.003690036900369,
      "grad_norm": 1.3266403675079346,
      "learning_rate": 1.2433129968808518e-05,
      "loss": 0.0362,
      "step": 2715
    },
    {
      "epoch": 2.007380073800738,
      "grad_norm": 2.7190613746643066,
      "learning_rate": 1.2349711388377943e-05,
      "loss": 0.0722,
      "step": 2720
    },
    {
      "epoch": 2.011070110701107,
      "grad_norm": 0.5153468251228333,
      "learning_rate": 1.2266481702114573e-05,
      "loss": 0.0674,
      "step": 2725
    },
    {
      "epoch": 2.014760147601476,
      "grad_norm": 1.2030431032180786,
      "learning_rate": 1.2183442152804453e-05,
      "loss": 0.0307,
      "step": 2730
    },
    {
      "epoch": 2.018450184501845,
      "grad_norm": 1.4579075574874878,
      "learning_rate": 1.2100593980394503e-05,
      "loss": 0.0497,
      "step": 2735
    },
    {
      "epoch": 2.022140221402214,
      "grad_norm": 0.49506765604019165,
      "learning_rate": 1.2017938421974026e-05,
      "loss": 0.031,
      "step": 2740
    },
    {
      "epoch": 2.025830258302583,
      "grad_norm": 0.8252438306808472,
      "learning_rate": 1.1935476711756194e-05,
      "loss": 0.0192,
      "step": 2745
    },
    {
      "epoch": 2.029520295202952,
      "grad_norm": 0.717593252658844,
      "learning_rate": 1.1853210081059616e-05,
      "loss": 0.0776,
      "step": 2750
    },
    {
      "epoch": 2.0332103321033212,
      "grad_norm": 0.9701664447784424,
      "learning_rate": 1.1771139758290034e-05,
      "loss": 0.0195,
      "step": 2755
    },
    {
      "epoch": 2.03690036900369,
      "grad_norm": 2.2838633060455322,
      "learning_rate": 1.1689266968921866e-05,
      "loss": 0.0492,
      "step": 2760
    },
    {
      "epoch": 2.040590405904059,
      "grad_norm": 4.879565238952637,
      "learning_rate": 1.1607592935479967e-05,
      "loss": 0.0751,
      "step": 2765
    },
    {
      "epoch": 2.044280442804428,
      "grad_norm": 0.26311829686164856,
      "learning_rate": 1.152611887752139e-05,
      "loss": 0.0367,
      "step": 2770
    },
    {
      "epoch": 2.047970479704797,
      "grad_norm": 1.8576960563659668,
      "learning_rate": 1.1444846011617137e-05,
      "loss": 0.0476,
      "step": 2775
    },
    {
      "epoch": 2.0516605166051662,
      "grad_norm": 2.8926141262054443,
      "learning_rate": 1.1363775551333994e-05,
      "loss": 0.0222,
      "step": 2780
    },
    {
      "epoch": 2.055350553505535,
      "grad_norm": 0.2784949243068695,
      "learning_rate": 1.1282908707216461e-05,
      "loss": 0.0385,
      "step": 2785
    },
    {
      "epoch": 2.059040590405904,
      "grad_norm": 1.6559406518936157,
      "learning_rate": 1.120224668676863e-05,
      "loss": 0.0399,
      "step": 2790
    },
    {
      "epoch": 2.062730627306273,
      "grad_norm": 0.9912197589874268,
      "learning_rate": 1.1121790694436138e-05,
      "loss": 0.0592,
      "step": 2795
    },
    {
      "epoch": 2.066420664206642,
      "grad_norm": 0.1605531871318817,
      "learning_rate": 1.1041541931588248e-05,
      "loss": 0.0286,
      "step": 2800
    },
    {
      "epoch": 2.066420664206642,
      "eval_loss": 0.06894487142562866,
      "eval_runtime": 592.5925,
      "eval_samples_per_second": 4.573,
      "eval_steps_per_second": 4.573,
      "step": 2800
    },
    {
      "epoch": 2.0701107011070112,
      "grad_norm": 1.2839933633804321,
      "learning_rate": 1.0961501596499844e-05,
      "loss": 0.0255,
      "step": 2805
    },
    {
      "epoch": 2.07380073800738,
      "grad_norm": 3.615651845932007,
      "learning_rate": 1.0881670884333562e-05,
      "loss": 0.0542,
      "step": 2810
    },
    {
      "epoch": 2.077490774907749,
      "grad_norm": 1.62949800491333,
      "learning_rate": 1.0802050987121984e-05,
      "loss": 0.0743,
      "step": 2815
    },
    {
      "epoch": 2.081180811808118,
      "grad_norm": 1.341800570487976,
      "learning_rate": 1.0722643093749755e-05,
      "loss": 0.0479,
      "step": 2820
    },
    {
      "epoch": 2.084870848708487,
      "grad_norm": 2.296569347381592,
      "learning_rate": 1.064344838993592e-05,
      "loss": 0.0694,
      "step": 2825
    },
    {
      "epoch": 2.088560885608856,
      "grad_norm": 3.036564588546753,
      "learning_rate": 1.056446805821614e-05,
      "loss": 0.0837,
      "step": 2830
    },
    {
      "epoch": 2.092250922509225,
      "grad_norm": 1.025934100151062,
      "learning_rate": 1.0485703277925105e-05,
      "loss": 0.044,
      "step": 2835
    },
    {
      "epoch": 2.095940959409594,
      "grad_norm": 2.634660482406616,
      "learning_rate": 1.0407155225178855e-05,
      "loss": 0.0609,
      "step": 2840
    },
    {
      "epoch": 2.0996309963099633,
      "grad_norm": 1.6712716817855835,
      "learning_rate": 1.0328825072857293e-05,
      "loss": 0.0258,
      "step": 2845
    },
    {
      "epoch": 2.103321033210332,
      "grad_norm": 4.199821949005127,
      "learning_rate": 1.0250713990586589e-05,
      "loss": 0.0901,
      "step": 2850
    },
    {
      "epoch": 2.107011070110701,
      "grad_norm": 2.5456674098968506,
      "learning_rate": 1.0172823144721796e-05,
      "loss": 0.0415,
      "step": 2855
    },
    {
      "epoch": 2.11070110701107,
      "grad_norm": 1.4859395027160645,
      "learning_rate": 1.0095153698329365e-05,
      "loss": 0.0392,
      "step": 2860
    },
    {
      "epoch": 2.114391143911439,
      "grad_norm": 1.8153024911880493,
      "learning_rate": 1.0017706811169842e-05,
      "loss": 0.0522,
      "step": 2865
    },
    {
      "epoch": 2.1180811808118083,
      "grad_norm": 1.9925254583358765,
      "learning_rate": 9.940483639680484e-06,
      "loss": 0.0501,
      "step": 2870
    },
    {
      "epoch": 2.121771217712177,
      "grad_norm": 1.6189802885055542,
      "learning_rate": 9.86348533695804e-06,
      "loss": 0.0347,
      "step": 2875
    },
    {
      "epoch": 2.125461254612546,
      "grad_norm": 2.8059885501861572,
      "learning_rate": 9.786713052741526e-06,
      "loss": 0.042,
      "step": 2880
    },
    {
      "epoch": 2.129151291512915,
      "grad_norm": 2.43234920501709,
      "learning_rate": 9.710167933395026e-06,
      "loss": 0.0586,
      "step": 2885
    },
    {
      "epoch": 2.132841328413284,
      "grad_norm": 1.0781952142715454,
      "learning_rate": 9.63385112189062e-06,
      "loss": 0.0273,
      "step": 2890
    },
    {
      "epoch": 2.1365313653136533,
      "grad_norm": 1.0970765352249146,
      "learning_rate": 9.557763757791293e-06,
      "loss": 0.0328,
      "step": 2895
    },
    {
      "epoch": 2.140221402214022,
      "grad_norm": 5.080964088439941,
      "learning_rate": 9.481906977233903e-06,
      "loss": 0.0607,
      "step": 2900
    },
    {
      "epoch": 2.140221402214022,
      "eval_loss": 0.06687062233686447,
      "eval_runtime": 595.803,
      "eval_samples_per_second": 4.548,
      "eval_steps_per_second": 4.548,
      "step": 2900
    },
    {
      "epoch": 2.143911439114391,
      "grad_norm": 0.06601503491401672,
      "learning_rate": 9.406281912912237e-06,
      "loss": 0.0444,
      "step": 2905
    },
    {
      "epoch": 2.14760147601476,
      "grad_norm": 1.2644795179367065,
      "learning_rate": 9.330889694060116e-06,
      "loss": 0.0145,
      "step": 2910
    },
    {
      "epoch": 2.151291512915129,
      "grad_norm": 2.808718681335449,
      "learning_rate": 9.255731446434488e-06,
      "loss": 0.0476,
      "step": 2915
    },
    {
      "epoch": 2.1549815498154983,
      "grad_norm": 0.06179938092827797,
      "learning_rate": 9.180808292298626e-06,
      "loss": 0.0799,
      "step": 2920
    },
    {
      "epoch": 2.158671586715867,
      "grad_norm": 0.6606511473655701,
      "learning_rate": 9.10612135040545e-06,
      "loss": 0.0254,
      "step": 2925
    },
    {
      "epoch": 2.162361623616236,
      "grad_norm": 1.3704946041107178,
      "learning_rate": 9.031671735980698e-06,
      "loss": 0.0531,
      "step": 2930
    },
    {
      "epoch": 2.1660516605166054,
      "grad_norm": 1.5043842792510986,
      "learning_rate": 8.957460560706344e-06,
      "loss": 0.0458,
      "step": 2935
    },
    {
      "epoch": 2.169741697416974,
      "grad_norm": 1.4224834442138672,
      "learning_rate": 8.883488932704007e-06,
      "loss": 0.0429,
      "step": 2940
    },
    {
      "epoch": 2.1734317343173433,
      "grad_norm": 1.9879802465438843,
      "learning_rate": 8.809757956518372e-06,
      "loss": 0.0654,
      "step": 2945
    },
    {
      "epoch": 2.177121771217712,
      "grad_norm": 2.8109309673309326,
      "learning_rate": 8.7362687331007e-06,
      "loss": 0.0457,
      "step": 2950
    },
    {
      "epoch": 2.180811808118081,
      "grad_norm": 0.8290753364562988,
      "learning_rate": 8.663022359792416e-06,
      "loss": 0.075,
      "step": 2955
    },
    {
      "epoch": 2.1845018450184504,
      "grad_norm": 1.0434608459472656,
      "learning_rate": 8.59001993030871e-06,
      "loss": 0.0116,
      "step": 2960
    },
    {
      "epoch": 2.188191881918819,
      "grad_norm": 1.4746617078781128,
      "learning_rate": 8.517262534722187e-06,
      "loss": 0.0609,
      "step": 2965
    },
    {
      "epoch": 2.1918819188191883,
      "grad_norm": 0.2964126467704773,
      "learning_rate": 8.444751259446595e-06,
      "loss": 0.023,
      "step": 2970
    },
    {
      "epoch": 2.195571955719557,
      "grad_norm": 2.242353677749634,
      "learning_rate": 8.37248718722065e-06,
      "loss": 0.0517,
      "step": 2975
    },
    {
      "epoch": 2.199261992619926,
      "grad_norm": 1.3305643796920776,
      "learning_rate": 8.300471397091785e-06,
      "loss": 0.0567,
      "step": 2980
    },
    {
      "epoch": 2.2029520295202953,
      "grad_norm": 2.3981010913848877,
      "learning_rate": 8.228704964400118e-06,
      "loss": 0.0427,
      "step": 2985
    },
    {
      "epoch": 2.206642066420664,
      "grad_norm": 1.6395480632781982,
      "learning_rate": 8.15718896076236e-06,
      "loss": 0.0448,
      "step": 2990
    },
    {
      "epoch": 2.2103321033210332,
      "grad_norm": 1.8614939451217651,
      "learning_rate": 8.085924454055788e-06,
      "loss": 0.0318,
      "step": 2995
    },
    {
      "epoch": 2.2140221402214024,
      "grad_norm": 2.960951089859009,
      "learning_rate": 8.014912508402336e-06,
      "loss": 0.0622,
      "step": 3000
    },
    {
      "epoch": 2.2140221402214024,
      "eval_loss": 0.0650692731142044,
      "eval_runtime": 593.7992,
      "eval_samples_per_second": 4.564,
      "eval_steps_per_second": 4.564,
      "step": 3000
    },
    {
      "epoch": 2.217712177121771,
      "grad_norm": 0.7419664263725281,
      "learning_rate": 7.944154184152718e-06,
      "loss": 0.036,
      "step": 3005
    },
    {
      "epoch": 2.2214022140221403,
      "grad_norm": 0.49458062648773193,
      "learning_rate": 7.873650537870547e-06,
      "loss": 0.0475,
      "step": 3010
    },
    {
      "epoch": 2.225092250922509,
      "grad_norm": 1.635029673576355,
      "learning_rate": 7.803402622316578e-06,
      "loss": 0.0436,
      "step": 3015
    },
    {
      "epoch": 2.2287822878228782,
      "grad_norm": 0.6179764270782471,
      "learning_rate": 7.733411486433015e-06,
      "loss": 0.0236,
      "step": 3020
    },
    {
      "epoch": 2.2324723247232474,
      "grad_norm": 1.9116837978363037,
      "learning_rate": 7.66367817532783e-06,
      "loss": 0.0572,
      "step": 3025
    },
    {
      "epoch": 2.236162361623616,
      "grad_norm": 1.5901999473571777,
      "learning_rate": 7.5942037302591225e-06,
      "loss": 0.0603,
      "step": 3030
    },
    {
      "epoch": 2.2398523985239853,
      "grad_norm": 3.297255516052246,
      "learning_rate": 7.524989188619641e-06,
      "loss": 0.0539,
      "step": 3035
    },
    {
      "epoch": 2.243542435424354,
      "grad_norm": 1.0494853258132935,
      "learning_rate": 7.456035583921226e-06,
      "loss": 0.0179,
      "step": 3040
    },
    {
      "epoch": 2.2472324723247232,
      "grad_norm": 3.038639545440674,
      "learning_rate": 7.387343945779413e-06,
      "loss": 0.0382,
      "step": 3045
    },
    {
      "epoch": 2.2509225092250924,
      "grad_norm": 0.25260472297668457,
      "learning_rate": 7.318915299898057e-06,
      "loss": 0.0465,
      "step": 3050
    },
    {
      "epoch": 2.254612546125461,
      "grad_norm": 0.2084852159023285,
      "learning_rate": 7.250750668054027e-06,
      "loss": 0.0462,
      "step": 3055
    },
    {
      "epoch": 2.2583025830258303,
      "grad_norm": 1.218538522720337,
      "learning_rate": 7.182851068081895e-06,
      "loss": 0.0538,
      "step": 3060
    },
    {
      "epoch": 2.2619926199261995,
      "grad_norm": 2.130192279815674,
      "learning_rate": 7.115217513858813e-06,
      "loss": 0.0714,
      "step": 3065
    },
    {
      "epoch": 2.265682656826568,
      "grad_norm": 2.793891429901123,
      "learning_rate": 7.0478510152893166e-06,
      "loss": 0.0421,
      "step": 3070
    },
    {
      "epoch": 2.2693726937269374,
      "grad_norm": 3.4039533138275146,
      "learning_rate": 6.980752578290256e-06,
      "loss": 0.0558,
      "step": 3075
    },
    {
      "epoch": 2.273062730627306,
      "grad_norm": 0.470706582069397,
      "learning_rate": 6.9139232047758185e-06,
      "loss": 0.0624,
      "step": 3080
    },
    {
      "epoch": 2.2767527675276753,
      "grad_norm": 0.6827083230018616,
      "learning_rate": 6.847363892642497e-06,
      "loss": 0.0083,
      "step": 3085
    },
    {
      "epoch": 2.280442804428044,
      "grad_norm": 3.6665291786193848,
      "learning_rate": 6.781075635754258e-06,
      "loss": 0.0389,
      "step": 3090
    },
    {
      "epoch": 2.284132841328413,
      "grad_norm": 1.1246269941329956,
      "learning_rate": 6.715059423927642e-06,
      "loss": 0.0268,
      "step": 3095
    },
    {
      "epoch": 2.2878228782287824,
      "grad_norm": 2.0165367126464844,
      "learning_rate": 6.649316242917036e-06,
      "loss": 0.0165,
      "step": 3100
    },
    {
      "epoch": 2.2878228782287824,
      "eval_loss": 0.06544697284698486,
      "eval_runtime": 599.3175,
      "eval_samples_per_second": 4.522,
      "eval_steps_per_second": 4.522,
      "step": 3100
    },
    {
      "epoch": 2.291512915129151,
      "grad_norm": 0.04502851888537407,
      "learning_rate": 6.583847074399907e-06,
      "loss": 0.0652,
      "step": 3105
    },
    {
      "epoch": 2.2952029520295203,
      "grad_norm": 4.028783798217773,
      "learning_rate": 6.518652895962194e-06,
      "loss": 0.0662,
      "step": 3110
    },
    {
      "epoch": 2.2988929889298895,
      "grad_norm": 1.2129141092300415,
      "learning_rate": 6.453734681083656e-06,
      "loss": 0.0166,
      "step": 3115
    },
    {
      "epoch": 2.302583025830258,
      "grad_norm": 1.88726007938385,
      "learning_rate": 6.389093399123369e-06,
      "loss": 0.0256,
      "step": 3120
    },
    {
      "epoch": 2.3062730627306274,
      "grad_norm": 5.157069683074951,
      "learning_rate": 6.324730015305253e-06,
      "loss": 0.0792,
      "step": 3125
    },
    {
      "epoch": 2.3099630996309966,
      "grad_norm": 3.3595235347747803,
      "learning_rate": 6.260645490703657e-06,
      "loss": 0.079,
      "step": 3130
    },
    {
      "epoch": 2.3136531365313653,
      "grad_norm": 2.8369553089141846,
      "learning_rate": 6.196840782228988e-06,
      "loss": 0.0737,
      "step": 3135
    },
    {
      "epoch": 2.3173431734317345,
      "grad_norm": 2.523402214050293,
      "learning_rate": 6.13331684261344e-06,
      "loss": 0.0534,
      "step": 3140
    },
    {
      "epoch": 2.321033210332103,
      "grad_norm": 0.4252462685108185,
      "learning_rate": 6.070074620396776e-06,
      "loss": 0.0365,
      "step": 3145
    },
    {
      "epoch": 2.3247232472324724,
      "grad_norm": 3.0627596378326416,
      "learning_rate": 6.007115059912139e-06,
      "loss": 0.0688,
      "step": 3150
    },
    {
      "epoch": 2.328413284132841,
      "grad_norm": 0.5303922891616821,
      "learning_rate": 5.944439101271973e-06,
      "loss": 0.0364,
      "step": 3155
    },
    {
      "epoch": 2.3321033210332103,
      "grad_norm": 2.680690050125122,
      "learning_rate": 5.882047680353991e-06,
      "loss": 0.101,
      "step": 3160
    },
    {
      "epoch": 2.3357933579335795,
      "grad_norm": 1.2276328802108765,
      "learning_rate": 5.819941728787165e-06,
      "loss": 0.0701,
      "step": 3165
    },
    {
      "epoch": 2.339483394833948,
      "grad_norm": 0.32111188769340515,
      "learning_rate": 5.758122173937841e-06,
      "loss": 0.0381,
      "step": 3170
    },
    {
      "epoch": 2.3431734317343174,
      "grad_norm": 1.2291704416275024,
      "learning_rate": 5.696589938895908e-06,
      "loss": 0.0895,
      "step": 3175
    },
    {
      "epoch": 2.3468634686346865,
      "grad_norm": 0.6529101729393005,
      "learning_rate": 5.635345942460968e-06,
      "loss": 0.033,
      "step": 3180
    },
    {
      "epoch": 2.3505535055350553,
      "grad_norm": 1.7703865766525269,
      "learning_rate": 5.574391099128648e-06,
      "loss": 0.0609,
      "step": 3185
    },
    {
      "epoch": 2.3542435424354244,
      "grad_norm": 2.3334498405456543,
      "learning_rate": 5.51372631907697e-06,
      "loss": 0.0893,
      "step": 3190
    },
    {
      "epoch": 2.357933579335793,
      "grad_norm": 1.189040184020996,
      "learning_rate": 5.453352508152696e-06,
      "loss": 0.0379,
      "step": 3195
    },
    {
      "epoch": 2.3616236162361623,
      "grad_norm": 1.0032154321670532,
      "learning_rate": 5.393270567857834e-06,
      "loss": 0.0663,
      "step": 3200
    },
    {
      "epoch": 2.3616236162361623,
      "eval_loss": 0.0653260201215744,
      "eval_runtime": 594.9402,
      "eval_samples_per_second": 4.555,
      "eval_steps_per_second": 4.555,
      "step": 3200
    },
    {
      "epoch": 2.3653136531365315,
      "grad_norm": 0.7757476568222046,
      "learning_rate": 5.333481395336207e-06,
      "loss": 0.0249,
      "step": 3205
    },
    {
      "epoch": 2.3690036900369003,
      "grad_norm": 3.7875397205352783,
      "learning_rate": 5.273985883360003e-06,
      "loss": 0.0672,
      "step": 3210
    },
    {
      "epoch": 2.3726937269372694,
      "grad_norm": 0.17098118364810944,
      "learning_rate": 5.2147849203164735e-06,
      "loss": 0.0765,
      "step": 3215
    },
    {
      "epoch": 2.376383763837638,
      "grad_norm": 0.7811250686645508,
      "learning_rate": 5.15587939019467e-06,
      "loss": 0.0209,
      "step": 3220
    },
    {
      "epoch": 2.3800738007380073,
      "grad_norm": 0.18436011672019958,
      "learning_rate": 5.097270172572252e-06,
      "loss": 0.0276,
      "step": 3225
    },
    {
      "epoch": 2.3837638376383765,
      "grad_norm": 1.3136285543441772,
      "learning_rate": 5.038958142602304e-06,
      "loss": 0.0513,
      "step": 3230
    },
    {
      "epoch": 2.3874538745387452,
      "grad_norm": 1.7508167028427124,
      "learning_rate": 4.98094417100034e-06,
      "loss": 0.0378,
      "step": 3235
    },
    {
      "epoch": 2.3911439114391144,
      "grad_norm": 1.2399580478668213,
      "learning_rate": 4.923229124031239e-06,
      "loss": 0.0495,
      "step": 3240
    },
    {
      "epoch": 2.3948339483394836,
      "grad_norm": 1.690981388092041,
      "learning_rate": 4.865813863496344e-06,
      "loss": 0.0357,
      "step": 3245
    },
    {
      "epoch": 2.3985239852398523,
      "grad_norm": 0.9256649613380432,
      "learning_rate": 4.8086992467205856e-06,
      "loss": 0.0409,
      "step": 3250
    },
    {
      "epoch": 2.4022140221402215,
      "grad_norm": 0.08689957857131958,
      "learning_rate": 4.751886126539692e-06,
      "loss": 0.028,
      "step": 3255
    },
    {
      "epoch": 2.4059040590405902,
      "grad_norm": 2.2774391174316406,
      "learning_rate": 4.695375351287429e-06,
      "loss": 0.0314,
      "step": 3260
    },
    {
      "epoch": 2.4095940959409594,
      "grad_norm": 0.2122199833393097,
      "learning_rate": 4.63916776478294e-06,
      "loss": 0.0136,
      "step": 3265
    },
    {
      "epoch": 2.4132841328413286,
      "grad_norm": 0.09995319694280624,
      "learning_rate": 4.583264206318175e-06,
      "loss": 0.0265,
      "step": 3270
    },
    {
      "epoch": 2.4169741697416973,
      "grad_norm": 1.9102333784103394,
      "learning_rate": 4.527665510645321e-06,
      "loss": 0.0744,
      "step": 3275
    },
    {
      "epoch": 2.4206642066420665,
      "grad_norm": 0.21775661408901215,
      "learning_rate": 4.472372507964345e-06,
      "loss": 0.0178,
      "step": 3280
    },
    {
      "epoch": 2.4243542435424352,
      "grad_norm": 2.103846311569214,
      "learning_rate": 4.417386023910627e-06,
      "loss": 0.0709,
      "step": 3285
    },
    {
      "epoch": 2.4280442804428044,
      "grad_norm": 3.267167091369629,
      "learning_rate": 4.3627068795426e-06,
      "loss": 0.0533,
      "step": 3290
    },
    {
      "epoch": 2.4317343173431736,
      "grad_norm": 2.295492649078369,
      "learning_rate": 4.3083358913294795e-06,
      "loss": 0.0304,
      "step": 3295
    },
    {
      "epoch": 2.4354243542435423,
      "grad_norm": 1.7399301528930664,
      "learning_rate": 4.254273871139131e-06,
      "loss": 0.0936,
      "step": 3300
    },
    {
      "epoch": 2.4354243542435423,
      "eval_loss": 0.06572537124156952,
      "eval_runtime": 593.8861,
      "eval_samples_per_second": 4.563,
      "eval_steps_per_second": 4.563,
      "step": 3300
    },
    {
      "epoch": 2.4391143911439115,
      "grad_norm": 2.5577199459075928,
      "learning_rate": 4.200521626225876e-06,
      "loss": 0.051,
      "step": 3305
    },
    {
      "epoch": 2.4428044280442807,
      "grad_norm": 3.3016858100891113,
      "learning_rate": 4.14707995921847e-06,
      "loss": 0.0486,
      "step": 3310
    },
    {
      "epoch": 2.4464944649446494,
      "grad_norm": 3.1808269023895264,
      "learning_rate": 4.093949668108149e-06,
      "loss": 0.0555,
      "step": 3315
    },
    {
      "epoch": 2.4501845018450186,
      "grad_norm": 2.343363046646118,
      "learning_rate": 4.041131546236642e-06,
      "loss": 0.0416,
      "step": 3320
    },
    {
      "epoch": 2.4538745387453873,
      "grad_norm": 1.7773722410202026,
      "learning_rate": 3.988626382284397e-06,
      "loss": 0.0303,
      "step": 3325
    },
    {
      "epoch": 2.4575645756457565,
      "grad_norm": 3.134054660797119,
      "learning_rate": 3.936434960258767e-06,
      "loss": 0.0617,
      "step": 3330
    },
    {
      "epoch": 2.4612546125461257,
      "grad_norm": 0.4491100609302521,
      "learning_rate": 3.884558059482293e-06,
      "loss": 0.0498,
      "step": 3335
    },
    {
      "epoch": 2.4649446494464944,
      "grad_norm": 0.2833981513977051,
      "learning_rate": 3.832996454581092e-06,
      "loss": 0.0376,
      "step": 3340
    },
    {
      "epoch": 2.4686346863468636,
      "grad_norm": 0.32268738746643066,
      "learning_rate": 3.7817509154732922e-06,
      "loss": 0.0391,
      "step": 3345
    },
    {
      "epoch": 2.4723247232472323,
      "grad_norm": 5.731948375701904,
      "learning_rate": 3.7308222073575094e-06,
      "loss": 0.0728,
      "step": 3350
    },
    {
      "epoch": 2.4760147601476015,
      "grad_norm": 0.1349635124206543,
      "learning_rate": 3.680211090701455e-06,
      "loss": 0.0546,
      "step": 3355
    },
    {
      "epoch": 2.4797047970479706,
      "grad_norm": 3.2329261302948,
      "learning_rate": 3.6299183212305383e-06,
      "loss": 0.1073,
      "step": 3360
    },
    {
      "epoch": 2.4833948339483394,
      "grad_norm": 1.5156272649765015,
      "learning_rate": 3.579944649916639e-06,
      "loss": 0.0469,
      "step": 3365
    },
    {
      "epoch": 2.4870848708487086,
      "grad_norm": 0.954068124294281,
      "learning_rate": 3.5302908229668337e-06,
      "loss": 0.0345,
      "step": 3370
    },
    {
      "epoch": 2.4907749077490777,
      "grad_norm": 0.33387407660484314,
      "learning_rate": 3.480957581812297e-06,
      "loss": 0.0323,
      "step": 3375
    },
    {
      "epoch": 2.4944649446494465,
      "grad_norm": 1.4299033880233765,
      "learning_rate": 3.4319456630972134e-06,
      "loss": 0.0761,
      "step": 3380
    },
    {
      "epoch": 2.4981549815498156,
      "grad_norm": 1.1242378950119019,
      "learning_rate": 3.383255798667767e-06,
      "loss": 0.0338,
      "step": 3385
    },
    {
      "epoch": 2.5018450184501844,
      "grad_norm": 1.7511957883834839,
      "learning_rate": 3.334888715561238e-06,
      "loss": 0.0793,
      "step": 3390
    },
    {
      "epoch": 2.5055350553505535,
      "grad_norm": 1.5817204713821411,
      "learning_rate": 3.2868451359951425e-06,
      "loss": 0.0462,
      "step": 3395
    },
    {
      "epoch": 2.5092250922509223,
      "grad_norm": 1.475242018699646,
      "learning_rate": 3.239125777356422e-06,
      "loss": 0.0279,
      "step": 3400
    },
    {
      "epoch": 2.5092250922509223,
      "eval_loss": 0.06457580626010895,
      "eval_runtime": 599.5253,
      "eval_samples_per_second": 4.52,
      "eval_steps_per_second": 4.52,
      "step": 3400
    },
    {
      "epoch": 2.5129151291512914,
      "grad_norm": 1.4340800046920776,
      "learning_rate": 3.1917313521907523e-06,
      "loss": 0.0554,
      "step": 3405
    },
    {
      "epoch": 2.5166051660516606,
      "grad_norm": 1.5868713855743408,
      "learning_rate": 3.144662568191917e-06,
      "loss": 0.0729,
      "step": 3410
    },
    {
      "epoch": 2.5202952029520294,
      "grad_norm": 1.5963199138641357,
      "learning_rate": 3.0979201281912005e-06,
      "loss": 0.0541,
      "step": 3415
    },
    {
      "epoch": 2.5239852398523985,
      "grad_norm": 1.4463778734207153,
      "learning_rate": 3.051504730146937e-06,
      "loss": 0.0392,
      "step": 3420
    },
    {
      "epoch": 2.5276752767527677,
      "grad_norm": 0.17774233222007751,
      "learning_rate": 3.0054170671340644e-06,
      "loss": 0.0188,
      "step": 3425
    },
    {
      "epoch": 2.5313653136531364,
      "grad_norm": 2.4994328022003174,
      "learning_rate": 2.9596578273337757e-06,
      "loss": 0.0893,
      "step": 3430
    },
    {
      "epoch": 2.5350553505535056,
      "grad_norm": 2.512526273727417,
      "learning_rate": 2.9142276940232386e-06,
      "loss": 0.0564,
      "step": 3435
    },
    {
      "epoch": 2.538745387453875,
      "grad_norm": 1.3644694089889526,
      "learning_rate": 2.8691273455654226e-06,
      "loss": 0.0456,
      "step": 3440
    },
    {
      "epoch": 2.5424354243542435,
      "grad_norm": 0.07690747082233429,
      "learning_rate": 2.824357455398935e-06,
      "loss": 0.0504,
      "step": 3445
    },
    {
      "epoch": 2.5461254612546127,
      "grad_norm": 0.46599701046943665,
      "learning_rate": 2.7799186920279634e-06,
      "loss": 0.0821,
      "step": 3450
    },
    {
      "epoch": 2.5498154981549814,
      "grad_norm": 0.4905661344528198,
      "learning_rate": 2.735811719012349e-06,
      "loss": 0.0482,
      "step": 3455
    },
    {
      "epoch": 2.5535055350553506,
      "grad_norm": 0.8866363167762756,
      "learning_rate": 2.6920371949576028e-06,
      "loss": 0.0444,
      "step": 3460
    },
    {
      "epoch": 2.5571955719557193,
      "grad_norm": 1.9122803211212158,
      "learning_rate": 2.648595773505114e-06,
      "loss": 0.0472,
      "step": 3465
    },
    {
      "epoch": 2.5608856088560885,
      "grad_norm": 0.7763166427612305,
      "learning_rate": 2.6054881033223926e-06,
      "loss": 0.0667,
      "step": 3470
    },
    {
      "epoch": 2.5645756457564577,
      "grad_norm": 0.29138562083244324,
      "learning_rate": 2.5627148280933655e-06,
      "loss": 0.0802,
      "step": 3475
    },
    {
      "epoch": 2.5682656826568264,
      "grad_norm": 0.8798866271972656,
      "learning_rate": 2.520276586508763e-06,
      "loss": 0.0344,
      "step": 3480
    },
    {
      "epoch": 2.5719557195571956,
      "grad_norm": 1.3219735622406006,
      "learning_rate": 2.478174012256612e-06,
      "loss": 0.0552,
      "step": 3485
    },
    {
      "epoch": 2.5756457564575648,
      "grad_norm": 0.2319089025259018,
      "learning_rate": 2.436407734012744e-06,
      "loss": 0.0275,
      "step": 3490
    },
    {
      "epoch": 2.5793357933579335,
      "grad_norm": 2.172945499420166,
      "learning_rate": 2.3949783754314087e-06,
      "loss": 0.0412,
      "step": 3495
    },
    {
      "epoch": 2.5830258302583027,
      "grad_norm": 3.0840659141540527,
      "learning_rate": 2.353886555135984e-06,
      "loss": 0.0301,
      "step": 3500
    },
    {
      "epoch": 2.5830258302583027,
      "eval_loss": 0.06420361250638962,
      "eval_runtime": 600.8097,
      "eval_samples_per_second": 4.511,
      "eval_steps_per_second": 4.511,
      "step": 3500
    },
    {
      "epoch": 2.586715867158672,
      "grad_norm": 0.9933333992958069,
      "learning_rate": 2.313132886709718e-06,
      "loss": 0.0354,
      "step": 3505
    },
    {
      "epoch": 2.5904059040590406,
      "grad_norm": 0.29186558723449707,
      "learning_rate": 2.2727179786865643e-06,
      "loss": 0.0137,
      "step": 3510
    },
    {
      "epoch": 2.5940959409594093,
      "grad_norm": 0.17971591651439667,
      "learning_rate": 2.232642434542123e-06,
      "loss": 0.0277,
      "step": 3515
    },
    {
      "epoch": 2.5977859778597785,
      "grad_norm": 4.060500621795654,
      "learning_rate": 2.192906852684598e-06,
      "loss": 0.0526,
      "step": 3520
    },
    {
      "epoch": 2.6014760147601477,
      "grad_norm": 1.699852705001831,
      "learning_rate": 2.153511826445881e-06,
      "loss": 0.0791,
      "step": 3525
    },
    {
      "epoch": 2.6051660516605164,
      "grad_norm": 3.2642123699188232,
      "learning_rate": 2.114457944072676e-06,
      "loss": 0.112,
      "step": 3530
    },
    {
      "epoch": 2.6088560885608856,
      "grad_norm": 1.4935957193374634,
      "learning_rate": 2.075745788717745e-06,
      "loss": 0.0353,
      "step": 3535
    },
    {
      "epoch": 2.6125461254612548,
      "grad_norm": 1.7088661193847656,
      "learning_rate": 2.0373759384311604e-06,
      "loss": 0.0301,
      "step": 3540
    },
    {
      "epoch": 2.6162361623616235,
      "grad_norm": 4.580039978027344,
      "learning_rate": 1.9993489661516985e-06,
      "loss": 0.0801,
      "step": 3545
    },
    {
      "epoch": 2.6199261992619927,
      "grad_norm": 1.9606924057006836,
      "learning_rate": 1.9616654396982875e-06,
      "loss": 0.0288,
      "step": 3550
    },
    {
      "epoch": 2.623616236162362,
      "grad_norm": 1.4626941680908203,
      "learning_rate": 1.924325921761519e-06,
      "loss": 0.0369,
      "step": 3555
    },
    {
      "epoch": 2.6273062730627306,
      "grad_norm": 0.0675877258181572,
      "learning_rate": 1.8873309698952373e-06,
      "loss": 0.0946,
      "step": 3560
    },
    {
      "epoch": 2.6309963099630997,
      "grad_norm": 3.1660478115081787,
      "learning_rate": 1.8506811365082393e-06,
      "loss": 0.0413,
      "step": 3565
    },
    {
      "epoch": 2.6346863468634685,
      "grad_norm": 1.4703869819641113,
      "learning_rate": 1.814376968856002e-06,
      "loss": 0.0567,
      "step": 3570
    },
    {
      "epoch": 2.6383763837638377,
      "grad_norm": 1.0598969459533691,
      "learning_rate": 1.778419009032517e-06,
      "loss": 0.0718,
      "step": 3575
    },
    {
      "epoch": 2.6420664206642064,
      "grad_norm": 2.061828374862671,
      "learning_rate": 1.7428077939622145e-06,
      "loss": 0.0481,
      "step": 3580
    },
    {
      "epoch": 2.6457564575645756,
      "grad_norm": 5.199922561645508,
      "learning_rate": 1.7075438553919076e-06,
      "loss": 0.0932,
      "step": 3585
    },
    {
      "epoch": 2.6494464944649447,
      "grad_norm": 0.7075324058532715,
      "learning_rate": 1.6726277198828937e-06,
      "loss": 0.029,
      "step": 3590
    },
    {
      "epoch": 2.6531365313653135,
      "grad_norm": 1.6280457973480225,
      "learning_rate": 1.6380599088030746e-06,
      "loss": 0.0415,
      "step": 3595
    },
    {
      "epoch": 2.6568265682656826,
      "grad_norm": 1.7309558391571045,
      "learning_rate": 1.6038409383191566e-06,
      "loss": 0.0397,
      "step": 3600
    },
    {
      "epoch": 2.6568265682656826,
      "eval_loss": 0.0642206147313118,
      "eval_runtime": 599.6672,
      "eval_samples_per_second": 4.519,
      "eval_steps_per_second": 4.519,
      "step": 3600
    },
    {
      "epoch": 2.660516605166052,
      "grad_norm": 1.313924789428711,
      "learning_rate": 1.5699713193889576e-06,
      "loss": 0.0514,
      "step": 3605
    },
    {
      "epoch": 2.6642066420664205,
      "grad_norm": 1.8660197257995605,
      "learning_rate": 1.5364515577537902e-06,
      "loss": 0.0438,
      "step": 3610
    },
    {
      "epoch": 2.6678966789667897,
      "grad_norm": 1.8189857006072998,
      "learning_rate": 1.503282153930885e-06,
      "loss": 0.0566,
      "step": 3615
    },
    {
      "epoch": 2.671586715867159,
      "grad_norm": 0.7019834518432617,
      "learning_rate": 1.4704636032059266e-06,
      "loss": 0.0349,
      "step": 3620
    },
    {
      "epoch": 2.6752767527675276,
      "grad_norm": 1.262666940689087,
      "learning_rate": 1.437996395625668e-06,
      "loss": 0.0672,
      "step": 3625
    },
    {
      "epoch": 2.678966789667897,
      "grad_norm": 0.731503963470459,
      "learning_rate": 1.4058810159906038e-06,
      "loss": 0.0527,
      "step": 3630
    },
    {
      "epoch": 2.6826568265682655,
      "grad_norm": 1.0446629524230957,
      "learning_rate": 1.3741179438477219e-06,
      "loss": 0.0607,
      "step": 3635
    },
    {
      "epoch": 2.6863468634686347,
      "grad_norm": 0.1601203829050064,
      "learning_rate": 1.3427076534833689e-06,
      "loss": 0.0287,
      "step": 3640
    },
    {
      "epoch": 2.6900369003690034,
      "grad_norm": 0.6805328726768494,
      "learning_rate": 1.3116506139161377e-06,
      "loss": 0.0243,
      "step": 3645
    },
    {
      "epoch": 2.6937269372693726,
      "grad_norm": 3.3461461067199707,
      "learning_rate": 1.2809472888898832e-06,
      "loss": 0.0467,
      "step": 3650
    },
    {
      "epoch": 2.697416974169742,
      "grad_norm": 0.8003926277160645,
      "learning_rate": 1.2505981368667984e-06,
      "loss": 0.0322,
      "step": 3655
    },
    {
      "epoch": 2.7011070110701105,
      "grad_norm": 3.8915462493896484,
      "learning_rate": 1.2206036110205627e-06,
      "loss": 0.0729,
      "step": 3660
    },
    {
      "epoch": 2.7047970479704797,
      "grad_norm": 1.8767586946487427,
      "learning_rate": 1.1909641592295656e-06,
      "loss": 0.036,
      "step": 3665
    },
    {
      "epoch": 2.708487084870849,
      "grad_norm": 0.13748717308044434,
      "learning_rate": 1.1616802240702384e-06,
      "loss": 0.0174,
      "step": 3670
    },
    {
      "epoch": 2.7121771217712176,
      "grad_norm": 1.9944595098495483,
      "learning_rate": 1.1327522428104392e-06,
      "loss": 0.0595,
      "step": 3675
    },
    {
      "epoch": 2.715867158671587,
      "grad_norm": 3.405123710632324,
      "learning_rate": 1.1041806474029054e-06,
      "loss": 0.0974,
      "step": 3680
    },
    {
      "epoch": 2.719557195571956,
      "grad_norm": 1.1250057220458984,
      "learning_rate": 1.0759658644788396e-06,
      "loss": 0.0445,
      "step": 3685
    },
    {
      "epoch": 2.7232472324723247,
      "grad_norm": 3.726780414581299,
      "learning_rate": 1.0481083153415088e-06,
      "loss": 0.0576,
      "step": 3690
    },
    {
      "epoch": 2.726937269372694,
      "grad_norm": 3.352012872695923,
      "learning_rate": 1.020608415959959e-06,
      "loss": 0.0457,
      "step": 3695
    },
    {
      "epoch": 2.7306273062730626,
      "grad_norm": 1.4340448379516602,
      "learning_rate": 9.934665769628122e-07,
      "loss": 0.0382,
      "step": 3700
    },
    {
      "epoch": 2.7306273062730626,
      "eval_loss": 0.06424421072006226,
      "eval_runtime": 597.7519,
      "eval_samples_per_second": 4.534,
      "eval_steps_per_second": 4.534,
      "step": 3700
    },
    {
      "epoch": 2.734317343173432,
      "grad_norm": 0.37088853120803833,
      "learning_rate": 9.666832036321404e-07,
      "loss": 0.0424,
      "step": 3705
    },
    {
      "epoch": 2.7380073800738005,
      "grad_norm": 1.5152705907821655,
      "learning_rate": 9.402586958973863e-07,
      "loss": 0.0649,
      "step": 3710
    },
    {
      "epoch": 2.7416974169741697,
      "grad_norm": 1.2738274335861206,
      "learning_rate": 9.141934483294229e-07,
      "loss": 0.0169,
      "step": 3715
    },
    {
      "epoch": 2.745387453874539,
      "grad_norm": 0.9406409859657288,
      "learning_rate": 8.884878501346478e-07,
      "loss": 0.059,
      "step": 3720
    },
    {
      "epoch": 2.7490774907749076,
      "grad_norm": 2.353428602218628,
      "learning_rate": 8.631422851491711e-07,
      "loss": 0.0395,
      "step": 3725
    },
    {
      "epoch": 2.7527675276752768,
      "grad_norm": 1.287653923034668,
      "learning_rate": 8.381571318330783e-07,
      "loss": 0.0491,
      "step": 3730
    },
    {
      "epoch": 2.756457564575646,
      "grad_norm": 0.4777575731277466,
      "learning_rate": 8.135327632648015e-07,
      "loss": 0.0427,
      "step": 3735
    },
    {
      "epoch": 2.7601476014760147,
      "grad_norm": 2.356327772140503,
      "learning_rate": 7.89269547135521e-07,
      "loss": 0.0332,
      "step": 3740
    },
    {
      "epoch": 2.763837638376384,
      "grad_norm": 2.193833351135254,
      "learning_rate": 7.653678457436864e-07,
      "loss": 0.076,
      "step": 3745
    },
    {
      "epoch": 2.767527675276753,
      "grad_norm": 0.8835663795471191,
      "learning_rate": 7.418280159896152e-07,
      "loss": 0.0342,
      "step": 3750
    },
    {
      "epoch": 2.7712177121771218,
      "grad_norm": 0.10097620636224747,
      "learning_rate": 7.18650409370153e-07,
      "loss": 0.0465,
      "step": 3755
    },
    {
      "epoch": 2.774907749077491,
      "grad_norm": 2.4641926288604736,
      "learning_rate": 6.958353719734273e-07,
      "loss": 0.0484,
      "step": 3760
    },
    {
      "epoch": 2.7785977859778597,
      "grad_norm": 2.0251498222351074,
      "learning_rate": 6.733832444736799e-07,
      "loss": 0.047,
      "step": 3765
    },
    {
      "epoch": 2.782287822878229,
      "grad_norm": 3.4433529376983643,
      "learning_rate": 6.512943621261841e-07,
      "loss": 0.0853,
      "step": 3770
    },
    {
      "epoch": 2.7859778597785976,
      "grad_norm": 1.8637727499008179,
      "learning_rate": 6.295690547622218e-07,
      "loss": 0.0541,
      "step": 3775
    },
    {
      "epoch": 2.7896678966789668,
      "grad_norm": 2.3117220401763916,
      "learning_rate": 6.082076467841951e-07,
      "loss": 0.0332,
      "step": 3780
    },
    {
      "epoch": 2.793357933579336,
      "grad_norm": 0.08671137690544128,
      "learning_rate": 5.872104571607334e-07,
      "loss": 0.0693,
      "step": 3785
    },
    {
      "epoch": 2.7970479704797047,
      "grad_norm": 2.252185583114624,
      "learning_rate": 5.665777994219774e-07,
      "loss": 0.0489,
      "step": 3790
    },
    {
      "epoch": 2.800738007380074,
      "grad_norm": 2.9357056617736816,
      "learning_rate": 5.463099816548579e-07,
      "loss": 0.0296,
      "step": 3795
    },
    {
      "epoch": 2.804428044280443,
      "grad_norm": 2.4807140827178955,
      "learning_rate": 5.264073064985303e-07,
      "loss": 0.0642,
      "step": 3800
    },
    {
      "epoch": 2.804428044280443,
      "eval_loss": 0.06408160924911499,
      "eval_runtime": 597.937,
      "eval_samples_per_second": 4.532,
      "eval_steps_per_second": 4.532,
      "step": 3800
    },
    {
      "epoch": 2.8081180811808117,
      "grad_norm": 0.1222873255610466,
      "learning_rate": 5.068700711398283e-07,
      "loss": 0.0252,
      "step": 3805
    },
    {
      "epoch": 2.811808118081181,
      "grad_norm": 2.7077081203460693,
      "learning_rate": 4.876985673088336e-07,
      "loss": 0.05,
      "step": 3810
    },
    {
      "epoch": 2.8154981549815496,
      "grad_norm": 1.0309388637542725,
      "learning_rate": 4.6889308127453515e-07,
      "loss": 0.0308,
      "step": 3815
    },
    {
      "epoch": 2.819188191881919,
      "grad_norm": 1.2061136960983276,
      "learning_rate": 4.5045389384053006e-07,
      "loss": 0.037,
      "step": 3820
    },
    {
      "epoch": 2.8228782287822876,
      "grad_norm": 1.447861671447754,
      "learning_rate": 4.323812803408572e-07,
      "loss": 0.0709,
      "step": 3825
    },
    {
      "epoch": 2.8265682656826567,
      "grad_norm": 2.8048102855682373,
      "learning_rate": 4.146755106358591e-07,
      "loss": 0.0177,
      "step": 3830
    },
    {
      "epoch": 2.830258302583026,
      "grad_norm": 0.708439826965332,
      "learning_rate": 3.973368491081708e-07,
      "loss": 0.043,
      "step": 3835
    },
    {
      "epoch": 2.8339483394833946,
      "grad_norm": 1.587243914604187,
      "learning_rate": 3.803655546587598e-07,
      "loss": 0.0793,
      "step": 3840
    },
    {
      "epoch": 2.837638376383764,
      "grad_norm": 0.16137495636940002,
      "learning_rate": 3.6376188070308405e-07,
      "loss": 0.0319,
      "step": 3845
    },
    {
      "epoch": 2.841328413284133,
      "grad_norm": 0.3031267523765564,
      "learning_rate": 3.4752607516726486e-07,
      "loss": 0.045,
      "step": 3850
    },
    {
      "epoch": 2.8450184501845017,
      "grad_norm": 1.6474494934082031,
      "learning_rate": 3.3165838048443423e-07,
      "loss": 0.0413,
      "step": 3855
    },
    {
      "epoch": 2.848708487084871,
      "grad_norm": 0.1847996860742569,
      "learning_rate": 3.161590335910791e-07,
      "loss": 0.0395,
      "step": 3860
    },
    {
      "epoch": 2.85239852398524,
      "grad_norm": 1.1464086771011353,
      "learning_rate": 3.0102826592351695e-07,
      "loss": 0.0681,
      "step": 3865
    },
    {
      "epoch": 2.856088560885609,
      "grad_norm": 2.7658228874206543,
      "learning_rate": 2.8626630341443974e-07,
      "loss": 0.055,
      "step": 3870
    },
    {
      "epoch": 2.859778597785978,
      "grad_norm": 1.6004120111465454,
      "learning_rate": 2.71873366489539e-07,
      "loss": 0.0814,
      "step": 3875
    },
    {
      "epoch": 2.8634686346863467,
      "grad_norm": 2.3195300102233887,
      "learning_rate": 2.578496700642169e-07,
      "loss": 0.0555,
      "step": 3880
    },
    {
      "epoch": 2.867158671586716,
      "grad_norm": 0.976421594619751,
      "learning_rate": 2.441954235403665e-07,
      "loss": 0.0385,
      "step": 3885
    },
    {
      "epoch": 2.8708487084870846,
      "grad_norm": 0.29511627554893494,
      "learning_rate": 2.3091083080326314e-07,
      "loss": 0.032,
      "step": 3890
    },
    {
      "epoch": 2.874538745387454,
      "grad_norm": 1.236059308052063,
      "learning_rate": 2.1799609021850575e-07,
      "loss": 0.0431,
      "step": 3895
    },
    {
      "epoch": 2.878228782287823,
      "grad_norm": 1.012030839920044,
      "learning_rate": 2.0545139462905815e-07,
      "loss": 0.0658,
      "step": 3900
    },
    {
      "epoch": 2.878228782287823,
      "eval_loss": 0.06404869258403778,
      "eval_runtime": 598.1104,
      "eval_samples_per_second": 4.531,
      "eval_steps_per_second": 4.531,
      "step": 3900
    },
    {
      "epoch": 2.8819188191881917,
      "grad_norm": 4.78549337387085,
      "learning_rate": 1.9327693135237635e-07,
      "loss": 0.0692,
      "step": 3905
    },
    {
      "epoch": 2.885608856088561,
      "grad_norm": 1.7463091611862183,
      "learning_rate": 1.8147288217759684e-07,
      "loss": 0.0522,
      "step": 3910
    },
    {
      "epoch": 2.88929889298893,
      "grad_norm": 4.3140459060668945,
      "learning_rate": 1.7003942336283885e-07,
      "loss": 0.0554,
      "step": 3915
    },
    {
      "epoch": 2.892988929889299,
      "grad_norm": 1.0414834022521973,
      "learning_rate": 1.5897672563255639e-07,
      "loss": 0.0753,
      "step": 3920
    },
    {
      "epoch": 2.896678966789668,
      "grad_norm": 0.26387670636177063,
      "learning_rate": 1.4828495417500976e-07,
      "loss": 0.0282,
      "step": 3925
    },
    {
      "epoch": 2.900369003690037,
      "grad_norm": 0.16149787604808807,
      "learning_rate": 1.379642686397731e-07,
      "loss": 0.0159,
      "step": 3930
    },
    {
      "epoch": 2.904059040590406,
      "grad_norm": 0.9122258424758911,
      "learning_rate": 1.2801482313537227e-07,
      "loss": 0.0319,
      "step": 3935
    },
    {
      "epoch": 2.907749077490775,
      "grad_norm": 0.4395073652267456,
      "learning_rate": 1.1843676622697308e-07,
      "loss": 0.0088,
      "step": 3940
    },
    {
      "epoch": 2.911439114391144,
      "grad_norm": 2.1796493530273438,
      "learning_rate": 1.0923024093416334e-07,
      "loss": 0.0504,
      "step": 3945
    },
    {
      "epoch": 2.915129151291513,
      "grad_norm": 1.1214442253112793,
      "learning_rate": 1.0039538472882415e-07,
      "loss": 0.0597,
      "step": 3950
    },
    {
      "epoch": 2.9188191881918817,
      "grad_norm": 0.10845483839511871,
      "learning_rate": 9.193232953306485e-08,
      "loss": 0.0329,
      "step": 3955
    },
    {
      "epoch": 2.922509225092251,
      "grad_norm": 1.5788066387176514,
      "learning_rate": 8.384120171726629e-08,
      "loss": 0.0611,
      "step": 3960
    },
    {
      "epoch": 2.92619926199262,
      "grad_norm": 0.34319591522216797,
      "learning_rate": 7.612212209818503e-08,
      "loss": 0.0782,
      "step": 3965
    },
    {
      "epoch": 2.9298892988929888,
      "grad_norm": 1.099743127822876,
      "learning_rate": 6.877520593715214e-08,
      "loss": 0.0416,
      "step": 3970
    },
    {
      "epoch": 2.933579335793358,
      "grad_norm": 2.077481508255005,
      "learning_rate": 6.180056293835501e-08,
      "loss": 0.0363,
      "step": 3975
    },
    {
      "epoch": 2.937269372693727,
      "grad_norm": 3.6803154945373535,
      "learning_rate": 5.519829724719428e-08,
      "loss": 0.0647,
      "step": 3980
    },
    {
      "epoch": 2.940959409594096,
      "grad_norm": 1.670228123664856,
      "learning_rate": 4.896850744872949e-08,
      "loss": 0.0298,
      "step": 3985
    },
    {
      "epoch": 2.944649446494465,
      "grad_norm": 2.4392004013061523,
      "learning_rate": 4.311128656621644e-08,
      "loss": 0.0279,
      "step": 3990
    },
    {
      "epoch": 2.948339483394834,
      "grad_norm": 0.35334134101867676,
      "learning_rate": 3.762672205969431e-08,
      "loss": 0.034,
      "step": 3995
    },
    {
      "epoch": 2.952029520295203,
      "grad_norm": 2.09455943107605,
      "learning_rate": 3.251489582471456e-08,
      "loss": 0.0881,
      "step": 4000
    },
    {
      "epoch": 2.952029520295203,
      "eval_loss": 0.06407692283391953,
      "eval_runtime": 601.8025,
      "eval_samples_per_second": 4.503,
      "eval_steps_per_second": 4.503,
      "step": 4000
    },
    {
      "epoch": 2.955719557195572,
      "grad_norm": 1.2108019590377808,
      "learning_rate": 2.7775884191083568e-08,
      "loss": 0.0279,
      "step": 4005
    },
    {
      "epoch": 2.959409594095941,
      "grad_norm": 0.5673457384109497,
      "learning_rate": 2.3409757921746823e-08,
      "loss": 0.0345,
      "step": 4010
    },
    {
      "epoch": 2.96309963099631,
      "grad_norm": 2.447756290435791,
      "learning_rate": 1.9416582211723155e-08,
      "loss": 0.0697,
      "step": 4015
    },
    {
      "epoch": 2.9667896678966788,
      "grad_norm": 1.0981190204620361,
      "learning_rate": 1.5796416687127723e-08,
      "loss": 0.078,
      "step": 4020
    },
    {
      "epoch": 2.970479704797048,
      "grad_norm": 3.855348587036133,
      "learning_rate": 1.2549315404289386e-08,
      "loss": 0.074,
      "step": 4025
    },
    {
      "epoch": 2.974169741697417,
      "grad_norm": 2.9024105072021484,
      "learning_rate": 9.675326848940236e-09,
      "loss": 0.032,
      "step": 4030
    },
    {
      "epoch": 2.977859778597786,
      "grad_norm": 1.6372991800308228,
      "learning_rate": 7.174493935491189e-09,
      "loss": 0.0646,
      "step": 4035
    },
    {
      "epoch": 2.981549815498155,
      "grad_norm": 3.4772567749023438,
      "learning_rate": 5.0468540063908225e-09,
      "loss": 0.0682,
      "step": 4040
    },
    {
      "epoch": 2.985239852398524,
      "grad_norm": 1.9018471240997314,
      "learning_rate": 3.292438831564715e-09,
      "loss": 0.0368,
      "step": 4045
    },
    {
      "epoch": 2.988929889298893,
      "grad_norm": 2.9701285362243652,
      "learning_rate": 2.1576463063860407e-09,
      "loss": 0.0929,
      "step": 4050
    },
    {
      "epoch": 2.992619926199262,
      "grad_norm": 2.2695205211639404,
      "learning_rate": 1.0750979602319745e-09,
      "loss": 0.035,
      "step": 4055
    },
    {
      "epoch": 2.9963099630996313,
      "grad_norm": 1.0418404340744019,
      "learning_rate": 3.6583367463427497e-10,
      "loss": 0.043,
      "step": 4060
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.8686005473136902,
      "learning_rate": 2.986404032956358e-11,
      "loss": 0.0313,
      "step": 4065
    },
    {
      "epoch": 3.0,
      "step": 4065,
      "total_flos": 1.74628245270528e+16,
      "train_loss": 0.07514838176936388,
      "train_runtime": 39885.9347,
      "train_samples_per_second": 0.815,
      "train_steps_per_second": 0.102
    }
  ],
  "logging_steps": 5,
  "max_steps": 4065,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.74628245270528e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
