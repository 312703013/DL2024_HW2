05/19/2024 23:05:18 - INFO - transformers.tokenization_utils_base - loading file tokenizer.model from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\tokenizer.model

05/19/2024 23:05:18 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\tokenizer.json

05/19/2024 23:05:18 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json from cache at None

05/19/2024 23:05:18 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\special_tokens_map.json

05/19/2024 23:05:18 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\tokenizer_config.json

05/19/2024 23:05:18 - INFO - llamafactory.data.loader - Loading dataset QT2_2.json...

05/19/2024 23:05:20 - INFO - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\config.json

05/19/2024 23:05:20 - INFO - transformers.configuration_utils - Model config LlamaConfig {
  "_name_or_path": "hfl/chinese-llama-2-1.3b",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 4,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 55296
}


05/19/2024 23:05:20 - WARNING - llamafactory.model.utils.attention - FlashAttention-2 is not installed.

05/19/2024 23:05:20 - INFO - llamafactory.model.utils.quantization - Quantizing model to 4 bit.

05/19/2024 23:05:20 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.

05/19/2024 23:05:21 - INFO - transformers.modeling_utils - loading weights file pytorch_model.bin from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\pytorch_model.bin

05/19/2024 23:05:21 - INFO - transformers.modeling_utils - Instantiating LlamaForCausalLM model under default dtype torch.float16.

05/19/2024 23:05:21 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0
}


05/19/2024 23:05:22 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing LlamaForCausalLM.


05/19/2024 23:05:22 - INFO - transformers.modeling_utils - All the weights of LlamaForCausalLM were initialized from the model checkpoint at hfl/chinese-llama-2-1.3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

05/19/2024 23:05:23 - INFO - transformers.generation.configuration_utils - loading configuration file generation_config.json from cache at C:\Users\zv750\.cache\huggingface\hub\models--hfl--chinese-llama-2-1.3b\snapshots\99f0fa34dfcdcd1497efdcefce45712cd9fed9ea\generation_config.json

05/19/2024 23:05:23 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "temperature": 0.2,
  "top_p": 0.9
}


05/19/2024 23:05:23 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.

05/19/2024 23:05:23 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.

05/19/2024 23:05:23 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

05/19/2024 23:05:23 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves\ChineseLLaMA2-1.3B\lora\train_2024-05-19-19-22-13

05/19/2024 23:05:23 - INFO - llamafactory.model.loader - all params: 1263046656

05/19/2024 23:05:23 - INFO - transformers.trainer - ***** Running Prediction *****

05/19/2024 23:05:23 - INFO - transformers.trainer -   Num examples = 1000

05/19/2024 23:05:23 - INFO - transformers.trainer -   Batch size = 2

05/19/2024 23:08:29 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves\ChineseLLaMA2-1.3B\lora\eval_2024-05-19-19-22-13\generated_predictions.jsonl

