我這次都是用PPT介紹的第三個方法：LLaMa Factory的介面去訓練模型
-
一開始選擇用Qwen系列的模型，因為這個系列看起來才有參數量比較少的小模型；結果很多都out of memory，所以就一直換、參數量比較少、越換越小的模型，最後用0.5B的模型之後才能使用，結果是0.64左右，沒想到這就是我這次最好的成績。
(跟我一起討論的其他同學，有用Qwen-8B的模型，結果非常卓越；他們是Kaggle排名第4、第5名)
-
剩沒幾天的時候，想說來是是看老師所說的llama3、llama2，但是不知道為何都不能使用，我看了報錯訊息說明說要先登入hugging face的帳號，但是我也都登入了，結果卻還是有一樣的報錯訊息；有其他的模型像是Gemma也要先登入hugging face，而且可以使用，不曉得為何llama系列模型在LLaMa Factory不能使用，或是我可能忽略了甚麼可能可以解決的辦法。總之最後沒有使用llama系列的模型
-
後來，剩沒多少時間的時候，各使用Gemma-2B、ChineseLLaMa-1.3B模型訓練了一次，成效都蠻差的，不知道為何分別只有0.292、0.235的結果...
-
